--- 13-07-10 ---

 * We can propose a number of barriers for different software/hardware configurations. The possible configs are:
  * send/recv                 (blocking com)
  * isend/irecv + recv/send   (non-blocking com)
  * coherent RMA (main memory is 'eventually' updated by the network card)
  * separated RMA (main memory is only updated upon action)
  * lossy communication (e.g. UDP)

  For modelling and algorithm design irecv/isend might be equivalent or very similar to coherent RMA

 * write something about reinitialization(problems/advantages) of the barriers REPORT
 * write something about process counts that are not a power of two REPORT

 * dissemination barrier implementation oddity: REPORT
  * sendrecv in openmpi 1.7 /mpich 3.0.4 is blocking. i.e. the send is blocking.
    Better would be to use an isend + recv and not wait send to complete.
    Torsten Hoefler's RDMA dissemination barrier (http://htor.inf.ethz.ch/publications/index.php?pub=22) and
    the original 1988 dissemination barrier (http://m1.archiveorange.com/m/att/jgLs0/ArchiveOrange_quY2vZm8S65M1KcPpGIe9XOlXCYa.pdf)
    both do it 'correctly'.

    ✓ implemented it myself. It is not faster on my dualcore+HT notebook
    □ benchmark isend dissemination vs dissemination properly

 * idea what is bad about (isend) dissemination REPORT:

  phase 1:  0 -> 1 -> 2 -> 3 -> 4 -> 5 -> 6 -> 7 -> 0
  phase 2:  0 ------> 2 ------> 4 ------> 6 ------> 0
                 1 ------> 3 ------> 5 ------> 7 ------> 1
  phase 3:  0 ----------------> 4 ----------------> 0
                 1 ----------------> 5 ----------------> 1
                      2 ----------------> 6 ----------------> 2
                           3 ----------------> 7 ----------------> 3

  now suppose one process (e.g. 4) has not yet arrived

  phase 1:  0 --> 1 ---> 2 ---> 3 --> (4) -x> 5 ---> 6 ---> 7 ---> 0
  phase 2:  0 ---------> 2  --------> (4) -----x---> 6 ----------> 0
                  1  ---------> 3 ---------> (5) ----x----> 7 --------> 1
  phase 3:  0 ----------------------> (4) -----------x-----------> 0
                 1 ------------------------> (5) -----------x----------> 1
                        2 ------------------------> (6) ----------x----------> 2
                               3 ------------------------> (7) ---------x---------> 3

  who knows whom while process 4 is missing:
   =phase 1=             =phase 2=             =phase 3=             =the ideal=
   7 know themselves     + 4 new               + 0 new               maximum would be 49
   + 6 new                                     total is 17
   0: 0           7    0: 0         6 7    0: 0         6 7    0: 0 1 2 3 5 6 7
   1: 0 1              1: 0 1         7    1: 0 1         7    1: 0 1 2 3 5 6 7
   2:   1 2            2: 0 1 2            2: 0 1 2            2: 0 1 2 3 5 6 7
   3:     2 3          3:   1 2 3          3:     2 3          3: 0 1 2 3 5 6 7
   5:         5        5:         5        5:         5        5: 0 1 2 3 5 6 7
   6:         5 6      6:         5 6      6:         5 6      6: 0 1 2 3 5 6 7
   7:           6 7    7:           6 7    7:           6 7    7: 0 1 2 3 5 6 7

  -> If one process is missing, the number of blocked processes doubles every phase.
     maximum progress is ~2*(n) of a maximum of (n-1)*(n-1). (Perhaps find out the exact formulas)

     note that the sent but not recved isends are waiting for the missing process to arrive. so the progression as
     soon as the missing process arrives should be quick.

     maybe we can build an argument about worst/avg/best cases for barrier algorithms.
     Consider different times of process arrival for this.

     □ suggest improved algorithms that mitigate this problem. (Is it really a problem? I would say so.)

 * Torsten Hoefler makes heavy use of the logP model. And claims the normal algorithms to be optimal wrt logP.
   It seems though, that logP is far from a proper model for distributed barrier algorithms (and worse for shared mem (pub=162 below).
   Maybe investigate further and write up that we deliberately ignore this model and the "optimality claims",
   and try to improve existing barrier algorithms.
   (claims are e.g. in TH's diploma thesis or http://spcl.inf.ethz.ch/Publications/index.php?pub=162). REPORT

--- 13-07-06 ---

 * added shared memory barriere using add-fetch and each thread has its own counter (is slower than normal add-fetch)

 * realization
  * dissemination barrier sends n*log n messages to achieve a barrier completion in log n rounds.
  * a gather+broadcast barrier (assuming both are implemented as tree-esque sends) would send a total of 2*(n-1) messages
    and completes in 2*log n rounds

    gather e.g.:  +--------+        +--------+             round (2)
                  v        |        v        |
                  0 <- 1   2 <- 3   4 <- 5   6 <- 7        round (1)
                  ^                 |
                  +-----------------+                      round (3)

  * i.e. dissemination is not optimal wrt the number of sent messages

 * interesting MPI non-blocking calls:

  I(-,b,s,r)send
  Irecv

  Wait(-, Any, Some, All)
  Test(-, Any, Some, All)

  Cancel

  Notes:
   * blocking and non-blocking p2p comminication can be mixed (e.g. isend + matching recv)
   * non-blocking collectives are not interesting, because everyone has to take part and no cancelation of anything is possible.
     they are generelly restrictive (evaluate on this in the REPORT)

 □ interesting MPI RMA calls:
  TBD

 □ familiarize yourself with MPI 3.0 RMA operations

 -> □ invent non-blocking-p2p and RMA barriers

--- 13-07-05 ---

 * implemented a new idea for a shared memory barrier called "super wasteful barrier"

  MPI 2.2 is bad for our purpose because:
   * put/get data may be not visible in main memory until a window is syncronized with fence/complete/wait
    * fence implies a barrier on the group
    * complete/wait is a local sync between fewer processes, but ultimately as bad as fence for our purpose
   -> motivate the consideration of 'MPI 3.0 only' in the REPORT

                       | 3.0 | 2.2
 ----------------------+-----+-----
  published            |sep12|sep09
                       |     |
  WinCreate            |  x  |  x
  WinAllocate          |  x  |  -
  WinAllocateShared    |  x  |  -
  WinSharedQuery       |  x  |  -
  WinCreateDynamic     |  x  |  -
  WinAttach            |  x  |  -
  WinDetach            |  x  |  -
  WinFree              |  x  |  x
                       |     |
  Put                  |  x  |  x
  Get                  |  x  |  x
  Accumulate           |  x  |  x
  GetAccumulate        |  x  |  -
  FetchAndOp           |  x  |  -
  CompareAndSwap       |  x  |  -
                       |     |
  RequestBasedRMA      |  x  |  -
                       |     |
  WinFence             |  x  |  x
  WinStart             |  x  |  x
  WinComplete          |  x  |  x
  WinPost              |  x  |  x
  WinWait              |  x  |  x
  WinTest              |  x  |  x
  WinLock              |  x  |  x
  WinLockAll           |  x  |  -
  WinUnlock            |  x  |  x
  WinUnlockAll         |  x  |  -
  WinFlush             |  x  |  -
  WinFlushAll          |  x  |  -
  WinFlushLocal        |  x  |  -
  WinFlushLocalAll     |  x  |  -
  WinSync              |  x  |  -
                       |     |
  OpenMPI       1.7    |  -  |  x
  MPICH         3.0.4  |  x  |  x
  MVAPICH2      1.9    |  x  |  x  (release may13, based on MPICH 3.0.3)
  Intel MPI     4.1    |  -  |  x  (release sep12, update june13, based on MPICH and MVAPICH, ref[1])
  Cray MPT      6.0.0  |  x  |  x  (release jun13, based on MPICH 3.0.3, ref[2], ref[3])
  IBM PE        v1.3   |  -  |  x  (source may13, ref[4]
  HP MPI               |     |     (couldn't find it. seems to have been sold.)
  SGI MPI              |     |     (couldn't find it)
  Bullx MPI            |  -  |  x  (based on OpenMPI, ref[6], ref[7])

  HPC Vendors of the current top500 (share of computers and performance in the june2013) :
   * 37.6% HP                * 32.7% IBM
   * 32.0% IBM               * 15.3% Cray
   * 09.8% Cray              * 14.0% HP
   * 03.6% SGI               * 03.8% SGI
   * 03.4% Bull              * 02.8% Bull

  ref claiming that OpenMPI powers big machines: http://blogs.cisco.com/performance/more-details-open-mpi-at-large-scale/

  refs:
   [1] http://software.intel.com/en-us/articles/intel-mpi-library-41-release-notes
   [2] http://docs.cray.com/books/S-9407-1306/
   [3] http://docs.cray.com/relnotes/
   [4] http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=%2Fcom.ibm.cluster.pe.doc%2Fpebooks.html
   [5] https://www.google.de/search?q=bullx+mpi+3.0
   [6] http://www.google.de/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&ved=0CDYQFjAB&url=http%3A%2F%2Fwww.cenapad.ufmg.br%2Findex.php%2Fdocumentosemanuais%3Fdownload%3D2%3Abullx-cluster-suite-userguide-ingles&ei=n5TWUeSVLY2LswaJ8YHIBQ&usg=AFQjCNHhLOBJ7S2-Cix-iwoS3l-4peMDDQ&sig2=izW7DZA_VsHNHKPfnqRLoA&bvm=bv.48705608,d.Yms
   [7] http://www.google.de/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CC4QFjAA&url=http%3A%2F%2Fsupport.bull.com%2Fdocumentation%2Fbyproduct%2Finfra%2Fsw-extremcomp%2Fsw-extremcomp-bas5xeon%2Fg%2F86Y222FA03%2F86A222FA03.pdf&ei=n5TWUeSVLY2LswaJ8YHIBQ&usg=AFQjCNFsuvPPxO_tuO-4is9fZ1qaTvKz6w&sig2=S6WDmcskMVwQq_UWgymBYA&bvm=bv.48705608,d.Yms

  * hardware specific implementations:
   * couldn't find anything special in MPICH, MVAPICH and OpenMPI
   * infiniband uses an RDMA based n-way dissemination barrier http://htor.inf.ethz.ch/publications/index.php?pub=22
   * other than that everyone uses usual (and RDMA based) dissemination or recursive doubling barriers, which are all slower than or as-fast-as the RDMA n-way dissemination described in the paper
   * A paper about dissemination barrier for shared memory (http://spcl.inf.ethz.ch/Publications/index.php?pub=162)

--- 13-06-29 ---
--- 13-06-28 ---

 * MPICH Barrier 3.0.4 implementation :
  * src/mpi/coll/barrier.c : MPIR_Barrier_intra
  * log2 Dissemination (k steps, 0<=k<=(ceil(log p)-1), proc i sends to proc (i + 2^k) % p and receives from proc (i - 2^k) % p)
   * "Two Algorithms for Barrier Synchronization," International Journal of Parallel Programming, 17(1):1-17, 1988

 * OpenMPI 1.7 Barrier implementation :
  * ompi/mca/coll/tuned/coll_tuned_barrier.c : ompi_coll_tuned_barrier_intra_recursivedoubling, ompi_coll_tuned_barrier_intra_bruck
  * n  = 2^k -> recursive doubling
  * n != 2^k -> bruck = dissemination with swapped receiver / sender procs
   * "Two Algorithms for Barrier Synchronization," International Journal of Parallel Programming, 17(1):1-17, 1988
  □ oddly, ompi-bruck = dissemination. bruck should communicate in the reverse direction. to be confirmed.

 * note: intra communicators are very different from inter communicators. we want intra!
  (□ maybe have a look at inter communicator barriers)
 
 -> we will use to the 2 way dissemination barrier to compare ours to

 * began rewriting the rapl-benchmark for mpi
  ✓ insert dissemination barrier instead of dummy

--- 13-06-25 ---

 * installed rapl measurement system (4 core sandbridge)

--- 13-06-24 ---

 * iterated on rapl benchmark
 * meeting with MV, SK
  * make it paperable is now priority 1 (beleg faellt schon ab, kein problem)
  * we don't need an add-fetch model right now
  * we want the distributed algorithm and a classical representative

--- 13-06-22 ---

 * thread pinning is now affinity aware (needed for venus)
 * add rapl benchmarks for:
  * uncontested __atomic_add_fetch
  * add-fetch wait spinning

--- 13-06-21 ---
--- 13-06-20 ---

 * as soon as you got access to a bigger system
  □ test rapl benchmark ronny-array with #define ARRAY_8
  □ make rapl benchmark measure multiple chips, not just cpu0

--- 13-06-19 ---
--- 13-06-18 ---
--- 13-06-14 + 15 ---
--- 13-06-13 ---

 * prepared and started taurus benchmarks
 * added taurus topology and beautified atlas topology

 * meeting with marcus haehnel
  * switching off powersaving
   * probably not possible on taurus
   * use "cpufreq-info" to observe current clock frequency
   * if powersaving is off on linux the wrong powersaving governor still does powersaving
     if /sys/devices/system/cpu/cpu0/cpufreq/scaling_driver = "pstate" it is the wrong governor

  * measurement using RAPL
   * per package, choice between with or without "uncore" - not clear what it means (e.g. with or without L3 Cache) - measuring the whole package probably the best choice
   * 1ms period with some deviation around measurement points
     -> use large intervals and use median of many repetitions
   * it is very hard to measure energy about cache state transitions, because you cannot redo benchmarks
     without 'preparing' the next run. You would need to measure the 'preparing' as well.
     preparing many cachelines and then start running is bad as well, because new cache effects might appear.
     generally every measurement that needs preparation and where a run is very short is impossible/hard/bad.
   * measuring the whole barrier is doable
  □ measure no-array- and array-ronny-barrier to see if there are large differences
    -> model arcordingly

--- 13-06-12 ---

 * new model minimization: remove backloop
   * steady state queries don't work anymore
   * use only from-initial-state queries

--- 13-06-07 ---

 * changed benchmark to measure cycles instead of nanoseconds
 * redid benchmarks
   * 100secs per measurement instead of 20, more datapoints, added plots, added topology picture of the measurement machine 'atlas'

--- 13-06-05 ---

 ✓ removed clraR 3, because (1) clr is not need for our purposes here (2) it is probably wrong (3) doesn't work for the one-loop variant because of the sync transition
 ✓ benchmark now uses a fixed amount of time (and counts repetitions), rather than a fixed amount of repetitions and measuring time
 ✓ added spin model of add-fetch barrier
 ✓ fixed two bugs in the ronny-barrier. both fail around 32+ threads. >>> one of them is probably the reason why the algorithm failed at 32 threads in the first report <<<

 ✓ did nice benchmarks up to 64 threads

 misc notes:
  ✓ it is possible to reduce the 2loop ronny model for non-ss queries:
   remove loc 10, 11 and set the while condition to 'true'
  □ one might also combine some of the transitions that are not in loops because they are triggered rarely

--- 13-05-17 ---

 * changed add-fetch benchmark to measure only the loop time, not the other uninteresting things
 * add array implementation (with flexible element size choice)
 * did some benchmarks (seems to scale not much worse than atomic ops)

 * add spin model for array version of the barrier

 todo:
  ✓ use aquire release mem model
  ✓ redo benchmarks (large deviations in runtime right now), and perhaps benchmark up to 64 threads
  □ model add-fetch barrier
    □ add energy rewards to add-fetch
    □ add energy rewards to ronny
    -> □ compare both
    -> □ line up measured data with both models

  □ interpret benchmarks. Complexity for both barriers?

 maybe todo:
  □ papi enable both barriers and make additional measurements
    cache stuff, wallclocktime/sleeptime, number of instructions
 
  □ put ronny barrier into libgomp and benchmark WITH backoff ->
    * argument that it is not considerably slower but doesnt need atomic ops foo
    □ quantify energy consumption?

--- 13-05-03 ---

 * implemented simple add-fetch-spin-wait barrier ( implementation/remember-others-and-add-fetch/* )
 * benchmarked ronny vs add-fetch (with $ time add-fetch ...)

--- 13-04-27 ---

 * implemented another simple model minimization (copy=0 as early as possible). semantically and quant. equivalent
 * new variant: remember whom you have seen: in models/prism/ctmc/with-cache + git/models/spin/remember-others

--- 13-04-26 ---
--- 13-04-15 ---
--- 13-04-13 ---

 grosser beleg meeting (baier, klueppel, tews):

 energie effizienz!
   * Beleg soll (irgendwie) in HAEC rein
   * RAPL-Zahlen nutzen

 algorithmus:
   * verteilte variante machen
     -> neues modell
   * andere varianten ausprobieren
   * parametrisieren
     * was parametrisieren?
     * Auswahl von energieeffizient bis performant

 modell:
   * wofuer?
     * gute parameter fuer den algorithmus aus dem modell ableiten
     * skalierung/performance mit mehr prozessen als messbar vorhersehen
     * argumentieren, dass der algorithmus gut ist
   * (system-/arbeits-)last nur sehr einfach modellieren. keine arbeit in gute last-modellierung stecken

--- before 13-04-01 ---

 □ make a distributed variant of the barrier
 
 ✓ go beyond 32 threads (i.e. use an array rather than one barrier variable)
 ✓ not only add yourself to the barrier, but everyone you have seen so far, too

 □ implement/model simple atomic increment barrier and compare against our protocol

 □ it might work to just remove copy and work directly on the shared vars.
   One more shared read, but a variable less (the model would be happy, perhaps)

 □ implement model minimizations
  -> □ scale model for more threads
  e.g.:
   ✓ remove second loop
   ✓ put cacheline state into a central object. e.g. in the case of modified, it is not needed to save the distinct state of the n-1 other threads because is implied.
   □ symmetry
   * partial order

 □ maybe model that during the work period the cacheline state might change

 actual implementation:
   □ try mwait, pause for the loops
   □ use back-off strategies
   □ address nthreads > maxthreads case
