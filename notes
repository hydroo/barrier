--- 13-06-14 + 15 ---

 * in the process of creating a rapl benchmark tool
 * add better atlas benchmark
 * add an new detail to taurus' topology

--- 13-06-13 ---

 * prepared and started taurus benchmarks
 * added taurus topology and beautified atlas topology

 * meeting with marcus haehnel

  * switching off powersaving
   * probably not possible on taurus
   * use "cpufreq-info" to observe current clock frequency
   * if powersaving is off on linux the wrong powersaving governor still does powersaving
     if /sys/devices/system/cpu/cpu0/cpufreq/scaling_driver = "pstate" it is the wrong governor

  * measurement using RAPL
   * per package, choice between with or without "uncore" - not clear what it means (e.g. with or without L3 Cache) - measuring the whole package probably the best choice
   * 10ms (bestaetigung noetig) period with deviation (how much?) around measurement points
     -> use large intervals and use median of many repetitions
   * it is very hard to measure energy about cache state transitions, because you cannot redo benchmarks
     without 'preparing' the next run. You would need to measure the 'preparing' as well.
     preparing many cachelines and then start running is bad as well, because new cache effects might appear.
     generally every measurement that needs preparation and where a run is very short is impossible/hard/bad.
   * measuring the whole barrier is doable

  □ measure no-array- and array-ronny-barrier to see if there are large differences
    -> model arcordingly (not sure yet what this exactly means)

--- 13-06-12 ---

 * fixed benchmark error for usleep > 0
   □ redo them
 * new model minimization: remove backloop
   * steady state queries don't work anymore
   * use only from-initial-state queries

--- 13-06-07 ---

 * changed benchmark to measure nanoseconds instead of cycles
 * redid benchmarks
   * 100secs per measurement instead of 20
   * more datapoints
   * added plots
   * added topology picture of the measurement machine 'atlas'

--- 13-06-05 ---

 ✓ removed clraR 3, because (1) clr is not need for our purposes here (2) it is probably wrong (3) doesn't work for the one-loop variant because of the sync transition
 ✓ benchmark now uses a fixed amount of time (and counts repetitions), rather than a fixed amount of repetitions and measuring time
 ✓ added spin model of add-fetch barrier
 ✓ fixed two bugs in the ronny-barrier. both fail around 32+ threads. >>> one of them is probably the reason why the algorithm failed at 32 threads in the first report <<<

 ✓ did nice benchmarks up to 64 threads

 misc notes:
  □ it is possible to reduce the 2loop ronny model for non-ss queries:
   remove loc 10, 11 and set the while condition to 'true'
  □ one might also combine some of the transitions that are not in loops because they are triggered rarely

--- 13-05-17 ---

 * changed add-fetch benchmark to measure only the loop time, not the other uninteresting things
 * add array implementation (with flexible element size choice)
 * did some benchmarks (seems to scale not much worse than atomic ops)

 * add spin model for array version of the barrier

 todo:
  ✓ use aquire release mem model
  ✓ redo benchmarks (large deviations in runtime right now), and perhaps benchmark up to 64 threads
  □ model add-fetch barrier
    □ add energy rewards to add-fetch
    □ add energy rewards to ronny
    -> □ compare both
    -> □ line up measured data with both models

  □ interpret benchmarks. Complexity for both barriers?

 maybe todo:
  □ papi enable both barriers and make additional measurements
    cache stuff, wallclocktime/sleeptime, number of instructions
 
  □ put ronny barrier into libgomp and benchmark WITH backoff ->
    * argument that it is not considerably slower but doesnt need atomic ops foo
    □ quantify energy consumption?

--- 13-05-03 ---

 * implemented simple add-fetch-spin barrier ( implementation/remember-others-and-add-fetch/* )
 * benchmarked ronny vs add-fetch (with $ time add-fetch ...)

--- 13-04-27 ---

 * implemented another simple model minimization (copy=0 as early as possible). semantically and quant. equivalent
 * new variant: remember whom you have seen: in models/prism/ctmc/with-cache + git/models/spin/remember-others

   n: 3, work: 0, read: 50, write: 100

          |dont remember others         |remember others
          |normal         one-loop      |normal         one-loop
   -------+-----------------------------+-------------------------
   states |259102         7789          |322957         10195
   trans  |777306         23199         |968871         30384
   matrix |336231         31085         |335075         33451

   clraR 1|300    copied  326           |262            296
   clraR 2|476    c       533           |441            496

--- 13-04-26 ---

 [..]

--- 13-04-15 ---

 paper meeting (baier,klueppel,tews,mc guire,mvoelp):

                 | las vegas  | monte carlo
 ----------------+------------+-----------------
 bounded runtime | variant y? | current barrier
 ----------------+------------+-----------------
 prob runtime    | pwcs       | variant x?
 ----------------+------------+-----------------
 energy effic    | ?          | ?


 possible topics
  * barrier construction (story?)
   * variants
  * model
  * analysis
   * garantierter fortschritt

  * energie
   * mwait/backoff
   * ???
   * ???
 
 schedule
  * 4-15 today
  ???
  * 4-22 small meeting
   * discuss variants
  ???
  * 4-29 bigger meeting
  ???
  * 5-13 to 5-30 writing

  * konferenz aussuchen

--- 13-04-13 ---

 grosser beleg meeting (baier, klueppel, tews):

 energie effizienz!
   * Beleg soll (irgendwie) in HAEC rein
   * RAPL-Zahlen nutzen

 algorithmus:
   * verteilte variante machen
     -> neues modell
   * andere varianten ausprobieren
   * parametrisieren
     * was parametrisieren?
     * Auswahl von energieeffizient bis performant

 modell:
   * wofuer?
     * gute parameter fuer den algorithmus aus dem modell ableiten
     * skalierung/performance mit mehr prozessen als messbar vorhersehen
     * argumentieren, dass der algorithmus gut ist
   * (system-/arbeits-)last nur sehr einfach modellieren. keine arbeit in gute last-modellierung stecken
 
 sonstiges:
  * die "kaputte" one-loop variante nicht untersuchen

--- before 13-04-01 ---

 □ make it work in a distributed setting
 
 ✓ go beyond 32 threads (i.e. use an array rather than one barrier variable)

 ✓ not only add yourself to the barrier, but everyone you have seen so far, too

 □ implement/model simple atomic increment barrier and compare against our protocol

 □ it might work to just remove copy and work directly on the shared vars.
   One more shared read, but a variable less (the model would be happy, perhaps)

 □ implement model minimizations
  -> □ scale model for more threads
  e.g.:
   ✓ remove second loop
   ✓ put cacheline state into a central object. e.g. in the case of modified, it is not needed to save the distinct state of the n-1 other threads because is implied.
   □ symmetry
   * partial order

 □ maybe model that during the work period the cacheline state might change

 actual implementation:
   □ try mwait, pause for the loops
   □ use back-off strategies
   □ address nthreads > maxthreads case
