--- 13-07-05 ---

 * implemented a new idea for a shared memory barrier called "super wasteful barrier"

  MPI 2.2 is bad for our purpose because:
   * put/get data may be not visible in main memory until a window is syncronized with fence/complete/wait
    * fence implies a barrier on the group
    * complete/wait is a local sync between fewer processes, but ultimately as bad as fence for our purpose

                       | 3.0 | 2.2
 ----------------------+-----+-----
  published            |sep12|sep09
                       |     |
  WinCreate            |  x  |  x
  WinAllocate          |  x  |  -
  WinAllocateShared    |  x  |  -
  WinSharedQuery       |  x  |  -
  WinCreateDynamic     |  x  |  -
  WinAttach            |  x  |  -
  WinDetach            |  x  |  -
  WinFree              |  x  |  x
                       |     |
  Put                  |  x  |  x
  Get                  |  x  |  x
  Accumulate           |  x  |  x
  GetAccumulate        |  x  |  -
  FetchAndOp           |  x  |  -
  CompareAndSwap       |  x  |  -
                       |     |
  RequestBasedRMA      |  x  |  -
                       |     |
  WinFence             |  x  |  x
  WinStart             |  x  |  x
  WinComplete          |  x  |  x
  WinPost              |  x  |  x
  WinWait              |  x  |  x
  WinTest              |  x  |  x
  WinLock              |  x  |  x
  WinLockAll           |  x  |  -
  WinUnlock            |  x  |  x
  WinUnlockAll         |  x  |  -
  WinFlush             |  x  |  -
  WinFlushAll          |  x  |  -
  WinFlushLocal        |  x  |  -
  WinFlushLocalAll     |  x  |  -
  WinSync              |  x  |  -
                       |     |
  OpenMPI       1.7    |  -  |  x
  MPICH         3.0.4  |  x  |  x
  MVAPICH2      1.9    |  x  |  x  (release may13, based on MPICH 3.0.3)
  Intel MPI     4.1    |  -  |  x  (release sep12, update june13, based on MPICH and MVAPICH, ref[1])
  Cray MPT      6.0.0  |  x  |  x  (release jun13, based on MPICH 3.0.3, ref[2], ref[3])
  IBM PE        v1.3   |  -  |  x  (source may13, ref[4]
  HP MPI               |     |     (couldn't find it. seems to have been sold.)
  SGI MPI              |     |     (couldn't find it)
  Bullx MPI            |  -  |  x  (based on OpenMPI, ref[6], ref[7])

  HPC Vendors of the current top500 (share of computers and performance in the june2013) :
   * 37.6% HP                * 32.7% IBM
   * 32.0% IBM               * 15.3% Cray
   * 09.8% Cray              * 14.0% HP
   * 03.6% SGI               * 03.8% SGI
   * 03.4% Bull              * 02.8% Bull

  ref claiming that OpenMPI powers big machines: http://blogs.cisco.com/performance/more-details-open-mpi-at-large-scale/

  refs:
   [1] http://software.intel.com/en-us/articles/intel-mpi-library-41-release-notes
   [2] http://docs.cray.com/books/S-9407-1306/
   [3] http://docs.cray.com/relnotes/
   [4] http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=%2Fcom.ibm.cluster.pe.doc%2Fpebooks.html
   [5] https://www.google.de/search?q=bullx+mpi+3.0
   [6] http://www.google.de/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&ved=0CDYQFjAB&url=http%3A%2F%2Fwww.cenapad.ufmg.br%2Findex.php%2Fdocumentosemanuais%3Fdownload%3D2%3Abullx-cluster-suite-userguide-ingles&ei=n5TWUeSVLY2LswaJ8YHIBQ&usg=AFQjCNHhLOBJ7S2-Cix-iwoS3l-4peMDDQ&sig2=izW7DZA_VsHNHKPfnqRLoA&bvm=bv.48705608,d.Yms
   [7] http://www.google.de/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CC4QFjAA&url=http%3A%2F%2Fsupport.bull.com%2Fdocumentation%2Fbyproduct%2Finfra%2Fsw-extremcomp%2Fsw-extremcomp-bas5xeon%2Fg%2F86Y222FA03%2F86A222FA03.pdf&ei=n5TWUeSVLY2LswaJ8YHIBQ&usg=AFQjCNFsuvPPxO_tuO-4is9fZ1qaTvKz6w&sig2=S6WDmcskMVwQq_UWgymBYA&bvm=bv.48705608,d.Yms

  * hardware specific implementations:
   * couldn't find anything special
   □ pending mail to torsten hoefler

--- 13-06-29 ---
--- 13-06-28 ---

 * MPICH Barrier 3.0.4 implementation :
  * src/mpi/coll/barrier.c : MPIR_Barrier_intra
  * log2 Dissemination (k steps, 0<=k<=(ceil(log p)-1), proc i sends to proc (i + 2^k) % p and receives from proc (i - 2^k) % p)
   * "Two Algorithms for Barrier Synchronization," International Journal of Parallel Programming, 17(1):1-17, 1988

 * OpenMPI 1.7 Barrier implementation :
  * ompi/mca/coll/tuned/coll_tuned_barrier.c : ompi_coll_tuned_barrier_intra_recursivedoubling, ompi_coll_tuned_barrier_intra_bruck
  * n  = 2^k -> recursive doubling
  * n != 2^k -> bruck = dissemination with swapped receiver / sender procs
   * "Two Algorithms for Barrier Synchronization," International Journal of Parallel Programming, 17(1):1-17, 1988
  □ oddly, ompi-bruck = dissemination. bruck should communicate in the reverse direction. to be confirmed.

 * note: intra communicators are very different from inter communicators. we want intra!
 
 * -> we will use to the 2 way dissemination barrier to compare ours to

 * began rewriting the rapl-benchmark for mpi
  ✓ insert dissemination barrier instead of dummy

--- 13-06-25 ---

 * installed rapl measurement system (4 core sandbridge)

--- 13-06-24 ---

 * iterated on rapl benchmark
 * meeting with MV, SK
  * make it paperable is now priority 1 (beleg faellt schon ab, kein problem)
  * we don't need an add-fetch model right now
  * we want the distributed algorithm and a classical representative

--- 13-06-22 ---

 * thread pinning is now affinity aware (needed for venus)
 * add rapl benchmarks for:
  * uncontested __atomic_add_fetch
  * add-fetch wait spinning

--- 13-06-21 ---
--- 13-06-20 ---

 * as soon as you got access to a bigger system
  □ test rapl benchmark ronny-array with #define ARRAY_8
  □ make rapl benchmark measure multiple chips, not just cpu0

--- 13-06-19 ---
--- 13-06-18 ---
--- 13-06-14 + 15 ---
--- 13-06-13 ---

 * prepared and started taurus benchmarks
 * added taurus topology and beautified atlas topology

 * meeting with marcus haehnel
  * switching off powersaving
   * probably not possible on taurus
   * use "cpufreq-info" to observe current clock frequency
   * if powersaving is off on linux the wrong powersaving governor still does powersaving
     if /sys/devices/system/cpu/cpu0/cpufreq/scaling_driver = "pstate" it is the wrong governor

  * measurement using RAPL
   * per package, choice between with or without "uncore" - not clear what it means (e.g. with or without L3 Cache) - measuring the whole package probably the best choice
   * 1ms period with some deviation around measurement points
     -> use large intervals and use median of many repetitions
   * it is very hard to measure energy about cache state transitions, because you cannot redo benchmarks
     without 'preparing' the next run. You would need to measure the 'preparing' as well.
     preparing many cachelines and then start running is bad as well, because new cache effects might appear.
     generally every measurement that needs preparation and where a run is very short is impossible/hard/bad.
   * measuring the whole barrier is doable
  □ measure no-array- and array-ronny-barrier to see if there are large differences
    -> model arcordingly

--- 13-06-12 ---

 * new model minimization: remove backloop
   * steady state queries don't work anymore
   * use only from-initial-state queries

--- 13-06-07 ---

 * changed benchmark to measure cycles instead of nanoseconds
 * redid benchmarks
   * 100secs per measurement instead of 20, more datapoints, added plots, added topology picture of the measurement machine 'atlas'

--- 13-06-05 ---

 ✓ removed clraR 3, because (1) clr is not need for our purposes here (2) it is probably wrong (3) doesn't work for the one-loop variant because of the sync transition
 ✓ benchmark now uses a fixed amount of time (and counts repetitions), rather than a fixed amount of repetitions and measuring time
 ✓ added spin model of add-fetch barrier
 ✓ fixed two bugs in the ronny-barrier. both fail around 32+ threads. >>> one of them is probably the reason why the algorithm failed at 32 threads in the first report <<<

 ✓ did nice benchmarks up to 64 threads

 misc notes:
  ✓ it is possible to reduce the 2loop ronny model for non-ss queries:
   remove loc 10, 11 and set the while condition to 'true'
  □ one might also combine some of the transitions that are not in loops because they are triggered rarely

--- 13-05-17 ---

 * changed add-fetch benchmark to measure only the loop time, not the other uninteresting things
 * add array implementation (with flexible element size choice)
 * did some benchmarks (seems to scale not much worse than atomic ops)

 * add spin model for array version of the barrier

 todo:
  ✓ use aquire release mem model
  ✓ redo benchmarks (large deviations in runtime right now), and perhaps benchmark up to 64 threads
  □ model add-fetch barrier
    □ add energy rewards to add-fetch
    □ add energy rewards to ronny
    -> □ compare both
    -> □ line up measured data with both models

  □ interpret benchmarks. Complexity for both barriers?

 maybe todo:
  □ papi enable both barriers and make additional measurements
    cache stuff, wallclocktime/sleeptime, number of instructions
 
  □ put ronny barrier into libgomp and benchmark WITH backoff ->
    * argument that it is not considerably slower but doesnt need atomic ops foo
    □ quantify energy consumption?

--- 13-05-03 ---

 * implemented simple add-fetch-spin-wait barrier ( implementation/remember-others-and-add-fetch/* )
 * benchmarked ronny vs add-fetch (with $ time add-fetch ...)

--- 13-04-27 ---

 * implemented another simple model minimization (copy=0 as early as possible). semantically and quant. equivalent
 * new variant: remember whom you have seen: in models/prism/ctmc/with-cache + git/models/spin/remember-others

--- 13-04-26 ---
--- 13-04-15 ---
--- 13-04-13 ---

 grosser beleg meeting (baier, klueppel, tews):

 energie effizienz!
   * Beleg soll (irgendwie) in HAEC rein
   * RAPL-Zahlen nutzen

 algorithmus:
   * verteilte variante machen
     -> neues modell
   * andere varianten ausprobieren
   * parametrisieren
     * was parametrisieren?
     * Auswahl von energieeffizient bis performant

 modell:
   * wofuer?
     * gute parameter fuer den algorithmus aus dem modell ableiten
     * skalierung/performance mit mehr prozessen als messbar vorhersehen
     * argumentieren, dass der algorithmus gut ist
   * (system-/arbeits-)last nur sehr einfach modellieren. keine arbeit in gute last-modellierung stecken

--- before 13-04-01 ---

 □ make a distributed variant of the barrier
 
 ✓ go beyond 32 threads (i.e. use an array rather than one barrier variable)
 ✓ not only add yourself to the barrier, but everyone you have seen so far, too

 □ implement/model simple atomic increment barrier and compare against our protocol

 □ it might work to just remove copy and work directly on the shared vars.
   One more shared read, but a variable less (the model would be happy, perhaps)

 □ implement model minimizations
  -> □ scale model for more threads
  e.g.:
   ✓ remove second loop
   ✓ put cacheline state into a central object. e.g. in the case of modified, it is not needed to save the distinct state of the n-1 other threads because is implied.
   □ symmetry
   * partial order

 □ maybe model that during the work period the cacheline state might change

 actual implementation:
   □ try mwait, pause for the loops
   □ use back-off strategies
   □ address nthreads > maxthreads case
