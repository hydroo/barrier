--- 13-10-30 ---

  Why am I exclusively using expected values:
   * easier to understand. Exponential distributions are hard.
   * only way to do stacked bar charts, because subtracting distributions doesn't work (y=%, x=time would a be very different kind of chart)
   * I tried it and it was less insightful than the bar charts I've later created
   * quick test showed that variance on ronny's barriers seems not to be higher than the central counter and dissemination

--- 13-10-23 ---

  * 2.5Ghz. Thus: 1 Watt = 0.4nJ/Cycle (1 cycle = 0.4ns)

  * measured:
    constant = sleep + uncore = 11W
    waitspin = 1:16W 2:22W
    cc barrier = 2:18W
    rs barrier = 2:24W

  * extrapolation:
    waitspin = +5W per core (+uncore)
    cc +4W per core (+uncore)
    rs +6W per core (+uncore)

  * expected model checking results:
    threads | cc (W) (nJ/cycle) | rs (W) (nJ/cycle)
    --------+-------------------+------------------
          2 |    19  7.6        |    23  9.2
          3 |    23  9.2        |    29  11.6
          4 |    27  10.8       |    35  14.0
          5 |    31  12.4       |
          6 |    35  14.0       |
          7 |    39  15.6       |
          8 |    43  17.2       |

    (Joule (W*s) will look better for rs)

  NOTE: these numbers are ridiculous, but I need some point of reference for model checking

--- 13-10-22 ---

  reference for energy rates (Core i5-2520M , 2.50GHz):

    4W (1.6nJ per cycle) sleep power. 28W (11.2nJ per cycle) maximum power.
     ->  sleep

    $ sudo ./rapl-benchmark 2 10 --avoid-ht --ghz 2.5 --add-fetch-barrier 2 --add-fetch-wait-spinning 2 --add-fetch-contested 2 --add-fetch-uncontested 2 --super-wasteful-2 2
    # measureSleepPowerConsumption:
    # measurement t   1 wallSecs 10.000, totalPower 4.243 W
    # measureUncorePowerConsumption:
    # measurement t   1 wallSecs 10.781, totalPower 19.537 W
    # measurement t   2 wallSecs 11.667, totalPower 28.019 W
    add-fetch-barrier     t   2, reps  116442000, wallSecs 10.176 sec, totalPower 18.196 W, cycles/reps 218.485, nJ/reps 1590.208
    add-fetch-wait-spin   t   1, wallSecs 10.000 sec, totalPower 15.907 W
    add-fetch-wait-spin   t   2, wallSecs 10.028 sec, totalPower 21.894 W
    add-fetch-contested   t   2, reps  196608000, wallSecs 11.144 sec, totalPower 16.125 W, cycles/reps 141.707, nJ/reps 914.036
    add-fetch-uncontested t   2, reps 1413120000, wallSecs 10.939 sec, totalPower 20.350 W, cycles/reps 19.352, nJ/reps 157.528
    super-wasteful-2      t   2, reps  240615000, wallSecs 10.916 sec, totalPower 23.646 W, cycles/reps 113.423, nJ/reps 1072.795
    super-wasteful-5      t   2, reps  642219000, wallSecs 11.000 sec, totalPower 25.691 W, cycles/reps 42.819, nJ/reps 440.031
    dissemination         t   2, reps  195345000, wallSecs 11.000 sec, totalPower 24.501 W, cycles/reps 140.777, nJ/reps 1379.690
    # context t   2, sleepPower 4.242727 W, uncorePower 6.813289 W


    $ sudo ./rapl-benchmark 4 3 --ghz 2.5 --add-fetch-barrier 2 3 4 --add-fetch-wait-spinning 2 3 4 --add-fetch-contested 2 3 4 --add-fetch-uncontested 2 3 4 --super-wasteful-2 2 3 4 --super-wasteful-5 2 3 4
    # measureSleepPowerConsumption:
    # measurement t   1 wallSecs 3.000, totalPower 4.092 W
    # measureUncorePowerConsumption:
    # measurement t   1 wallSecs 3.927, totalPower 19.678 W
    # measurement t   2 wallSecs 4.386, totalPower 19.755 W
    # measurement t   3 wallSecs 4.604, totalPower 27.940 W
    # measurement t   4 wallSecs 4.761, totalPower 28.178 W
    add-fetch-barrier     t   2, reps  111837000, wallSecs 3.871 sec, totalPower 18.593 W, cycles/reps 86.541, nJ/reps 643.630
    add-fetch-barrier     t   3, reps   29397000, wallSecs 4.000 sec, totalPower 20.740 W, cycles/reps 340.168, nJ/reps 2822.072
    add-fetch-barrier     t   4, reps   22038000, wallSecs 4.000 sec, totalPower 21.930 W, cycles/reps 453.746, nJ/reps 3980.237
    add-fetch-wait-spin   t   1, wallSecs 3.002 sec, totalPower 16.074 W
    add-fetch-wait-spin   t   2, wallSecs 3.000 sec, totalPower 21.846 W
    add-fetch-wait-spin   t   3, wallSecs 3.000 sec, totalPower 21.959 W
    add-fetch-wait-spin   t   4, wallSecs 3.029 sec, totalPower 22.111 W
    add-fetch-contested   t   2, reps   98304000, wallSecs 4.330 sec, totalPower 14.319 W, cycles/reps 110.107, nJ/reps 630.647
    add-fetch-contested   t   3, reps   36864000, wallSecs 4.558 sec, totalPower 16.856 W, cycles/reps 309.081, nJ/reps 2083.914
    add-fetch-contested   t   4, reps   36864000, wallSecs 3.871 sec, totalPower 16.652 W, cycles/reps 262.552, nJ/reps 1748.818
    add-fetch-uncontested t   2, reps  184320000, wallSecs 3.379 sec, totalPower 15.729 W, cycles/reps 45.832, nJ/reps 288.361
    add-fetch-uncontested t   3, reps  208896000, wallSecs 4.021 sec, totalPower 20.544 W, cycles/reps 48.118, nJ/reps 395.410
    add-fetch-uncontested t   4, reps  208896000, wallSecs 4.004 sec, totalPower 20.552 W, cycles/reps 47.924, nJ/reps 393.973
    super-wasteful-2      t   2, reps  207018000, wallSecs 3.831 sec, totalPower 19.710 W, cycles/reps 46.264, nJ/reps 364.741
    super-wasteful-2      t   3, reps   63684000, wallSecs 4.000 sec, totalPower 25.296 W, cycles/reps 157.015, nJ/reps 1588.725
    super-wasteful-2      t   4, reps   50625000, wallSecs 4.000 sec, totalPower 27.056 W, cycles/reps 197.530, nJ/reps 2137.750
    super-wasteful-5      t   2, reps  293178000, wallSecs 4.000 sec, totalPower 20.450 W, cycles/reps 34.108, nJ/reps 278.999
    super-wasteful-5      t   3, reps   90966000, wallSecs 4.000 sec, totalPower 27.119 W, cycles/reps 109.930, nJ/reps 1192.463
    super-wasteful-5      t   4, reps   82884000, wallSecs 4.000 sec, totalPower 28.272 W, cycles/reps 120.647, nJ/reps 1364.384
    dissemination         t   2, reps  104778000, wallSecs 3.966 sec, totalPower 19.617 W, cycles/reps 94.633, nJ/reps 742.578
    dissemination         t   3, reps   34200000, wallSecs 4.000 sec, totalPower 25.304 W, cycles/reps 292.387, nJ/reps 2959.420
    dissemination         t   4, reps   29499000, wallSecs 4.000 sec, totalPower 27.806 W, cycles/reps 339.000, nJ/reps 3770.535
    # context t   4, sleepPower 4.092123 W, uncorePower 12.752734 W

--- 13-10-05 ---

 * ronny fancy seems now faster than ronny simple, due to the recent optimizations

--- 13-09-16 ---

 * decided to ditch memory manager, because
   * general:
   * shared memory:
     * inform is not a thing
     * invalidation should be a broadcast, not independt invalidate messages
     * ownership is not a thing in shared state
   * remote memory:
    * the idea of one variable shared among many in distributed setting is still weird (makes no sense to me)
    * validity, inform, invalidation, ownership don't exist
    * i don't know how to do distributed memory on this basis

   * shared memory seems doable, but being able to use it for distributed at the same time looks hard
   * many changes would be needed. Or maybe it is not possible at all.
   * it would not resemble anything near sasha's idea
   * therefore I thought it's no use "modifying" it

--- 13-09-11 ---

 □ dissemination with 2^i + 1 cores needs i+1 rounds. runtime increase is non-linear because of $\lceil ld~n \rceil$ rounds
   this is especially bad for 4->5 8->9 etc processes. because the communication overhead is similar to the next higher
   power of two ... there should exist ways to diminish the bad effect of this by grouping processes more cleverly

   in fact our barriers perform better in these non-power-of-two scenarios, than in power-of-two against dissemination,
   because they simply don't mind

   super-wasteful provides a more balanced scaling behaviour, where dissemination has a "jump" at power of two core counts

   dissemination could be improved by reducing the additional 1 round to as few processes as possible and then, with the
   new groups, continue with a power-of-two number of groups. Then you would not have the maximum of all threads doing the
   additional round, but only a few.

--- 13-09-10 ---

 * I changed super wasteful 5 aka Ronny's fancy barrier aka our-more-elaborate to have a different initial index
   other derived or similar barriers might not be up to date because of this change. Beware!

 * realization about super wasteful 5
  □ try using different increments of i (e.g. i = i*2. this doesnt work everytime. race. be cleverer)
  □ try randomizing the choice of i!
    (all to forego network contention)

--- 13-08-19 ---

 * started paper-push subdir (where ofcourse work directed to the paper will be kept)

--- 13-07-27 ---

 * added super wasteful variant 4, which reduces polling other thread by keeping track of each thread that has successfully arrived
 * added super wasteful variant 5, which minimizes polling by giving info about already seen threads to others as well

 □ true something write remotely only. perhaps adapt variant 5.
   (on the surface remote writing sounds quicker than remote reading)
   (atomic add or no atomic add?)

--- 13-07-13 ---

 * I cannot set the MPI_WIN_MODEL to MPI_WIN_SEPARATE and MPI_WIN_UNIFIED myself.
 * Mpich 3.0.4 can only do MPI_WIN_UNIFIED. The next mpich version will feature MPI_WIN_UNIFIED

--- 13-07-12 ---
--- 13-07-11 ---

 * add two super wasteful variations
  * super-wasteful-2 avoids resetting using a counter with add_fetch instead of a bit with store
    is faster than bit with store
  * super-wasteful-3 avoids resetting and memorizes the first couple of threads and sets i to this instead of -1

  * why is super wasteful so quick? REPORT

   All stores/add-fetchs can be done concurrently.
   Load spinning is inexpensive because each bit will only change once,
   thus the load is spinning on local cache except when at first load and when the bit gets flipped to one.
   In the classical add fetch barrier, the load spin gets disrupted by up to n-1 invalidates as well.
   So the sole reason seems to be the concurrently executing store/add-fetch.
   I also tried using RELEASE instead of ACQ_REL for the add-fetch instruction in the classical add-fetch barrier. Doesn't matter.

 * if we want to eveluate on the super-wasteful / shared memory barrier in general we have to:
  □ implement n-way dissemination and
  □ have a close look at http://spcl.inf.ethz.ch/Publications/index.php?pub=162 and compare

 * numbers(cycles) from my notebook (2 cores no HT) (remember-others-and-add-fetch-rapl add-fetch-(un)contested code has been used)

                                 uncontested    contested
    add-fetch        rel         18             130
    add-fetch        acq-rel     18             130
    add-fetch        seq-con     18             130
    store            rel         1              1.1
    store            seq-con     35             120
    load             acq         0.5            0.5
    load             seq-con     0.5            0.5
    store+load       acq-rel     1              2
    store+load       sec-con     35             165

  i am very unsure how to interprete them

--- 13-07-10 ---

 * We can propose a number of barriers for different software/hardware configurations. The possible configs are:
  * send/recv                 (blocking com)
  * isend/irecv + recv/send   (non-blocking com)
  * coherent RMA (main memory is 'eventually' updated by the network card)
  * separated RMA (main memory is only updated upon action)
  * lossy communication (e.g. UDP)

  For modelling and algorithm design irecv/isend might be equivalent or very similar to coherent RMA

 * write something about reinitialization(problems/advantages) of the barriers REPORT
 * write something about process counts that are not a power of two REPORT

 * dissemination barrier implementation oddity: REPORT
  * sendrecv in openmpi 1.7 /mpich 3.0.4 is blocking. i.e. the send is blocking.
    Better would be to use an isend + recv and not wait send to complete.
    Torsten Hoefler's RDMA dissemination barrier (http://htor.inf.ethz.ch/publications/index.php?pub=22) and
    the original 1988 dissemination barrier (http://m1.archiveorange.com/m/att/jgLs0/ArchiveOrange_quY2vZm8S65M1KcPpGIe9XOlXCYa.pdf)
    both do it 'correctly'.

    ✓ implemented it myself. It is not faster on my dualcore+HT notebook
    □ benchmark isend dissemination vs dissemination properly

 * idea what is bad about (isend) dissemination REPORT:

  phase 1:  0 -> 1 -> 2 -> 3 -> 4 -> 5 -> 6 -> 7 -> 0
  phase 2:  0 ------> 2 ------> 4 ------> 6 ------> 0
                 1 ------> 3 ------> 5 ------> 7 ------> 1
  phase 3:  0 ----------------> 4 ----------------> 0
                 1 ----------------> 5 ----------------> 1
                      2 ----------------> 6 ----------------> 2
                           3 ----------------> 7 ----------------> 3

  now suppose one process (e.g. 4) has not yet arrived

  phase 1:  0 --> 1 ---> 2 ---> 3 --> (4) -x> 5 ---> 6 ---> 7 ---> 0
  phase 2:  0 ---------> 2  --------> (4) -----x---> 6 ----------> 0
                  1  ---------> 3 ---------> (5) ----x----> 7 --------> 1
  phase 3:  0 ----------------------> (4) -----------x-----------> 0
                 1 ------------------------> (5) -----------x----------> 1
                        2 ------------------------> (6) ----------x----------> 2
                               3 ------------------------> (7) ---------x---------> 3

  who knows whom while process 4 is missing:
   =phase 1=             =phase 2=             =phase 3=             =the ideal=
   7 know themselves     + 4 new               + 0 new               maximum would be 49
   + 6 new                                     total is 17
   0: 0           7    0: 0         6 7    0: 0         6 7    0: 0 1 2 3 5 6 7
   1: 0 1              1: 0 1         7    1: 0 1         7    1: 0 1 2 3 5 6 7
   2:   1 2            2: 0 1 2            2: 0 1 2            2: 0 1 2 3 5 6 7
   3:     2 3          3:   1 2 3          3:     2 3          3: 0 1 2 3 5 6 7
   5:         5        5:         5        5:         5        5: 0 1 2 3 5 6 7
   6:         5 6      6:         5 6      6:         5 6      6: 0 1 2 3 5 6 7
   7:           6 7    7:           6 7    7:           6 7    7: 0 1 2 3 5 6 7

  -> If one process is missing, the number of blocked processes doubles every phase.
     maximum progress is ~2*(n) of a maximum of (n-1)*(n-1). (Perhaps find out the exact formulas)

     note that the sent but not recved isends are waiting for the missing process to arrive. so the progression as
     soon as the missing process arrives should be quick.

     maybe we can build an argument about worst/avg/best cases for barrier algorithms.
     Consider different times of process arrival for this.

     □ suggest improved algorithms that mitigate this problem. (Is it really a problem? I would say so.)

 * Torsten Hoefler makes heavy use of the logP model. And claims the normal algorithms to be optimal wrt logP.
   It seems though, that logP is far from a proper model for distributed barrier algorithms (and worse for shared mem (pub=162 below).
   Maybe investigate further and write up that we deliberately ignore this model and the "optimality claims",
   and try to improve existing barrier algorithms.
   (claims are e.g. in TH's diploma thesis or http://spcl.inf.ethz.ch/Publications/index.php?pub=162). REPORT

--- 13-07-06 ---

 * added shared memory barriere using add-fetch and each thread has its own counter (is slower than normal add-fetch)

 * realization
  * dissemination barrier sends n*log n messages to achieve a barrier completion in log n rounds.
  * a gather+broadcast barrier (assuming both are implemented as tree-esque sends) would send a total of 2*(n-1) messages
    and completes in 2*log n rounds

    gather e.g.:  +--------+        +--------+             round (2)
                  v        |        v        |
                  0 <- 1   2 <- 3   4 <- 5   6 <- 7        round (1)
                  ^                 |
                  +-----------------+                      round (3)

  * i.e. dissemination is not optimal wrt the number of sent messages

 * interesting MPI non-blocking calls:

  I(-,b,s,r)send
  Irecv

  Wait(-, Any, Some, All)
  Test(-, Any, Some, All)

  Cancel

  Notes:
   * blocking and non-blocking p2p comminication can be mixed (e.g. isend + matching recv)
   * non-blocking collectives are not interesting, because everyone has to take part and no cancelation of anything is possible.
     they are generelly restrictive (evaluate on this in the REPORT)

 ✓ interesting MPI RMA calls:
  InfoCreate
  InfoSet

  WinAllocate/WinCreate + no_locks + accumulate_ordering + accumulate_ops
  WinFree

  Put
  Get(, RGet)
  Accumulate + ordering restrictions (rar, war, waw, raw)
  GetAccumulate
  FetchAndOp
  CompareAndSwap

  WinFence
  WinFlush(Local)(All)
  WinSync

  Assertions: NoPrecede, NoSucceed, NoStore, NoPut

 ✓ familiarize yourself with MPI 3.0 RMA operations

 -> □ invent non-blocking-p2p and RMA barriers

--- 13-07-05 ---

 * implemented a new idea for a shared memory barrier called "super wasteful barrier"

  MPI 2.2 is bad for our purpose because:
   * put/get data may be not visible in main memory until a window is syncronized with fence/complete/wait
    * fence implies a barrier on the group
    * complete/wait is a local sync between fewer processes, but ultimately as bad as fence for our purpose
   -> motivate the consideration of 'MPI 3.0 only' in the REPORT

                       | 3.0 | 2.2
 ----------------------+-----+-----
  published            |sep12|sep09
                       |     |
  WinCreate            |  x  |  x
  WinAllocate          |  x  |  -
  WinAllocateShared    |  x  |  -
  WinSharedQuery       |  x  |  -
  WinCreateDynamic     |  x  |  -
  WinAttach            |  x  |  -
  WinDetach            |  x  |  -
  WinFree              |  x  |  x
                       |     |
  Put                  |  x  |  x
  Get                  |  x  |  x
  Accumulate           |  x  |  x
  GetAccumulate        |  x  |  -
  FetchAndOp           |  x  |  -
  CompareAndSwap       |  x  |  -
                       |     |
  RequestBasedRMA      |  x  |  -
                       |     |
  WinFence             |  x  |  x
  WinStart             |  x  |  x
  WinComplete          |  x  |  x
  WinPost              |  x  |  x
  WinWait              |  x  |  x
  WinTest              |  x  |  x
  WinLock              |  x  |  x
  WinLockAll           |  x  |  -
  WinUnlock            |  x  |  x
  WinUnlockAll         |  x  |  -
  WinFlush             |  x  |  -
  WinFlushAll          |  x  |  -
  WinFlushLocal        |  x  |  -
  WinFlushLocalAll     |  x  |  -
  WinSync              |  x  |  -
                       |     |
  OpenMPI       1.7    |  -  |  x
  MPICH         3.0.4  |  x  |  x
  MVAPICH2      1.9    |  x  |  x  (release may13, based on MPICH 3.0.3)
  Intel MPI     4.1    |  -  |  x  (release sep12, update june13, based on MPICH and MVAPICH, ref[1])
  Cray MPT      6.0.0  |  x  |  x  (release jun13, based on MPICH 3.0.3, ref[2], ref[3])
  IBM PE        v1.3   |  -  |  x  (source may13, ref[4]
  HP MPI               |     |     (couldn't find it. seems to have been sold.)
  SGI MPI              |     |     (couldn't find it)
  Bullx MPI            |  -  |  x  (based on OpenMPI, ref[6], ref[7])

  HPC Vendors of the current top500 (share of computers and performance in the june2013) :
   * 37.6% HP                * 32.7% IBM
   * 32.0% IBM               * 15.3% Cray
   * 09.8% Cray              * 14.0% HP
   * 03.6% SGI               * 03.8% SGI
   * 03.4% Bull              * 02.8% Bull

  ref claiming that OpenMPI powers big machines: http://blogs.cisco.com/performance/more-details-open-mpi-at-large-scale/

  refs:
   [1] http://software.intel.com/en-us/articles/intel-mpi-library-41-release-notes
   [2] http://docs.cray.com/books/S-9407-1306/
   [3] http://docs.cray.com/relnotes/
   [4] http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/index.jsp?topic=%2Fcom.ibm.cluster.pe.doc%2Fpebooks.html
   [5] https://www.google.de/search?q=bullx+mpi+3.0
   [6] http://www.google.de/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&ved=0CDYQFjAB&url=http%3A%2F%2Fwww.cenapad.ufmg.br%2Findex.php%2Fdocumentosemanuais%3Fdownload%3D2%3Abullx-cluster-suite-userguide-ingles&ei=n5TWUeSVLY2LswaJ8YHIBQ&usg=AFQjCNHhLOBJ7S2-Cix-iwoS3l-4peMDDQ&sig2=izW7DZA_VsHNHKPfnqRLoA&bvm=bv.48705608,d.Yms
   [7] http://www.google.de/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&ved=0CC4QFjAA&url=http%3A%2F%2Fsupport.bull.com%2Fdocumentation%2Fbyproduct%2Finfra%2Fsw-extremcomp%2Fsw-extremcomp-bas5xeon%2Fg%2F86Y222FA03%2F86A222FA03.pdf&ei=n5TWUeSVLY2LswaJ8YHIBQ&usg=AFQjCNFsuvPPxO_tuO-4is9fZ1qaTvKz6w&sig2=S6WDmcskMVwQq_UWgymBYA&bvm=bv.48705608,d.Yms

  * hardware specific implementations:
   * couldn't find anything special in MPICH, MVAPICH and OpenMPI
   * infiniband uses an RDMA based n-way dissemination barrier http://htor.inf.ethz.ch/publications/index.php?pub=22
   * other than that everyone uses usual (and RDMA based) dissemination or recursive doubling barriers, which are all slower than or as-fast-as the RDMA n-way dissemination described in the paper
   * A paper about dissemination barrier for shared memory (http://spcl.inf.ethz.ch/Publications/index.php?pub=162)

--- 13-06-29 ---
--- 13-06-28 ---

 * MPICH Barrier 3.0.4 implementation :
  * src/mpi/coll/barrier.c : MPIR_Barrier_intra
  * log2 Dissemination (k steps, 0<=k<=(ceil(log p)-1), proc i sends to proc (i + 2^k) % p and receives from proc (i - 2^k) % p)
   * "Two Algorithms for Barrier Synchronization," International Journal of Parallel Programming, 17(1):1-17, 1988

 * OpenMPI 1.7 Barrier implementation :
  * ompi/mca/coll/tuned/coll_tuned_barrier.c : ompi_coll_tuned_barrier_intra_recursivedoubling, ompi_coll_tuned_barrier_intra_bruck
  * n  = 2^k -> recursive doubling
  * n != 2^k -> bruck = dissemination with swapped receiver / sender procs
   * "Two Algorithms for Barrier Synchronization," International Journal of Parallel Programming, 17(1):1-17, 1988
  □ oddly, ompi-bruck = dissemination. bruck should communicate in the reverse direction. to be confirmed.

 * note: intra communicators are very different from inter communicators. we want intra!
  (□ maybe have a look at inter communicator barriers)
 
 -> we will use to the 2 way dissemination barrier to compare ours to

 * began rewriting the rapl-benchmark for mpi
  ✓ insert dissemination barrier instead of dummy

--- 13-06-25 ---

 * installed rapl measurement system (4 core sandbridge)

--- 13-06-24 ---

 * iterated on rapl benchmark
 * meeting with MV, SK
  * make it paperable is now priority 1 (beleg faellt schon ab, kein problem)
  * we don't need an add-fetch model right now
  * we want the distributed algorithm and a classical representative

--- 13-06-22 ---

 * thread pinning is now affinity aware (needed for venus)
 * add rapl benchmarks for:
  * uncontested __atomic_add_fetch
  * add-fetch wait spinning

--- 13-06-21 ---
--- 13-06-20 ---

 * as soon as you got access to a bigger system
  □ test rapl benchmark ronny-array with #define ARRAY_8
  □ make rapl benchmark measure multiple chips, not just cpu0

--- 13-06-19 ---
--- 13-06-18 ---
--- 13-06-14 + 15 ---
--- 13-06-13 ---

 * prepared and started taurus benchmarks
 * added taurus topology and beautified atlas topology

 * meeting with marcus haehnel
  * switching off powersaving
   * probably not possible on taurus
   * use "cpufreq-info" to observe current clock frequency
   * if powersaving is off on linux the wrong powersaving governor still does powersaving
     if /sys/devices/system/cpu/cpu0/cpufreq/scaling_driver = "pstate" it is the wrong governor

  * measurement using RAPL
   * per package, choice between with or without "uncore" - not clear what it means (e.g. with or without L3 Cache) - measuring the whole package probably the best choice
   * 1ms period with some deviation around measurement points
     -> use large intervals and use median of many repetitions
   * it is very hard to measure energy about cache state transitions, because you cannot redo benchmarks
     without 'preparing' the next run. You would need to measure the 'preparing' as well.
     preparing many cachelines and then start running is bad as well, because new cache effects might appear.
     generally every measurement that needs preparation and where a run is very short is impossible/hard/bad.
   * measuring the whole barrier is doable
  □ measure no-array- and array-ronny-barrier to see if there are large differences
    -> model arcordingly

--- 13-06-12 ---

 * new model minimization: remove backloop
   * steady state queries don't work anymore
   * use only from-initial-state queries

--- 13-06-07 ---

 * changed benchmark to measure cycles instead of nanoseconds
 * redid benchmarks
   * 100secs per measurement instead of 20, more datapoints, added plots, added topology picture of the measurement machine 'atlas'

--- 13-06-05 ---

 ✓ removed clraR 3, because (1) clr is not need for our purposes here (2) it is probably wrong (3) doesn't work for the one-loop variant because of the sync transition
 ✓ benchmark now uses a fixed amount of time (and counts repetitions), rather than a fixed amount of repetitions and measuring time
 ✓ added spin model of add-fetch barrier
 ✓ fixed two bugs in the ronny-barrier. both fail around 32+ threads. >>> one of them is probably the reason why the algorithm failed at 32 threads in the first report <<<

 ✓ did nice benchmarks up to 64 threads

 misc notes:
  ✓ it is possible to reduce the 2loop ronny model for non-ss queries:
   remove loc 10, 11 and set the while condition to 'true'
  □ one might also combine some of the transitions that are not in loops because they are triggered rarely

--- 13-05-17 ---

 * changed add-fetch benchmark to measure only the loop time, not the other uninteresting things
 * add array implementation (with flexible element size choice)
 * did some benchmarks (seems to scale not much worse than atomic ops)

 * add spin model for array version of the barrier

 todo:
  ✓ use aquire release mem model
  ✓ redo benchmarks (large deviations in runtime right now), and perhaps benchmark up to 64 threads
  □ model add-fetch barrier
    □ add energy rewards to add-fetch
    □ add energy rewards to ronny
    -> □ compare both
    -> □ line up measured data with both models

  □ interpret benchmarks. Complexity for both barriers?

 maybe todo:
  □ papi enable both barriers and make additional measurements
    cache stuff, wallclocktime/sleeptime, number of instructions
 
  □ put ronny barrier into libgomp and benchmark WITH backoff ->
    * argument that it is not considerably slower but doesnt need atomic ops foo
    □ quantify energy consumption?

--- 13-05-03 ---

 * implemented simple add-fetch-spin-wait barrier ( implementation/remember-others-and-add-fetch/* )
 * benchmarked ronny vs add-fetch (with $ time add-fetch ...)

--- 13-04-27 ---

 * implemented another simple model minimization (copy=0 as early as possible). semantically and quant. equivalent
 * new variant: remember whom you have seen: in models/prism/ctmc/with-cache + git/models/spin/remember-others

--- 13-04-26 ---
--- 13-04-15 ---
--- 13-04-13 ---

 grosser beleg meeting (baier, klueppel, tews):

 energie effizienz!
   * Beleg soll (irgendwie) in HAEC rein
   * RAPL-Zahlen nutzen

 algorithmus:
   * verteilte variante machen
     -> neues modell
   * andere varianten ausprobieren
   * parametrisieren
     * was parametrisieren?
     * Auswahl von energieeffizient bis performant

 modell:
   * wofuer?
     * gute parameter fuer den algorithmus aus dem modell ableiten
     * skalierung/performance mit mehr prozessen als messbar vorhersehen
     * argumentieren, dass der algorithmus gut ist
   * (system-/arbeits-)last nur sehr einfach modellieren. keine arbeit in gute last-modellierung stecken

--- before 13-04-01 ---

 □ make a distributed variant of the barrier
 
 ✓ go beyond 32 threads (i.e. use an array rather than one barrier variable)
 ✓ not only add yourself to the barrier, but everyone you have seen so far, too

 □ implement/model simple atomic increment barrier and compare against our protocol

 □ it might work to just remove copy and work directly on the shared vars.
   One more shared read, but a variable less (the model would be happy, perhaps)

 □ implement model minimizations
  -> □ scale model for more threads
  e.g.:
   ✓ remove second loop
   ✓ put cacheline state into a central object. e.g. in the case of modified, it is not needed to save the distinct state of the n-1 other threads because is implied.
   □ symmetry
   * partial order

 □ maybe model that during the work period the cacheline state might change

 actual implementation:
   □ try mwait, pause for the loops
   □ use back-off strategies
   □ address nthreads > maxthreads case
