--- 13-06-05 ---

 ✓ removed clraR 3, because (1) clr is not need for our purposes here (2) it is probably wrong (3) doesn't work for the one-loop variant because of the sync transition

--- 13-05-17 ---

 * changed add-fetch benchmark to measure only the loop time, not the other uninteresting things
 * add array implementation (with flexible element size choice)
 * did some benchmarks (seems to scale not much worse than atomic ops)

 * add spin model for array version of the barrier

 todo:
  ✓ use aquire release mem model
  □ redo benchmarks (large deviations in runtime right now), and perhaps benchmark up to 64 threads
  □ model add-fetch barrier
    □ add energy rewards to add-fetch
    □ add energy rewards to ronny
    -> □ compare both
    -> □ line up measured data with both models

  □ interpret benchmarks. Complexity for both barriers?

 maybe todo:
  □ papi enable both barriers and make additional measurements
    cache stuff, wallclocktime/sleeptime, number of instructions
 
  □ put ronny barrier into libgomp and benchmark WITH backoff ->
    * argument that it is not considerably slower but doesnt need atomic ops foo
    □ quantify energy consumption?

--- 13-05-03 ---

 * implemented simple add-fetch-spin barrier ( implementation/remember-others-and-add-fetch/* )
 * benchmarked ronny vs add-fetch (with $ time add-fetch ...)

--- 13-04-27 ---

 * implemented another simple model minimization (copy=0 as early as possible). semantically and quant. equivalent
 * new variant: remember whom you have seen: in models/prism/ctmc/with-cache + git/models/spin/remember-others

   n: 3, work: 0, read: 50, write: 100

          |dont remember others         |remember others
          |normal         one-loop      |normal         one-loop
   -------+-----------------------------+-------------------------
   states |259102         7789          |322957         10195
   trans  |777306         23199         |968871         30384
   matrix |336231         31085         |335075         33451

   clraR 1|300    copied  326           |262            296
   clraR 2|476    c       533           |441            496

--- 13-04-26 ---

 [..]

--- 13-04-15 ---

 paper meeting (baier,klueppel,tews,mc guire,mvoelp):

                 | las vegas  | monte carlo
 ----------------+------------+-----------------
 bounded runtime | variant y? | current barrier
 ----------------+------------+-----------------
 prob runtime    | pwcs       | variant x?
 ----------------+------------+-----------------
 energy effic    | ?          | ?


 possible topics
  * barrier construction (story?)
   * variants
  * model
  * analysis
   * garantierter fortschritt

  * energie
   * mwait/backoff
   * ???
   * ???
 
 schedule
  * 4-15 today
  ???
  * 4-22 small meeting
   * discuss variants
  ???
  * 4-29 bigger meeting
  ???
  * 5-13 to 5-30 writing

  * konferenz aussuchen

--- 13-04-13 ---

 grosser beleg meeting (baier, klueppel, tews):

 energie effizienz!
   * Beleg soll (irgendwie) in HAEC rein
   * RAPL-Zahlen nutzen

 algorithmus:
   * verteilte variante machen
     -> neues modell
   * andere varianten ausprobieren
   * parametrisieren
     * was parametrisieren?
     * Auswahl von energieeffizient bis performant

 modell:
   * wofuer?
     * gute parameter fuer den algorithmus aus dem modell ableiten
     * skalierung/performance mit mehr prozessen als messbar vorhersehen
     * argumentieren, dass der algorithmus gut ist
   * (system-/arbeits-)last nur sehr einfach modellieren. keine arbeit in gute last-modellierung stecken
 
 sonstiges:
  * die "kaputte" one-loop variante nicht untersuchen

--- before 13-04-01 ---

 □ make it work in a distributed setting
 
 ✓ go beyond 32 threads (i.e. use an array rather than one barrier variable)

 ✓ not only add yourself to the barrier, but everyone you have seen so far, too

 □ implement/model simple atomic increment barrier and compare against our protocol

 □ it might work to just remove copy and work directly on the shared vars.
   One more shared read, but a variable less (the model would be happy, perhaps)

 □ implement model minimizations
  -> □ scale model for more threads
  e.g.:
   ✓ remove second loop
   ✓ put cacheline state into a central object. e.g. in the case of modified, it is not needed to save the distinct state of the n-1 other threads because is implied.
   □ symmetry
   * partial order

 □ maybe model that during the work period the cacheline state might change

 actual implementation:
   □ try mwait, pause for the loops
   □ use back-off strategies
   □ address nthreads > maxthreads case
