--- 12-12-15  ---

 □ print out average cycles, standard deviation, ... to get a better feel what is going on with the code (maybe min and max, too)
 □ make sure you measure correctly!!!

 □ mail pending
   """
   [..]
   Beobachtungen:
    * Die Abweichungen sind ziemlich gross ... 50% ... manchmal 100% und noch hoeher

    * Signifikant steigt die Zeit nicht mit hoeherer Anzahl an Cores mit invaliden
   cachelines. Eher steigt sie gar nicht.

    * Die Anzahl der restlichen unbeteiligten threads (./scaling-scores.sh) ist wie erwartet
   (scheinbar) egal.

   Fragen:
    * (Disclamer: Es ist nicht unwahrscheinlich dass da noch Fehler drinnen sind)

    * Ich messe das dann naechste Woche automatisch, und errechne auch die
   Standardabweichung. Was koennte ich noch sinnvolles machen/ausrechnen?

    * Sollte ich den Test vielleicht mit mehreren Cachelines machen, damit groessere Zeiten
   herauskommen (die dauer einer messung liegt nur zwischen 50 und 500 Takten)? (Oder ist
   das wegen prefetching oder w/e) eher nicht sinnvoll?

    * Habt ihr sonst noch Hinweise fuer mich?
   """

--- 12-12-15 gist of last meeting ---

 * don't measure detailed model rates
 □ measure wether the number of to be invalidated cores influences the timing of a write operation
   (use atomic exchange for measurement, otherwise queueing and intelligence will make cache writes non-blocking ~)
 □ think of properties
 ✓ split the two writes into two locations in the current ctmc
 □ model cacheline validity to be able to assign rates for reads and writes based on the cacheline state

 Ronny's personal agenda:
  □ benchmark against gcc and intel openmp barriers (maybe more shared memory barriers? openmpi?)

--- 12-12-01 ---

 * fixed two huge mistakes in the old prism mdp model.
   luckily, this does not effect the correctness of the old modelchecking results,
   since the main results come from the spin model.
   i also rechecked that everything is alright with the corrected model.
 * shortened mdp (omitted unnecessary transitions now that we proved its correctness through a fine grained mdp model) -> dtmc
 * created ctmc stub with dummies for missing measured rates
 * open questions regarding the current ctmc in mail pending

   """
   1)
   Kann ich 2 -> 3 -> 4 eigentlich zu einer Transition zusammenfassen?
   In meinem Kopf wuerde das bedeuten, dass wenn die guard erfuellt ist er mit der work_rate
   aus der Schleife herausgeht.
   Was nicht ist was ich modellieren will. Sondern er soll "schnell" herausgehen und danach
   die Transition mit der niedrigen work_rate machen.
   Aber eventuell verstehe ich hier noch die CTMC-Semantik falsch.

   2)
   So 100%ig genau ist das nicht was da steht. Es werden noch mehr Takte benoetig um bei
   einigen Operationen, aber ist das grob ok so?
   Ich kann die Messungen so machen dass alles was am Kanten-Label steht gemessen wird. Das
   sollte ja reichen, oder?

   3)
   Fast immer ist die tatsaechliche Rate an einem Punkt abhaengig vom Cacheline-Zustand!
   Sollte ich den Cache modellieren (und wenn ja, wie detailiert? Und hat jemand so etwas
   schonmal gemacht wo ich mir inspiration holen kann?), oder Messe ich den Algorithmus als
   solches und bestimme eine einzelne Rate die der durchschnittlichen Takt-Anzahl (bzw
   takt_periode / takt_anzahl) der jeweiligen Operation entspricht?

   4)
   Wenn ich den Cache nicht direkt modelliere brauche ich streng genommen auch nicht die
   genauen Zahlen fuer reads/writes in Abhaengigkeit vom Cacheline-Zustand, richtig?

   5)
   "exit:=0; left:=false" (2->3 transition) braucht 1 oder 2 writes je nachdem ob sie auf
   der selben Cacheline liegen, richtig? Also ist das hier vmtl. 1*write_rate, oder?

   6)
   Sollte ich bei den Raten auch Verteilungen verwenden? Wir hatten ja sowas gesagt wie
   20-50 Takte fuer ein cache_read_miss und die Arbeitsperiode soll 1000-10000 takte lang
   sein.

   7)
   In der Doku von prism steht dass der Scheduler (fuer parallele Komposition) anhand der
   Raten schedulet. Das heisst vermutlich, dass ich auch den Scheduler selbst modellieren
   muss, oder? Weil sonst waere das doch wie als ob alle auf dem selben core sind? Nee eigtl
   nicht. Mh. Keine Ahnung, ehrlich gesagt.

   8)
   Und properties macht man genau wie in DTMCs? Koennt ihr mir ganz kurz schildern, was dann
   anders aussehen wird als mit DTMCs?

   9)
   Ansonsten wollte ich generell fragen ob ich auf dem richtigen Weg bin, bzw. Tipps haben.
   Ich mach das hier ja zum ersten mal.
   """

--- 12-11-24: notes from the last meeting ---

 ✓ show the model's non-blocking (with fairness for the parallel composition) with an LTL supporting modelchecker
   (prism can only handle probabilistic queries)
   ✓ also try 4 or 5 processes just to be sure - used "□◇(left==true) ∧ □◇(left==false)"
   ✗ and/or prove it by hand (maybe later, for the report)

 ✓ tighten/reduce the model if you are sure no properties are lost
   ✓ remove non-det while/if
   ✓ unify previously split commands, perhaps

 ✗ get the required numbers for the ctmc model
   * approx:
     * work period 1000-10000 cycles
     * shared memory read w/ invalid cache 20-50 cycles
     * shared memory read w/ valid cache ?? cycles
     * shared memory write w/ copied cache lines 100 cycles
     * shared memory write w/o copied cache lines ?? cycles
     * maybe differ between the numbers of cores that have to invalidate their cache lines
       * if the differences are large enough and
       * if the model does not grow a too much

   * mind:
     * switch off powersaving / turboboost / hyperthreading
     * check the assembly/objdump of your benchmark binaries
     * measure rdtsc alone
     * rdtsc's measuring point is in the middle of the command
     * rdtsc might be reordered in the cpu
     * rdtsc commands without using the returned values might make the compiler remove the rdtsc command
     * rdtsc code:
       asm volatile("rdtsc" : "=a" (lower), "=d" (upper));
       uint64_t cycles = (((uint64_t)lower) | (((uint64_t)upper) << 32));
     * make sure no unnessecary stuff happens during the measurement (read the asm)
       watch out for sp/esp/rsp (stackpointer) usage (shouldn't do that)

 □ try variations of the implementation (don't model it, though)
   * using mwait or usleep after a write of the shared variable

 * future: create a distributed memory push only version of this (promissing!)

--- 12-11-17 ---

 * fixed huge bug in the model (initialized the entry/exit barrier wrongly)

 * Probing for undesired behaviour

   non-det scheduling
   fairness enabled (strong fairness)
   
   // correctness of the locations (no racing through it is possible)
   ✓ Pmax=? [F "invalid_locations"] = 0.0

   // the barrier bitmask will never be invalid
   ✓ Pmax=?[F "invalid_barrier"] = 0.0

   // if one of the processes got out of the entry/exit barrier, all of them will
   ✓ Pmax=? [F ((p1_between|p1_exit_barrier)&((p2_entry_barrier&(G p2_entry_barrier))|(p3_entry_barrier&(G p3_entry_barrier))))] = 0.0
   ✓ Pmax=? [F ((p1_before|p1_entry_barrier)&((p2_exit_barrier&(G p2_exit_barrier))|(p3_exit_barrier&(G p3_exit_barrier))))] = 0.0
   ✓ Pmax=? [F ((p2_between|p2_exit_barrier)&((p1_entry_barrier&(G p1_entry_barrier))|(p3_entry_barrier&(G p3_entry_barrier))))] = 0.0
   ✓ Pmax=? [F ((p2_before|p2_entry_barrier)&((p1_exit_barrier&(G p1_exit_barrier))|(p3_exit_barrier&(G p3_exit_barrier))))] = 0.0
   ✓ Pmax=? [F ((p3_between|p3_exit_barrier)&((p1_entry_barrier&(G p1_entry_barrier))|(p2_entry_barrier&(G p2_entry_barrier))))] = 0.0
   ✓ Pmax=? [F ((p3_before|p3_entry_barrier)&((p1_exit_barrier&(G p1_exit_barrier))|(p2_exit_barrier&(G p2_exit_barrier))))] = 0.0


  * Characterization of the Constantly Resetting Race

    I attempt to characterize it to be able to check wether there are other unexpected races,
    besides this "constantly resetting race"

    At the moment I cannot find it.
    And prism says Pmax=? [F G p1_entry_barrier] = Pmax=? [F G p1_exit_barrier]= 0.0

    Pmin=? [G F p1_between] = 1.0

    So it might not even exist.
    * (make sure)


--- 12-11-03 16:40 ---

 ✓ get to know ctmc s
   ✓ create a ctmc model for timing and probabilistic analysis
     □ script model generation, and perhaps property generation
 ✓ think of / make up useful properties and test them
   ✓ show the absence of other deadlocks than the "constantly resetting race"
 ✓ find a quad or more core (w/o HT) machine and check the cache impact there
   * (fyi) dual core HT notebook CPU uses about 1% of its cache bandwidth per core.
     so it is probably 2% used cache bandwidth in total for my 2core+HT notebook.


--- 12-11-03 11:30: Gist of the last meeting ---

near TODO:
 ✓ switch to bitmask from primes
 ✓ reuse the local copy instead of using two copies
 ✓ switch to MDP and introduce non-determinism in while statements where appropriate

not as near TODO:
 ✓ (copied to above) switch to ctmc
   □ (copied to above) script model generation, and perhaps property generation

far TODO:
 ✓ model timing differences, work periods and cache latency/behaviour, too

general possible directions (markus/sascha):
 ❏ timing analysis
 ✗ "constantly resetting race" -> prove/account for the assumption behind pW/CS (there is no constantly resetting race)
 ❏ methodolgy/tools support -> generalize modelling to also be able to check other similar algorithms
 ❏ 512 processes

possibly/unimportant TODO:
 ❏ properly implement the barrier with a switch where it decides itself which of the two phases/while loops to enter

dont forget TODO:
 ❏ optimization: processes memorize who they already saw and add them (besides themselves) to the barrier (don't model it, though)
 ✓ (copied to above) think of and test properties/queries and test them
   ✓ (copied to above) show the absence of other deadlocks than the "constantly resetting race"
 ✓ (copied to above) measure cache impact (need a proper >4 multi core (without HT) machine for this)
   * (fyi) inside one HT-able core there occur simply no cache misses, since both
     threads use the same execution units/cache and everything

misc:
 ✓ assume core count >= thread count for modelling
   This simplifies the real implementation of the algorithm. If core count < threads
   the scheduler would dissrupt the efficiency/workings of the algorithm very very much
 *
   cp := orig              <
   if i am not in cp {     < model timing differences here, too
     add me to cp          <
     orig := cp
   }

--- 12-10-27 ---
 * hg in Betrieb genommen
 * Modell korrigiert / verfeinert
 * von ascii art auf dot-file fuer das visuelle Modell gewechselt
 * aufgeraeumt
 * erste kleine properties eingefuegt (unmoegliche states und sowas)

vorher:
1)
if one threads gets his value overwritten by another and sees that another threads multiplicated value is missing,
he should multiply this value too, not only his own.

2   3   5
|   |\  |
2   | \ |
| \ 6  \|
|   |   5
|   | / |
|   30  |
|   |   |

without this mechanism thread 3 would write 15, because the multiplying of 2 has not been registered by 5

-----------------------------------------------------------

2)
use a bit array instead of an integer and primes.

every odd bit marks the entered state of a thread.
every even bit marks the left state of a thread.

initial state:           00 00 00 00
everyone arrived:        10 10 10 10
everyone reached the end 11 11 11 11

if one state reaches the 11 11 11 11 state it sets it to 00 00 00 00

the above mechanism is already a work around for a race. the same race as with using multiplication and division
for primes.

-----------------------------------------------------------

Ich hab das Beispiel gefunden.

(das spielt sich alles in der entry loop ab)

2   3   5
|   r1  r1
|   |   w5
r5  |   |
|   w3  |
|   |   r3
w10 |   |
|   |   |
|   r10 |
|   w30 |
r30 x   |
x       |
        w15
        deadlock

Das bedeutet gleichzeitig dass ein "resetten" der entry barrier
auf 30 (nachdem der erste thread raus ist aus der loop) nicht gehen wuerde,
weil nr 5 danach immernoch 15 schreiben
kann und der selbe deadlock entsteht.

wenn man statt dem "resetten" eine extra variable nimmt, kann das nicht passieren.

email 12-10-09 18:31:
> Hallo,
> 
> die Zahlen fuer hohe Thread-Anzahl koennte man auch bessern indem man in der 
> schleife den thread "yielded" ... in anderen Sprachen/Bibliotheken kann man 
> sagen, dass man erstmal keine rechenzeit braucht und die anderen weitermachen 
> duerfen (nicht sicher ob pthread es auch kann. Ich nehme an: ja). Das duerfte 
> das Problem ab 5 threads mildern oder beheben. (Was vermutlich passiert ist, 
> das einer bis oder vier fuer sich immer wieder multiplizieren und pruefen und 
> der 5te lang nicht dran kommt oder sich unguenstig abwechselt -- sodass die 
> die 4 aktiven quasi sinnlos pruefen und multiplizieren ... weil ohnehin ein 
> anderer fehlt).
> 
> 
> 
> Die dritte Variable weglassen geht in dem Code wirklich nicht. (Die ist erst 
> dadurch ueberhaupt dazugekommen)
> 
> Ich wollte es dir gerade nochmal beschreiben, aber jetzt faellt mir auf, dass 
> der deadlock scheinbar anders zustande kommt. Aber er passiert definitiv.
> 
> ("someoneLeftTheEntryBarrier" ueberall aus dem quellcode loeschen (und aus dem 
> while bedingungen die und klausel auch wegmachen) und compilieren)
> 
> 3 threads, 1000 iterationen.
> 
> 5 bleibt im entry loop mit entryBarrier = 10.
> 2, 3 sind im exit loop mit exitBarrier = 6.
> tot.
> 
> Aber ich versteh auch gerade nicht warum, und muss jetzt auch los^^.
> 
> Ich hoffe ich konnte helfen.
> 
> gruesse und danke fuers angucken des codes.
> Ronny    
> 
> On Tuesday, October 09, 2012 16:39:42 Marcus Völp wrote:
> > Hi,
> > die Zahlen sehen vielversprechend aus. Ich schaue mir heute Abend bzw.
> > Morgen mal Deinen Code im Detail an. Deinen Deadlock sehe ich nicht auf
> > anhieb aber Michael Scott hat auch mal gesagt, das man pro lockfreiem
> > Algorithmus einen PhD braucht. Insofern ist das nicht verwunderlich.
> >    
> >     Marcus
> > 
> > On 10/09/2012 03:42 PM, Ronny Brendel wrote:
> > > So, ich habe mit pthread_barrier_* das gleiche nochmal implementiert
> > > und mal beides gemessen.
> > > (mit "time (real)" ueber die gesamte Programmlaufzeit)
> > >
> > > # 2 threads 1mio iterations
> > > pthreads: 16.103s
> > > neu: 0.292s
> > >
> > > # 3 threads 1mio iterations
> > > pthreads: 29.290s
> > > neu: 0.561s
> > >
> > > # 4 threads 400k iterations
> > > pthreads: 17.102s
> > > neu: 0.336s
> > >
> > > # 5 threads 300k iterations
> > > pthreads: 16.979s
> > > neu: wird niemals fertig (100 iterationen dauern 4 sekunden)
> > >
> > > zwischen 5 und 15 threads (was das Maximum wegen Primzahlen und 64bit
> > > integer ist) aendert es sich nicht mehr so stark ... circa Verdopplung
> > > der Zeit die fuer 100 Iterationen benoetigt wird.
> > >
> > > Scheinbar liegt es ihm am Herzen auf den 4 cores (2 cores+HT)
> > > gleichzeitig laufen zu koennen.
> > >
> > >
> > > Interessant.
> > >
> > > Also fuer wenige Threads ist es Wahnsinnig schnell, aber wenn die
> > > gegenseitigen Ueberschreibungen sich haeufen wird es schnell
> > > schlimmer. (wie erwartet)
> > > Aber der Sprung von 4 auf 5 threads. ist ein besonders krasser
> > > Unterschied auf meinem laptop.
> > >
> > > disclamer:
> > > Die Messung ist natuerlich nicht sehr schoen, weil noch ein paar
> > > kleine Einfluesse eine Rolle spielen (zB ein bisschen
> > > commandozeilen-output, eine bessere Uhr koennte man benutzen, die
> > > Initialisierungs und Abbau-Phase sollte man nicht mitmessen). Aber im
> > > groben sollte es stimmen, was da steht.
> > >
> > > ps: im code unten ist noch ein kleiner bug gefixt der verhindert hat
> > > mehr als 9 threads zu benutzen (integer statt int64).
> > >
> > >
> > > gruesse!
> > > Ronny
> > >
> > > Quoting Ronny Brendel <ronny.brendel@tu-dresden.de>:
> > >
> > > > Da ich den Markus gerade nicht erwischt schreib ich mal ne mail.
> > > >
> > > > Hallo,
> > > >
> > > > ich habe jetzt eine scheinbar funktionierende Implementation der
> > > > Barrier mit 2 Variablen, plus einer extra.
> > > >
> > > >
> > > >         int64_t entryBarrierCopy = 1;
> > > >         c->entryBarrier = 1;
> > > >
> > > >         /* Entry Loop */
> > > >         do {
> > > >             entryBarrierCopy = c->entryBarrier;
> > > >             if (entryBarrierCopy % prime != 0) {
> > > >                 entryBarrierCopy *= prime;
> > > >                 c->entryBarrier = entryBarrierCopy;
> > > >             }
> > > >         }while (entryBarrierCopy < productOfAllPrimes &&
> > > > c->someoneLeftTheEntryBarrier == 0);
> > > >
> > > >
> > > >         c->someoneLeftTheEntryBarrier = 1;
> > > >
> > > >         int64_t exitBarrierCopy = 1;
> > > >         c->exitBarrier = 1;
> > > >
> > > >         /* Exit Loop */
> > > >         do {
> > > >             exitBarrierCopy = c->exitBarrier;
> > > >             if (exitBarrierCopy % prime != 0) {
> > > >                 exitBarrierCopy *= prime;
> > > >                 c->exitBarrier = exitBarrierCopy;
> > > >             }
> > > >         }while (exitBarrierCopy < productOfAllPrimes &&
> > > > c->someoneLeftTheEntryBarrier == 1);
> > > >
> > > >         c->someoneLeftTheEntryBarrier = 0;
> > > >     }
> > > >
> > > >
> > > > Notes:
> > > > 1)
> > > > die *Copy-Variablen existieren, weil wenn ich statt
> > > >
> > > >             entryBarrierCopy = c->entryBarrier;
> > > >             if (entryBarrierCopy % prime != 0) {
> > > >                 entryBarrierCopy *= prime;
> > > >                 c->entryBarrier = entryBarrierCopy;
> > > >             }
> > > > ---
> > > >             if (c->entryBarrier % prime != 0) {
> > > >                 c->entryBarrier *= prime;
> > > >             }
> > > >
> > > > mache passiert der if-Vergleich und die Multiplikation eventuell nicht
> > > > auf dem gleichen Datum und es kann zB passieren dass eine Primzahl
> > > > zweimal auf die barrier-Variable multipliziert wird. (Ich denke das
> > > > kann dazu fuehren dass die barrier ihr limit erreicht obwohl in
> > > > wirklichkeit nur 1 thread 3 mal eingecheckt hat)
> > > >
> > > >
> > > > 2   3   5
> > > > ---------
> > > > r1  r1  |
> > > > w2  |   r2
> > > > |   w3  |
> > > > r3  |   |   <- barrier % 2 != 0
> > > > |   |   w10
> > > > r10 |   |
> > > > w20 |   |   <- barrier *= prime  // 20 = 2 * 2 * 5
> > > >
> > > > XXX
> > > >
> > > >
> > > > Ich habe deswegen mich ein bisschen zu cmpxchg belesen und versucht
> > > > rauszufinden was der compiler macht. if () mit dem *= ist scheinbar
> > > > kein Kandidat fuer diese atomare Operation. Der assembler output des
> > > > gcc ist auch frei von cmpxchg-Befehlen (auch den 64bittigen) .
> > > >
> > > >
> > > > 2)
> > > > Wenn ich das someoneLeftTheEntryBarrier-Zeug weglasse gibt es einen
> > > > deadlock.
> > > > Der erste thread der aus der entry loop rausrennt kann eventuell seine
> > > > (kurze) Anwesenheit den anderen nicht mitteilen weil seine
> > > > schreiboperation von einem anderen thread direkt ueberschrieben wird.
> > > >
> > > > Sodass am Ende ein thread in der exit loop und alle anderen in der
> > > > entry loop haengen.
> > > >
> > > > Dieses flag stellt sicher, dass wenn der Erste die entry loop
> > > > verlaesst es die anderen threads merken und die entry loop verlassen
> > > > koennen.
> > > >
> > > > Solange nicht alle threads in der der exit loop angekommen sind geht
> > > > keiner heraus. Wenn der erste rausgeht setzt er das flag zurueck
> > > > (analog zur entry loop) sodass erstens keiner vorzeitig aus der entry
> > > > loop in der naechsten iteration heraus kann bis alle angekommen sind,
> > > > und zweitens es alle merken wenn der erste die exit loop verlaesst
> > > > (selbes problem wie bei der entry loop).
> > > >
> > > > ----
> > > > Meine Tests (von 1 bis 9 threads mit beliebig vielen iterationen ---
> > > > also wiederholtem nutzen der selben barriere) funktionieren alle bisher.
> > > >
> > > >
> > > > Die Frage ist: Was nun?
> > > > Besonders elegant sieht es nicht aus. Gibt es noch Sachen die ich
> > > > veraendern kann / sollte?
> > > >
> > > > Ich koennte auch spasseshalber mal gegen die pthread_barrier
> > > > benchmarken. (Das geht sicher auch fix)
> > > >
> > > > Wie kann ich herausfinden ob das Ding wirklich deadlock-frei ist?
> > > > (Beziehungsweise nur der Fall des zyklischen Austauschens der
> > > > barrier-Variable uebrig bleibt (der laesst sich scheinbar nicht
> > > > verhindern, richtig? und er sollte sich auch "irgendwann" selbst
> > > > erledigen, wenn er oft genug versucht auszutauschen. mh obwohl.)
> > > >
> > > > Mich ins modellieren einlesen und die barrier erstmal so lassen waere
> > > > auch eine Moeglichkeit.
> > > >
> > > >
> > > > Meinungen?
> > > >
> > > > viele Gruesse,
> > > > Ronny
> > >
> > >
