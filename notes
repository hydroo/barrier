--- 13-04-27 ---

 * new variant: remember whom you have seen: git/models/spin/remember-others

--- 13-04-26 ---

       |normal-no-work one-loop-no-work
-------+-------------------------------
states |364701         10795
trans  |1094103        31647
matrix |438694         39024

clraR 1|304            326
clraR 2|487            533
clraR 3|414            177
SS m   |0.076          0.291
SS s   |0.617          0.100
SS i   |0.307          0.608

deutet auf fehler im normal-no-work, oder?


--- 13-04-15 ---

 paper meeting (baier,klueppel,tews,mc guire,mvoelp):

                 | las vegas  | monte carlo
 ----------------+------------+-----------------
 bounded runtime | variant y? | current barrier
 ----------------+------------+-----------------
 prob runtime    | pwcs       | variant x?
 ----------------+------------+-----------------
 energy effic    | ?          | ?


 possible topics
  * barrier construction (story?)
   * variants
  * model
  * analysis
   * garantierter fortschritt

  * energie
   * mwait/backoff
   * ???
   * ???
 
 schedule
  * 4-15 today
  ???
  * 4-22 small meeting
   * discuss variants
  ???
  * 4-29 bigger meeting
  ???
  * 5-13 to 5-30 writing

  * konferenz aussuchen

--- 13-04-13 ---

 grosser beleg meeting (baier, klueppel, tews):

 energie effizienz!
   * Beleg soll (irgendwie) in HAEC rein
   * RAPL
     ✗ leider, keine Moeglichkeit nachzuahmen (ausser memory/dram: paper "RAPL: Memory ...")
       □  aber es gibt projekte wie zB mummi die Modelle erstellen (paper "Power-Aware Predictive ...")
   * messen am board mit ~100ns moeglich

 algorithmus:
   * verteilte variante machen
     -> neues modell
   * andere varianten ausprobieren
   * parametrisieren
     * was parametrisieren?
     * Auswahl von energieeffizient bis performant

 modell:
   * wofuer?
     * gute parameter fuer den algorithmus aus dem modell ableiten
     * skalierung/performance mit mehr prozessen als messbar vorhersehen
     * argumentieren, dass der algorithmus gut ist
   * (system-/arbeits-)last nur sehr einfach modellieren. keine arbeit in gute last-modellierung stecken
 
 sonstiges:
  * die one-loop variante nicht untersuchen

--- before 13-04-01 ---

 □ make it work in a distributed setting
 
 □ go beyond 32 threads (i.e. use an array rather than one barrier variable)

 □ not only add yourself to the barrier, but everyone you have seen so far, too

 □ implement/model simple atomic increment barrier and compare against our protocol

 □ it might work to just remove copy and work directly on the shared vars.
   One more shared read, but a variable less (the model would be happy, perhaps)

 □ implement model minimizations
  -> □ scale model for more threads
  e.g.:
   * remove second loop
   * put cacheline state into a central object. e.g. in the case of modified, it is not needed to save the distinct state of the n-1 other threads because is implied.
   * symmetry
   * partial order

 □ maybe model that during the work period the cacheline state might change

 actual implementation:
   □ try mwait, pause for the loops
   □ use back-off strategies
   □ address nthreads > maxthreads case
