--- 13-03-20 ---

 □ implement atomic increment barrier and compare against our protocol

--- 13-03-09 ---

 □ it might work to just remove copy and work directly on the shared vars.
   One more shared read, but a variable less (the model would be happy, perhaps)

--- 13-02-09 ---

 ✓ is the model still correct?

 done:
 * began a correctness check on 4 processes on yoda.inf.tu-dresden.de
 * cache test on yoda.inf.tu-dresden.de
 * update experiment with 3 processes according to the new/modified model

 todo:
 □ experiments with one loop and varying working periods
   ~failure probability
 □ begin planning the report and slides
 □ if possible do experiments with 4 processes on yoda
 * ignore further reductions (?maybe not?)

--- 13-02-08 ---

 * bulldozer and sandybridge 33+ processes can cause a race to appear, barrier breaks
  (some are out, some are not ... deadlock for those that are not)
   not yet researched what exactly goes wrong, but it is certainly because
   the committing of only 32bit to the shared variable at one moment in time

 done :
  * add an openmp, pthread, ronny barrier micro benchmark
  * make a oneloop version of the ctmc
  * script query generation
  * add begin of report and presentation
  * fix: add a missing cp_1'=value and saved an insane amount of states
    500k instead of 2mio for 3 processes
    ✓ 4/5 processes might now be feasible 

--- 13-02-07 gist of the last meeting ---

 * quantitatively analyze one loop without the second and varying work periods
   interesting queries and numbers because this version is not terminating,
   but a stochastic algorithm with failure probability
 * scale on more cores
 □ do a presentation on implementation/benchmarking and theory/modelling
 □ write a report
 ✓ saturday: measure scaling-cores anew and try non-idling and idling cores as part-takers
   marcus hypothesis is that powered down cores slow down atomic ops, where as busy cores do not
   (they don't actively take part in anything and don't communicate, ofc)

     they busy wait all the time

 ✓ maybe redo benchmarks where you make sure that the barrier is started anew when all threads left the previous one
   barrier-attempt/benchmark/*

--- 13-02-04 ---

 * began scripting the model. can now reproduce the old model for 3 and 2 processes correctly
  ✓ clean up
  ✓ generate queries depending on the process count

  ✓ next step is to think of reductions and implement them
   ✓ oneloop version

 some queries and results (3 processes):
  * expected value for one process to get behind the first barrier = 575 ticks
  * all processes = 683.4144882738982
  * one process is exactly behind the barrier to all of them are = 204 ticks

--- 13-01-26 ---
 * gist of the last meeting
  □ implement model minimizations that do not change the specific query answers, but would break other properties
    e.g. remove the second half of the algorithm if possible
   □ scale model for more threads
    ✓ scripting is probably necessary for the scaling
   -> measure new things that were not possible before the better scaling

 * Important realization: barrier implementations might forbid entering a new barrier
   before the last barrier of the team in question has been left by all participants.
   my barrier does not have this "limitation"
   when benchmarking keep in mind that our barrier makes progress while in other implementations still wait
   for the previous barrier to finish.

 * done today:
  * mini changes to the model
  * some new / different queries
  * 2 new experiment graphs for getting behind the entry and exit barrier
   ✗ "l_*>=11" shows odd behaviour ... why is that? (because it should have been l_1 <=5 | l_1>=11)

--- 13-01-22 ---
 * 2 processes - ss prob: m=0.431, s=0.138, i=0.431
 * 3 processes - P=? [ true U<=(500/2500) l_1=5 ] - sparse - gauss-seidel - 1min - 0.42
 * 3 processes - S=? [ "shared_1" ]  - sparse - gauss-seidel  - 15min - 0.401
 * 3 processes - S=? [ "modified_1" ] - sparse - gauss-seidel - 15min - 0.143
 * 3 processes - S=? [ "invalid_1" ] - sparse - gauss-seidel - 15min - 0.455

--- 13-01-21  ---
 * bug fix (a wrong rate)
 * left is now a synced local variable. This saves ~ 1/7th of the state spaces.
 * reordered variables to reduce the mtbdd by 1%

 * 3 processes - P>=1 [ G F l_1=17 ] (MC-DRA product) - 511.272s
 * 3 processes - S=? [ "modified_1" ] - a long time >=10min ... more like 30min? - "10k iterations is not enough"

--- 13-01-19 ---

 * modelchecking 3 processes takes too long (building model 3 minutes), modelchecking >20min perhaps way more. (4000 states for 2 processes, 5mio for 3 states)
  ✓ perhaps make a version with "assign labels" in order to remove the dotted states


 * what are interesting questions? (--> queries)
  * how long does it take to get past the first barrier?
    (second is symmetric and therefor not really interesting?)
  * how much is the cache strained?
    (is not much because false sharing has a limit to its speed, and therefor
     only uses a measured sub 2% of the cachebandwidth)
  * in which states is the cache how much of the time?
  * more?

 * rather benchmarking than modelchecking?

questions:
 ✓ how do the rates influence the time modelchecking (or building) takes?
  * convergence behaviour is dependend on the numbers itself
    no way to predict this or to influence it directly, though
 ✓ next steps
  * goal for complexlab and lab!
  □ paper? = probably
  * benchmarking?
  * actual implementation? test mwait and whatnot
   * contributing to gnu openmp?

--- 13-01-14 ---
 □ don't forget to model that work periods might change the cacheline's state.
   Or maybe model it as a self loop in the cacheline to make it "randomly" invalidate itself
   with a certain probability/rate

 TODO:
 * ~/desktop/barrier/barrier-attempt/model-prism-ctmc-with-cache-invalidation/model
  ✓ model cache behaviour
   ✓ add validity state
   ✓ split writes into two transitions so that we do not modify global state in a synchronous transition (forbidden in prism)
   ✓ add synchronous transition labels to the process
   ✓ test exhaustively

 DONE:
 * cleaned up spin model

 * moved "entry := 0" to the end of the algorithm / model, because it belongs after the second loop and thusly
   inside the else if branch in a unified entry (this wouldn't work without moving it to the end).
   The model is still correct.

 * using if (left==false) and else if (left==true) we can indeed decide correctly in which
   barrier part we are ("model-spin-unified-entry-point")
   thus we can nicely implement this algorithm without using two barriers for one round. yay!

 * old ctmc
  * bugfix
  * slight changes
  * split the else branch of "while()" into two transitions to model
    that a cacheline is invalidated between the cacheread of "left==false" and the cachewrite on "exit := 0"

 * new ctmc "model-prism-ctmc-with-cache-invalidation"
   * began working on integrating the cacheline state into the model

--- 13-01-12 ---

 * see cache-invalidation-on-different-core-counts/observations
   in short: the number of invalidatees influences the time needed,
   but I cannot quanitify it

--- 12-12-15 ---

 ✓ print out average cycles, standard deviation, ... to get a better feel what is going on with the code (maybe min and max, too)
 ✓ make sure you measure correctly!!!

 ✗ mail pending
   """
   [..]
   """

--- 12-12-15 gist of last meeting ---

 * don't measure detailed model rates (it would have been to cumbersome and error prone to measure so tiny amounts of time)
 ✓ measure wether the number of to be invalidated cores influences the timing of a write operation
   (use atomic exchange for measurement, otherwise queueing and intelligence will make cache writes non-blocking ~)
 ✓ think of properties
 ✓ split the two writes into two locations in the current ctmc
 ✓ model cacheline validity to be able to assign rates for reads and writes based on the cacheline state

 Ronny's personal agenda:
  ✓ benchmark against gcc and intel openmp barriers (maybe more shared memory barriers? openmpi?)

--- 12-12-01 ---

 * fixed two huge mistakes in the old prism mdp model.
   luckily, this does not effect the correctness of the old modelchecking results,
   since the main results come from the spin model.
   i also rechecked that everything is alright with the corrected model.
 * shortened mdp (omitted unnecessary transitions now that we proved its correctness through a fine grained mdp model) -> dtmc
 * created ctmc stub with dummies for missing measured rates
 * open questions regarding the current ctmc in mail pending

   email:
   """
   [..]
   """

--- 12-11-24: notes from the last meeting ---

 ✓ show the model's non-blocking (with fairness for the parallel composition) with an LTL supporting modelchecker
   (prism can only handle probabilistic queries)
   ✓ also try 4 or 5 processes just to be sure - used "□◇(left==true) ∧ □◇(left==false)"
   ✗ and/or prove it by hand (maybe later, for the report)

 ✓ tighten/reduce the model if you are sure no properties are lost
   ✓ remove non-det while/if
   ✓ unify previously split commands, perhaps

 ✓ get the required numbers for the ctmc model
   * approx:
     * work period 1000-10000 cycles
     * shared memory read w/ invalid cache 20-50 cycles
     * shared memory read w/ valid cache ?? cycles
     * shared memory write w/ copied cache lines 100 cycles
     * shared memory write w/o copied cache lines ?? cycles
     * maybe differ between the numbers of cores that have to invalidate their cache lines
       * if the differences are large enough and
       * if the model does not grow a too much

   * mind:
     * switch off powersaving / turboboost / hyperthreading
     * check the assembly/objdump of your benchmark binaries
     * measure rdtsc alone
     * rdtsc's measuring point is in the middle of the command
     * rdtsc might be reordered in the cpu
     * rdtsc commands without using the returned values might make the compiler remove the rdtsc command
     * rdtsc code:
       asm volatile("rdtsc" : "=a" (lower), "=d" (upper));
       uint64_t cycles = (((uint64_t)lower) | (((uint64_t)upper) << 32));
     * make sure no unnessecary stuff happens during the measurement (read the asm)
       watch out for sp/esp/rsp (stackpointer) usage (shouldn't do that)

 □ try variations of the implementation (don't model it, though)
   * using mwait or usleep after a write of the shared variable

 □ future: create a distributed memory push only version of this (promissing!)

--- 12-11-17 ---

 * fixed huge bug in the model (initialized the entry/exit barrier wrongly)

 * Probing for undesired behaviour

   non-det scheduling
   fairness enabled (strong fairness)

   // correctness of the locations (no racing through it is possible)
   ✓ Pmax=? [F "invalid_locations"] = 0.0

   // the barrier bitmask will never be invalid
   ✓ Pmax=?[F "invalid_barrier"] = 0.0

   // if one of the processes got out of the entry/exit barrier, all of them will
   ✓ Pmax=? [F ((p1_between|p1_exit_barrier)&((p2_entry_barrier&(G p2_entry_barrier))|(p3_entry_barrier&(G p3_entry_barrier))))] = 0.0
   ✓ Pmax=? [F ((p1_before|p1_entry_barrier)&((p2_exit_barrier&(G p2_exit_barrier))|(p3_exit_barrier&(G p3_exit_barrier))))] = 0.0
   ✓ Pmax=? [F ((p2_between|p2_exit_barrier)&((p1_entry_barrier&(G p1_entry_barrier))|(p3_entry_barrier&(G p3_entry_barrier))))] = 0.0
   ✓ Pmax=? [F ((p2_before|p2_entry_barrier)&((p1_exit_barrier&(G p1_exit_barrier))|(p3_exit_barrier&(G p3_exit_barrier))))] = 0.0
   ✓ Pmax=? [F ((p3_between|p3_exit_barrier)&((p1_entry_barrier&(G p1_entry_barrier))|(p2_entry_barrier&(G p2_entry_barrier))))] = 0.0
   ✓ Pmax=? [F ((p3_before|p3_entry_barrier)&((p1_exit_barrier&(G p1_exit_barrier))|(p2_exit_barrier&(G p2_exit_barrier))))] = 0.0


  * Characterization of the Constantly Resetting Race

    I attempt to characterize it to be able to check wether there are other unexpected races,
    besides this "constantly resetting race"

    At the moment I cannot find it.
    And prism says Pmax=? [F G p1_entry_barrier] = Pmax=? [F G p1_exit_barrier]= 0.0

    Pmin=? [G F p1_between] = 1.0

    So it might not even exist.
    * (make sure)


--- 12-11-03 16:40 ---

 ✓ get to know ctmc s
   ✓ create a ctmc model for timing and probabilistic analysis
     ✓ script model generation, and perhaps property generation
 ✓ think of / make up useful properties and test them
   ✓ show the absence of other deadlocks than the "constantly resetting race"
 ✓ find a quad or more core (w/o HT) machine and check the cache impact there
   * (fyi) dual core HT notebook CPU uses about 1% of its cache bandwidth per core.
     so it is probably 2% used cache bandwidth in total for my 2core+HT notebook.


--- 12-11-03 11:30: Gist of the last meeting ---

near TODO:
 ✓ switch to bitmask from primes
 ✓ reuse the local copy instead of using two copies
 ✓ switch to MDP and introduce non-determinism in while statements where appropriate

not as near TODO:
 ✓ (copied to above) switch to ctmc
   ✓ (copied to above) script model generation, and perhaps property generation

far TODO:
 ✓ model timing differences, work periods and cache latency/behaviour, too

general possible directions (markus/sascha):
 * timing analysis
 ✗ "constantly resetting race" -> prove/account for the assumption behind pW/CS (there is no constantly resetting race)
 * methodolgy/tools support -> generalize modelling to also be able to check other similar algorithms
 * 512 processes

possibly/unimportant TODO:
 ✓ properly implement the barrier with a switch where it decides itself which of the two phases/while loops to enter

dont forget TODO:
 ❏ optimization: processes memorize who they already saw and add them (besides themselves) to the barrier (don't model it, though)
 ✓ (copied to above) think of and test properties/queries and test them
   ✓ (copied to above) show the absence of other deadlocks than the "constantly resetting race"
 ✓ (copied to above) measure cache impact (need a proper >4 multi core (without HT) machine for this)
   * (fyi) inside one HT-able core there occur simply no cache misses, since both
     threads use the same execution units/cache and everything

misc:
 ✓ assume core count >= thread count for modelling
   This simplifies the real implementation of the algorithm. If core count < threads
   the scheduler would dissrupt the efficiency/workings of the algorithm very very much
 *
   cp := orig              <
   if i am not in cp {     < model timing differences here, too
     add me to cp          <
     orig := cp
   }

--- 12-10-27 ---
 * hg in Betrieb genommen
 * Modell korrigiert / verfeinert
 * von ascii art auf dot-file fuer das visuelle Modell gewechselt
 * aufgeraeumt
 * erste kleine properties eingefuegt (unmoegliche states und sowas)

vorher:
1)
if one threads gets his value overwritten by another and sees that another threads multiplicated value is missing,
he should multiply this value too, not only his own.

2   3   5
|   |\  |
2   | \ |
| \ 6  \|
|   |   5
|   | / |
|   30  |
|   |   |

without this mechanism thread 3 would write 15, because the multiplying of 2 has not been registered by 5

-----------------------------------------------------------

2)
use a bit array instead of an integer and primes.

every odd bit marks the entered state of a thread.
every even bit marks the left state of a thread.

initial state:           00 00 00 00
everyone arrived:        10 10 10 10
everyone reached the end 11 11 11 11

if one state reaches the 11 11 11 11 state it sets it to 00 00 00 00

the above mechanism is already a work around for a race. the same race as with using multiplication and division
for primes.

-----------------------------------------------------------

Ich hab das Beispiel gefunden.

(das spielt sich alles in der entry loop ab)

2   3   5
|   r1  r1
|   |   w5
r5  |   |
|   w3  |
|   |   r3
w10 |   |
|   |   |
|   r10 |
|   w30 |
r30 x   |
x       |
        w15
        deadlock

Das bedeutet gleichzeitig dass ein "resetten" der entry barrier
auf 30 (nachdem der erste thread raus ist aus der loop) nicht gehen wuerde,
weil nr 5 danach immernoch 15 schreiben
kann und der selbe deadlock entsteht.

wenn man statt dem "resetten" eine extra variable nimmt, kann das nicht passieren.

email 12-10-09 18:31:
> Hallo,
> 
> die Zahlen fuer hohe Thread-Anzahl koennte man auch bessern indem man in der 
> schleife den thread "yielded" ... in anderen Sprachen/Bibliotheken kann man 
> sagen, dass man erstmal keine rechenzeit braucht und die anderen weitermachen 
> duerfen (nicht sicher ob pthread es auch kann. Ich nehme an: ja). Das duerfte 
> das Problem ab 5 threads mildern oder beheben. (Was vermutlich passiert ist, 
> das einer bis oder vier fuer sich immer wieder multiplizieren und pruefen und 
> der 5te lang nicht dran kommt oder sich unguenstig abwechselt -- sodass die 
> die 4 aktiven quasi sinnlos pruefen und multiplizieren ... weil ohnehin ein 
> anderer fehlt).
> 
> 
> 
> Die dritte Variable weglassen geht in dem Code wirklich nicht. (Die ist erst 
> dadurch ueberhaupt dazugekommen)
> 
> Ich wollte es dir gerade nochmal beschreiben, aber jetzt faellt mir auf, dass 
> der deadlock scheinbar anders zustande kommt. Aber er passiert definitiv.
> 
> ("someoneLeftTheEntryBarrier" ueberall aus dem quellcode loeschen (und aus dem 
> while bedingungen die und klausel auch wegmachen) und compilieren)
> 
> 3 threads, 1000 iterationen.
> 
> 5 bleibt im entry loop mit entryBarrier = 10.
> 2, 3 sind im exit loop mit exitBarrier = 6.
> tot.
> 
> Aber ich versteh auch gerade nicht warum, und muss jetzt auch los^^.
> 
> Ich hoffe ich konnte helfen.
> 
> gruesse und danke fuers angucken des codes.
> Ronny    
> 
> On Tuesday, October 09, 2012 16:39:42 Marcus Völp wrote:
> > Hi,
> > die Zahlen sehen vielversprechend aus. Ich schaue mir heute Abend bzw.
> > Morgen mal Deinen Code im Detail an. Deinen Deadlock sehe ich nicht auf
> > anhieb aber Michael Scott hat auch mal gesagt, das man pro lockfreiem
> > Algorithmus einen PhD braucht. Insofern ist das nicht verwunderlich.
> >    
> >     Marcus
> > 
> > On 10/09/2012 03:42 PM, Ronny Brendel wrote:
> > > So, ich habe mit pthread_barrier_* das gleiche nochmal implementiert
> > > und mal beides gemessen.
> > > (mit "time (real)" ueber die gesamte Programmlaufzeit)
> > >
> > > # 2 threads 1mio iterations
> > > pthreads: 16.103s
> > > neu: 0.292s
> > >
> > > # 3 threads 1mio iterations
> > > pthreads: 29.290s
> > > neu: 0.561s
> > >
> > > # 4 threads 400k iterations
> > > pthreads: 17.102s
> > > neu: 0.336s
> > >
> > > # 5 threads 300k iterations
> > > pthreads: 16.979s
> > > neu: wird niemals fertig (100 iterationen dauern 4 sekunden)
> > >
> > > zwischen 5 und 15 threads (was das Maximum wegen Primzahlen und 64bit
> > > integer ist) aendert es sich nicht mehr so stark ... circa Verdopplung
> > > der Zeit die fuer 100 Iterationen benoetigt wird.
> > >
> > > Scheinbar liegt es ihm am Herzen auf den 4 cores (2 cores+HT)
> > > gleichzeitig laufen zu koennen.
> > >
> > >
> > > Interessant.
> > >
> > > Also fuer wenige Threads ist es Wahnsinnig schnell, aber wenn die
> > > gegenseitigen Ueberschreibungen sich haeufen wird es schnell
> > > schlimmer. (wie erwartet)
> > > Aber der Sprung von 4 auf 5 threads. ist ein besonders krasser
> > > Unterschied auf meinem laptop.
> > >
> > > disclamer:
> > > Die Messung ist natuerlich nicht sehr schoen, weil noch ein paar
> > > kleine Einfluesse eine Rolle spielen (zB ein bisschen
> > > commandozeilen-output, eine bessere Uhr koennte man benutzen, die
> > > Initialisierungs und Abbau-Phase sollte man nicht mitmessen). Aber im
> > > groben sollte es stimmen, was da steht.
> > >
> > > ps: im code unten ist noch ein kleiner bug gefixt der verhindert hat
> > > mehr als 9 threads zu benutzen (integer statt int64).
> > >
> > >
> > > gruesse!
> > > Ronny
> > >
> > > Quoting Ronny Brendel <ronny.brendel@tu-dresden.de>:
> > >
> > > > Da ich den Markus gerade nicht erwischt schreib ich mal ne mail.
> > > >
> > > > Hallo,
> > > >
> > > > ich habe jetzt eine scheinbar funktionierende Implementation der
> > > > Barrier mit 2 Variablen, plus einer extra.
> > > >
> > > >
> > > >         int64_t entryBarrierCopy = 1;
> > > >         c->entryBarrier = 1;
> > > >
> > > >         /* Entry Loop */
> > > >         do {
> > > >             entryBarrierCopy = c->entryBarrier;
> > > >             if (entryBarrierCopy % prime != 0) {
> > > >                 entryBarrierCopy *= prime;
> > > >                 c->entryBarrier = entryBarrierCopy;
> > > >             }
> > > >         }while (entryBarrierCopy < productOfAllPrimes &&
> > > > c->someoneLeftTheEntryBarrier == 0);
> > > >
> > > >
> > > >         c->someoneLeftTheEntryBarrier = 1;
> > > >
> > > >         int64_t exitBarrierCopy = 1;
> > > >         c->exitBarrier = 1;
> > > >
> > > >         /* Exit Loop */
> > > >         do {
> > > >             exitBarrierCopy = c->exitBarrier;
> > > >             if (exitBarrierCopy % prime != 0) {
> > > >                 exitBarrierCopy *= prime;
> > > >                 c->exitBarrier = exitBarrierCopy;
> > > >             }
> > > >         }while (exitBarrierCopy < productOfAllPrimes &&
> > > > c->someoneLeftTheEntryBarrier == 1);
> > > >
> > > >         c->someoneLeftTheEntryBarrier = 0;
> > > >     }
> > > >
> > > >
> > > > Notes:
> > > > 1)
> > > > die *Copy-Variablen existieren, weil wenn ich statt
> > > >
> > > >             entryBarrierCopy = c->entryBarrier;
> > > >             if (entryBarrierCopy % prime != 0) {
> > > >                 entryBarrierCopy *= prime;
> > > >                 c->entryBarrier = entryBarrierCopy;
> > > >             }
> > > > ---
> > > >             if (c->entryBarrier % prime != 0) {
> > > >                 c->entryBarrier *= prime;
> > > >             }
> > > >
> > > > mache passiert der if-Vergleich und die Multiplikation eventuell nicht
> > > > auf dem gleichen Datum und es kann zB passieren dass eine Primzahl
> > > > zweimal auf die barrier-Variable multipliziert wird. (Ich denke das
> > > > kann dazu fuehren dass die barrier ihr limit erreicht obwohl in
> > > > wirklichkeit nur 1 thread 3 mal eingecheckt hat)
> > > >
> > > >
> > > > 2   3   5
> > > > ---------
> > > > r1  r1  |
> > > > w2  |   r2
> > > > |   w3  |
> > > > r3  |   |   <- barrier % 2 != 0
> > > > |   |   w10
> > > > r10 |   |
> > > > w20 |   |   <- barrier *= prime  // 20 = 2 * 2 * 5
> > > >
> > > > XXX
> > > >
> > > >
> > > > Ich habe deswegen mich ein bisschen zu cmpxchg belesen und versucht
> > > > rauszufinden was der compiler macht. if () mit dem *= ist scheinbar
> > > > kein Kandidat fuer diese atomare Operation. Der assembler output des
> > > > gcc ist auch frei von cmpxchg-Befehlen (auch den 64bittigen) .
> > > >
> > > >
> > > > 2)
> > > > Wenn ich das someoneLeftTheEntryBarrier-Zeug weglasse gibt es einen
> > > > deadlock.
> > > > Der erste thread der aus der entry loop rausrennt kann eventuell seine
> > > > (kurze) Anwesenheit den anderen nicht mitteilen weil seine
> > > > schreiboperation von einem anderen thread direkt ueberschrieben wird.
> > > >
> > > > Sodass am Ende ein thread in der exit loop und alle anderen in der
> > > > entry loop haengen.
> > > >
> > > > Dieses flag stellt sicher, dass wenn der Erste die entry loop
> > > > verlaesst es die anderen threads merken und die entry loop verlassen
> > > > koennen.
> > > >
> > > > Solange nicht alle threads in der der exit loop angekommen sind geht
> > > > keiner heraus. Wenn der erste rausgeht setzt er das flag zurueck
> > > > (analog zur entry loop) sodass erstens keiner vorzeitig aus der entry
> > > > loop in der naechsten iteration heraus kann bis alle angekommen sind,
> > > > und zweitens es alle merken wenn der erste die exit loop verlaesst
> > > > (selbes problem wie bei der entry loop).
> > > >
> > > > ----
> > > > Meine Tests (von 1 bis 9 threads mit beliebig vielen iterationen ---
> > > > also wiederholtem nutzen der selben barriere) funktionieren alle bisher.
> > > >
> > > >
> > > > Die Frage ist: Was nun?
> > > > Besonders elegant sieht es nicht aus. Gibt es noch Sachen die ich
> > > > veraendern kann / sollte?
> > > >
> > > > Ich koennte auch spasseshalber mal gegen die pthread_barrier
> > > > benchmarken. (Das geht sicher auch fix)
> > > >
> > > > Wie kann ich herausfinden ob das Ding wirklich deadlock-frei ist?
> > > > (Beziehungsweise nur der Fall des zyklischen Austauschens der
> > > > barrier-Variable uebrig bleibt (der laesst sich scheinbar nicht
> > > > verhindern, richtig? und er sollte sich auch "irgendwann" selbst
> > > > erledigen, wenn er oft genug versucht auszutauschen. mh obwohl.)
> > > >
> > > > Mich ins modellieren einlesen und die barrier erstmal so lassen waere
> > > > auch eine Moeglichkeit.
> > > >
> > > >
> > > > Meinungen?
> > > >
> > > > viele Gruesse,
> > > > Ronny
> > >
> > >
