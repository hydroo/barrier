\documentclass[a4paper, 10pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{color}
	\definecolor{gray}{rgb}{0.5,0.5,0.5}
\usepackage{colortbl}
\usepackage{datetime}
\usepackage{fancyvrb}
\usepackage{float}
\usepackage{graphicx}
	\graphicspath{{images/}}
	\DeclareGraphicsExtensions{.eps, .pdf,.png}
\usepackage{epstopdf}
	\DeclareGraphicsRule{.eps}{pdf}{.pdf}{`epstopdf #1}
	\pdfcompresslevel=9
\usepackage{intcalc}
\usepackage{latexsym}
\usepackage[latin1]{inputenc}
\usepackage{listings}
	\lstset{
		frame=single,
		%numbers=left,
		numberstyle=\small\color{gray},
		tabsize=4,
		morekeywords={and, boolean, do, else, false, finished, for, if, initialised, integer, is, mod, reset, true, until, use, wait, while},
	}
\usepackage{multirow}
\usepackage{relsize} % used for larger sum symbols
\usepackage{setspace}
%\usepackage{silence}% Filter out unwanted warnings and error messages
%\WarningFilter{pdftex}{destination with the same}
\usepackage{tikz}
	\usetikzlibrary{shapes, arrows, positioning}
	\tikzstyle{-}    = [draw]
	\tikzstyle{->}   = [draw, -latex']
	\tikzstyle{<-}   = [draw, latex'-]
	\tikzstyle{<->}  = [draw, latex'-latex']
	\tikzstyle{o}    = [draw, circle]
	\tikzstyle{box}  = [draw, rectangle]
\usepackage{url}
	\renewcommand{\UrlBreaks}{\do\/\do\-\do\.\do\_\do\c\do\l\do\e\do\3} % carefully tuned to the needs of the bibliography. It avoid underful hboxes due to difficult url line breaking. If the bibliography changes, this might have to be reviewed.

\usepackage{hyperref} % should be last usepackage to avoid errors.

\def \todo{\textbf{\textcolor{yellow}{TODO}}}
\def \citationneeded{\textbf{\textcolor{yellow}{CITATION NEEDED}}}
\newcommand{\listingrule}[1]{\rule{#1}{0.4pt}}

\newcommand*\cleartooddpage{
	\clearpage
	\ifthenelse{\isodd{\thepage}}
		{}
		{\newpage \mbox{} \clearpage}
}

\newdateformat{titledate}{\ordinalnum{\THEDAY} \monthname[\THEMONTH], \THEYEAR}
\titledate

%\setcounter{secnumdepth}{5}

\title{Development and Analysis of Barrier Protocols}
\author{Ronny Brendel\\Tutors: Sascha Kl\"uppelholz \& Marcus V\"olp}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
\pagenumbering{alph}

\begin{center}
\includegraphics[width=3cm]{tu-logo}~\\[1cm]
%\includegraphics[width=3cm]{tu-logo-thick}~\\[1cm]
\textsc{\LARGE Dresden University of Technology}\\[0.5cm]
\textsc{\Large Faculty of Computer Science}\\[0.2cm]
\textsc{\large Institute of Theoretical Computer Science}\\[0.2cm]
\textsc{\large Chair for Algebraic and Logical Foundations of Computer Science}\\[3cm]
\Huge Study's Thesis \\[1cm]
\huge Development and Analysis of Barrier Protocols\\[3cm]
\end{center}

\vfill
\begin{flushleft} \large
	Author: Ronny Brendel \\
	Responsible University Professor: Prof. Dr. Christel Baier \\
	Supervisor: Dr. Sascha Kl\"uppelholz
\end{flushleft}
\vspace{0.2cm}
\begin{flushright}
	\large Vienna, \today
\end{flushright}

\end{titlepage}

\pagebreak
\newpage \thispagestyle{empty} \mbox{}
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}

%\section*{Aufgabenstellung}
%\label{sec:task}
%\begin{enumerate}
%	\item Literaturrecherche und ausf\"uhrlicher \"Uberblick aktuell genutzter Barrierenprotokolle sowie grunds\"atzlicher Implementierungsm\"oglichkeiten f\"ur Barrieren.
%	\item Untersuchung und Weiterentwicklung der von Nicolas Mc~Guire vorgeschlagenen Barriere.
%	\item Identifikation und Formalisierung der zentralen funktionalen Eigenschaften im Bezug auf Korrektheit sowie der quantitativen Eigenschaften zur Ermittlung der Performance von Barrierenprotokollen bez\"uglich Energieverbrauch und Geschwindigkeit.
%		\item Modellierung und Quantitative Analyse:
%			\begin{itemize}
%				\item Modellierung a) eines prominenten Vertreters von Barrierenprotokollen mit verteiltem Speicher, b) eines prominenten Vertreters von Barrierenprotokollen mit geteilten Speicher, c) des auf Mc~Guire beruhenden Barriereprotokolls jeweils im probabilistischem Model Checker PRISM.
%				\item Analyse und Vergleich der drei oben genannten Modelle im Bezug auf die in 3. identifizierten funktionalen sowie quantitativen Eigenschaften in PRISM. Wenn m\"oglich: Erg\"anzung des Performance-Vergleichs der drei Barrieren mittels messbasierter Methoden.
%			\end{itemize}
%		\item Zusammenfassung und Ausblick: Diskussion der in 4. gewonnenen Erkenntnisse
%\end{enumerate}
%
\section*{Task}
\label{sec:task}
\begin{enumerate}
	\item Literature research and detailed survey of currently used barrier protocols as well as implementation possibilities for barriers.
	\item Analysis and improvement of Nicolas Mc~Guire's proposed barrier.
	\item Identification and formalisation of key functional properties concerning correctness as well as quantitative aspects for determining performance of barrier protocols with regard to energy consumption and speed.
		\item Modelling and quantitative analysis:
			\begin{itemize}
				\item Modelling a) a prominent representative of barrier protocols for distributed memory, b) a prominent representative of barrier protocols for shared memory, c) the barrier protocol based on Mc~Guire's idea, in each case using the probabilistic model checker PRISM.
				\item Analysis and comparison of the three above-mentioned models, regarding the functional as well as quantitative properties identified in 3., using PRISM. If possible: complement the performance evaluation of the three barriers using measurement-based methods.
			\end{itemize}
		\item Summary and outlook: Discussion of the insights gained in 4.
\end{enumerate}

\pagebreak
\newpage \thispagestyle{empty} \mbox{}
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}

%\section*{Selbstst\"andigkeitserkl\"arung}
%\label{sec:integrity}
%Ich erkl\"are hiermit, dass ich die vorliegende Arbeit selbst\"andig und ohne Benutzung anderer als der angegebenen Hilfsmittel angefertigt habe. Die aus fremden Quellen w\"ortlich oder sinngem\"a\ss ~\"ubernommenen Gedanken sind als solche kenntlich gemacht. Ich erkl\"are ferner, dass ich die vorliegende Arbeit an keiner anderen Stelle als Pr\"ufungsarbeit eingereicht habe oder einreichen werde.

\section*{Statement of Academic Integrity}
\label{sec:integrity}
I hereby declare that I prepared this thesis independently and without use of tools other than specified. Foreign thoughts, taken literally or in spirit, are marked as such. I also declare that I have not filed the present work at any other location or will submit it.

\pagebreak
\newpage \thispagestyle{empty} \mbox{}
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{arabic}

\renewcommand{\contentsname}{Contents}
\tableofcontents

\pagebreak
\newpage \mbox{}
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Don't forget}
% \begin{itemize}
% 	\item maybe rename:
% 		\begin{itemize}
% 			\item section: means to implement barrier protocols
% 			\item section: currently used barrier protocols
% 			\item section: discussion
% 		\end{itemize}
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleartooddpage
\section{Introduction}
\label{sec:introduction}
Synchronisation is a central element of parallel programs. Various synchronisation mechanisms with different goals have been devised. One such mechanism is the barrier. If a group of threads or processes execute a barrier operation, each of them must wait at this point until all other threads/processes arrive at the barrier.

Many synchronisation operations imply a barrier. For example OpenMP's~\cite{openmp} parallel for loop makes sure that no thread continues program execution until all iterations are completed, since the following operations might rely on the results of the loop. Likewise in MPI~\cite{mpiforum}, global operations like reduce, scatter and gather imply a barrier synchronisation.

Many parallel programs use a lock-step approach to interleave computation with communication phases.
For example a program simulating weather divides the map into a grid.
Each square in this grid is assigned to one process, so that the weather progression of the next few minutes, for all squares, can be simulated concurrently.
Changes in weather condition at the borders of a square influence the next simulation step of neighbouring squares.
Therefore, before entering the next simulation phase, information, like temperature, humidity or cloud movement, about the border regions is exchanged.
Before beginning this exchange the program needs to make sure that each square has completed the current simulation step. To achieve that, they collectively invoke a barrier operation.
Once it is completed, data is exchanged and the next simulation step starts.

The barrier is a commonly used synchronisation operation. According to a survey conducted at the High Performance Computing Center Stuttgart~\cite{rab00} in 2000, 5.3\% of all time spent synchronising and 0.7\% of the total program execution time is spent in barriers.

Today's barrier implementations use protocols that are fairly old. One example is the Dissemination Barrier~\cite{hensgen1988}, presented in 1988. This suggests that, due to the rapidly changing computer science and engineering landscape, one can improve upon existing barriers.

At a recent workshop, Nicholas Mc~Guire proposed a new low-level synchronisation scheme, called probabilistic Write/Copy-Select (short pW/CS) lock~\cite{pwcs}. It aims to improve the performance of exclusive access to shared memory. The core ideas, though, apply to synchronisation in general:
\begin{itemize}
	\item Concurrent algorithms work, mostly, in a deterministic fashion. That is the arrangement of synchronisation is predetermined. The program is executed in a strict form. Faulty behaviour is avoided at great cost.
	\item One can improve performance by relieving this constraint. Tolerate inconsistencies, make errors detectable and/or ignorable.
	\item Leverage the inherent randomness~\cite{mcg09}, induced by caching, scheduling, memory access latency differences, or short: the complexity of today's computer systems, in order to view concurrent memory access as genuinely random. This enables the use of probabilistic algorithms for development, and the tools of probability theory for analysis.
\end{itemize}
One wants to design algorithms that have a sufficiently high probability of success, tolerate faulty behaviour and, on average, perform better than their deterministic cousins. They aim to be better in ways, e.g. speed, energy efficiency and memory consumption, that depend on the use case.
We believe that synchronisation primitives, in particular barriers, can be improved by applying these principles.

To gain confidence that a program works properly, i.e. satisfies certain properties, one normally uses testing and benchmarks.
Testing, as well as measurement, is usually deterministic. When testing probabilistic algorithms deterministically, there is always a chance of errors not being revealed. Randomising tests only gives a certain degree of confidence as well. Both methods are not exhaustive.
Two more drawbacks of measurement are that the granularity of the measurement scale is limited, and that measuring itself can influence the outcome. This makes analysing low-level synchronisation primitives, and small program pieces in general, hard and sometimes impossible.
Model checking avoids these three problems. It is exhaustive, arbitrarily granular and, since it simulates the protocol, involves no measurement overhead. But Model checking has, of course, its own set of shortcomings. First of all, the available tools are difficult to use. Second, the accuracy of results depends on the quality of the model. Third, exhaustive analysis needs a lot of hardware resources, and can therefore be infeasible even in relatively confined scenarios.
Model checking, thus, complements measurement-based analysis.

The thesis is structured as follows. First, we survey the tools developers have at their disposal to implement barriers. Second, we present today's mainly used barrier algorithms. To conclude the background study, we give an account of the basic anatomy of barriers.
We then move on to present three newly devised barrier protocols.
The largest part of the thesis concerns the analysis and comparison of two known and two new barriers. We analyse them by means of an informal scrutiny combined with functional and probabilistic model checking.
The final section summarises the presented work and gives an outlook on possible directions for future research.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleartooddpage
\section{Background}
\label{sec:background}
In this section we will first take look at the tools at our disposal to implement barriers, on both shared memory and distributed memory architectures, followed by an overview of which barrier algorithms are used in today's parallel programming frameworks.
Finally we present a view of the barrier as a modularised algorithm consisting of various independent components.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Means to Implement Barrier Protocols}
\label{ssec:background-means}
A shared memory system consists of a number of processors which are connected to one chunk of memory. Information can be exchanged between processors by simply reading from and writing to the same memory location, since they all share the same address space.
Desktops, laptops and small to medium servers are typical examples of shared memory systems.

Many shared memory systems (called nodes) can be connected via a network to compose a distributed memory system. Such systems do not have one global memory, instead each of the nodes has its own private memory. Computation is done on each node using only local memory. In order to share information between nodes messages via network need to be exchanged.
For example large servers and high-performance computers are oftentimes distributed memory systems.

Programming shared memory systems is very different from developing for distributed memory architectures. Therefore, we will subsequently differentiate between these two worlds.

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Shared Memory Systems}
\label{sssec:background-means-shared}
A parallel program for a shared memory system consists of multiple threads of execution -- short \emph{threads}. 
The straightforward way to share information between threads is to \emph{load} from and \emph{store} to the same memory address.

Furthermore, most modern processors provide means to execute a small series of computations on a date while making the memory unavailable to other threads. They are called \emph{atomic operations}. Two examples are the \emph{test-and-set}- and \emph{add-fetch}-instruction. The former stores a value to an address if a given condition is fulfilled, the latter loads a date, adds a given value to it and stores it back.

Without these operations parallel programming is much more difficult and error-prone, since concurrently issuing normal load and store operations can easily cause \emph{race conditions}. This convenience, on the other hand, comes at the cost of additional management overhead for the processor. To date shared memory systems are usually limited to 64 threads and less. Some high-performance computing implementations reach 512 concurrent threads. In comparison distributed memory systems can have over a million processing units.

The intuitive ordering of concurrent memory accesses is \emph{sequentially consistent}.
\begin{quote}
	\textit{The result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.} (Lamport~\cite{sequentialconsistency})
\end{quote}

Enforcing this intuitive semantic comes at a cost. Modern processors buffer memory operations on many levels. To enforce sequential consistency mechanisms to control and circumvent this buffering have to be introduced resulting in higher memory access latency and more transistors in the processor.

If an algorithm is carefully designed, one can weaken the consistency model to gain better performance. One prominent way of such a model is \textit{release consistency}~\cite{gha90}. Memory access appears unordered except for so called \emph{acquire} and \emph{release} operations. No memory access that has been issued after an acquire operation is permitted to finish before this acquire operation. No memory access is allowed to finish later than the next release operation. Acquire and release encapsulate the memory accesses in between.

Aside from implementing barriers in software vendors can wire a barrier implementation into the system's hardware. This is almost never done on shared memory architectures. One example for hardware barrier support is the SGI~UV~2000~\cite{sgiuv2000}, a high-performance shared memory computer.

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Distributed Memory Systems}
\label{sssec:background-means-distributed}
A parallel program for a distributed memory architecture consists of multiple \emph{processes}. Each process has its own address space and is therefore unable to directly share memory with other processes.

First, we will give an overview of general ways to implement synchronisation primitives on distributed memory systems followed by an analysis of one specific distributed memory programming standard.

One way to exchange information between two processes is via \emph{synchronous message passing}. Two processes meet at a point in time, exchange information, then part and continue their computational tasks. This method requires the involved peers to wait for each other, buffer and queue information and send it via network.

Another way to approach this is to wait as few as possible, since there is, for example, no need for a sending entity to wait until a receiver arrives. The sender might as well give its information to the communication framework and continue its computational work while the framework delivers the message as soon as the receiver is ready. This method is called \emph{asynchronous message passing}. Aside from less waiting time asynchronous message passing incurs similar overhead to synchronous message passing, because on the communication framework level the same work has to be done. The framework establishes network channels, buffers and queues data, and waits for the receiver to arrive.

A comparatively recent development is the move to \emph{remote memory access} (RMA). Remote memory access makes a portion of main memory available via network, so that others can load from it and store to it without actively involving the memory's owning process. It is therefore sometimes called \emph{one-sided communication}. Implementations of RMA oftentimes support atomic operations.
In order for RMA to be available, the network hardware has to support it. For example InfiniBand~\cite{infiniband} has RMA facilities built in.
Due to its one-sided nature RMA incurs less overhead than the two previous techniques, but still buffers data. At very high networking bandwidths buffering data becomes a serious problem. To mitigate this, vendors implement so called \emph{zero-copy} techniques.

Hardware support for barriers in distributed memory systems is widespread especially in high-performance computing. Examples include the Earth Simulator~\cite{earthsimulator}, IBM Blue Gene/L~\cite{bluegenel}, IBM Blue Gene/Q~\cite{bluegeneq} and a low-cost FPGA-based system~\cite{hoefler2006b}

Another interesting and seemingly untested approach is to use lossy communication channels for developing synchronisation protocols. Hardware is always erroneous. For example network packages conflict, packages drop, packages are corrupted or connections drop all the time. Reliability-increasing techniques and protocols such as the Transmission Control Protocol (TCP), that are used to mitigate these errors, incur a great performance penalty for transmitting short messages. TCP for example creates comparatively huge packages for small amounts of data, and each transmitted package has to be acknowledged by the receiver. Checking for flipped bits in the package adds further latency.
If one would instead craft algorithms so that they do not require 100\% reliable connections, further performance improvements are conceivable.
Two ways one could implement such algorithms are via the User Datagram Protocol (UDP)~\cite{udp} and InfiniBand's unreliable connections~\cite{infiniband}.

We will now investigate the facilities the Message Passing Interface (MPI)~\cite{mpi3}, the most prominent high-performance distributed programming interface, provides to implement barrier protocols. MPI is a standardised, low-level, language-independent and portable interface specification. It is very customizable and supports various modes of operation, in order to support many different hardware configurations and allow the user to achieve maximum performance. MPI is widely adopted in high-performance computing and scientific computing\cite{mpiadoptiona, mpiadoptionb, mpiadoptionc} and there exist a variety of implementations in all major programming languages.

In our analysis we will only scratch the surface of what the standard offers, since diving deep involves lots of details, complicated semantics, and many of the details do not matter for our purpose.

MPI supports synchronous message passing via \emph{send} and \emph{receive}. Additionally, you can issue a send operation that immediately returns, if no receiver is already in place waiting for the sender.

Furthermore, MPI facilitates asynchronous message passing. One can send and receive messages without blocking, and one can test if a message has been successfully transmitted. Synchronous and asynchronous message passing can be mixed. That means one is able to, for example, issue a non-blocking send that is matched by a blocking receive operation.

Since version 3.0 the MPI standard supports \emph{non-blocking collectives}. ``Collectives'' is a collective name for all communication operations where there is a variable number of communicating  peers, like barrier, broadcast, reduce and more.
Non-blocking collectives allow, in the example of the barrier, to not block at the operation, but to register at the barrier upon arrival and subsequently test the barrier for completion, doing computational work meanwhile.
This facility is probably unsuited for building new barriers on top of it, but is nevertheless an interesting recent development.

MPI also supports RMA. Although some MPI implementations, for example Open~MPI~\cite{openmpi}, do not yet support version 3.0 of the specification, we will restrict ourselves to analysing it, because it subsumes the features of version 2.2~\cite{mpi2}, it is already a year old and supported by other prominent implementations such as MPICH~3.0~\cite{mpich} and its derivatives MVAPICH2~\cite{mvapich} and Cray MPT~\cite{craympt}.

The definition of RMA according to the standard is rather involved, and we have to go into more detail to highlight important points.

To initiate sharing remote memory each participating process issues a collective call to create a \emph{window} of memory to be shared. A window is associated with the group of processes that created it, and through it each participating process exposes a portion of local memory to the other peers.
Upon creating a window, a number of restrictions regarding the use of the window can be established, in order to gain better performance. One can for example lower the memory consistency requirements or guarantee to not use certain operations.

Depending on the underlying hardware and MPI implementation there are two different memory models. A window may either have the \emph{separate} or the \emph{unified} model. The separate model states that the memory that is visible to the remote peers and the memory that is visible to the local process is not the same and might therefore not automatically update between these two copies. That means a change by a peer to one process's memory might not be made visible. This process has to manually update its local copy in order to see the change. In contrast the unified model makes sure that the two copies are eventually updated. Therefore, manual refreshing is unnecessary. This differentiation is a direct result of trying to support many different hardware configurations.

Before going into the details of window synchronisation we have to introduce the concept of \emph{epochs}. An epoch is a period between two window synchronisation calls. RMA operations like \emph{put}, \emph{get}, \emph{accumulate} may only be used inside an epoch. Epochs are used to accumulate multiple operations to one larger transfer, in order to achieve better efficiency.

MPI differentiates between \emph{active} and \emph{passive} target communication. In active target communication all peers are actively involved in window synchronisation, whereas in passive target communication only one side's activity is required.

One way to synchronise a window is via the \emph{fence} operation. A fence is a collective operation among all window group members that makes sure all outstanding RMA requests on this window are finished before the operation returns. It implies a barrier synchronisation.

A more fine-grained instance of active target communication is via the operations \emph{start}, \emph{complete}, \emph{post} and \emph{wait}. Before issuing any remote access calls, the issuing process first has to advertise its intention through the start operation. Once he is finished he will invoke the complete operation to announce that its transmission is complete. The receiving end similarly issues post and wait to expose and synchronise its memory. It is allowed to access its window's local memory meanwhile. In contrast to a fence operation, one can restrict such synchronisation to arbitrary processes of the window.

The \emph{lock} and \emph{unlock} operations facilitate one way of passive target communication. Through these two functions one can block all access to a chunk of remote memory for all other window members. Between lock and unlock, the process can then issue remote memory access operations, without being interfered.

Another method to synchronise memory passively is through \emph{flush}. A flush makes sure all outstanding RMA calls, issued by the caller, are completed before returning. One can choose if the operations have to be completed only at the issuer's side or on the receiver's side as well.

Window synchronisation can be optimised through giving the implementation information about which operations are not used during the epoch and what happens before and after this synchronisation.

Inside an epoch one may issue remote memory access operations. The two simple ones are \emph{put} and \emph{get}. Memory access through put and get appears unordered inside an epoch.
Furthermore, one can use atomic operations like \emph{accumulate}, \emph{fetch-and-op} or \emph{compare-and-swap}, with the intuitive semantic and where \emph{op} is a placeholder for various operations. The default global visibility order of atomic operations is sequentially consistent. When creating a window one can weaken this by, for example, allowing remote write operations to overtake remote read operations. Any combination of overtaking between reads, writes or between equal operations can be allowed.

Remote memory access in MPI is subject to a number of restrictions:
\begin{itemize}
	\item The buffer supplied to a put call should not be written to until the operation is finished.
	\item The buffer supplied to a get call should not be accessed until the operation is finished.
	\item The outcome of concurrent remote memory access to the same location inside an epoch is undefined.
	\item The outcome of concurrent local and remote access to the same location inside an epoch is undefined.
	\item The outcome of a single process issuing multiple put operations to the same location inside an epoch is undefined.
\end{itemize}
Therefore, the only way to make sure that data is correctly written or read is through atomic operations or isolating access to the same location with costly window synchronisation operations.
Normally a processor commits 32 or 64 bit to main memory atomically, and in the remainder of this thesis we will assume this is the case.
Therefore, these restrictions make sense for buffers at sizes above this, because concurrent memory access of interfering operations might overlap and the outcome is then indeed undefined.
For amounts of memory below this threshold we wish to have a semantic that is more specific. For example if two puts of at most 64 bits are concurrently issued to the same location, the outcome is that of the last put committed. Since this is not the case, MPI's restrictive RMA semantic makes it unsuitable to implement barrier algorithms based on the ideas of the pW/CS lock on top of it.

Shared memory barriers usually rely on atomic operations. Most distributed memory barriers use synchronous message passing. To improve efficiency, these protocols have also been implemented using RMA, e.g.~in~\cite{rdma1,rdma2,hoefler2006a}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Current Barriers}
\label{ssec:background-current}
Having surveyed the tools for implementing synchronisation primitives we will now examine the barrier protocols which power today's implementations for both shared and distributed memory systems.

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Shared Memory Systems}
\label{sssec:background-current-shared}

Many prominent implementations of shared memory concurrent programming facilities are part of closed source commercial products, like for example Intel Composer and Microsoft Visual Studio, and can therefore not be analysed. So we restrict ourselves to examining the GNU~C~Library~\cite{glibc} and GNU~OpenMP~\cite{gomp}.

% gnu-openmp/gcc-4.8.0/libgomp/libgomp.h, gnu-openmp/gcc-4.8.0/libgomp/barrier.c, gnu-openmp/gcc-4.8.0/libgomp/config/linux/, gnu-openmp/gcc-4.8.0/libgomp/config/linux/bar.h, gnu-openmp/gcc-4.8.0/libgomp/config/linux/bar.c, gnu-openmp/gcc-4.8.0/libgomp/config/linux/futex.h (cpu_relax), gnu-openmp/gcc-4.8.0/libgomp/config/linux/wait.h
The GNU~OpenMP implementation, included in the GNU Compiler Collection (GCC) version 4.8.0, implements a \emph{Central Counter Barrier}. Figure~\ref{fig:pseudocode-central-counter} describes the algorithm in pseudocode, which is executed on each thread taking part in the barrier synchronisation.
The Central Counter Barrier uses the variable \texttt{barrier} to hold the number of threads that do not yet have arrived at the barrier. Initially it is assigned the number of threads. Upon entering the barrier each thread atomically decrements the variable by one and then waits until it reaches zero. After it reaches zero, all threads eventually leave the barrier.

\begin{figure}[H]
	\centering
	\input{listings/central-counter}
	\caption{Pseudocode for the Central Counter Barrier}
	\label{fig:pseudocode-central-counter}
\end{figure}

The protocol can be separated into two phases (Figure~\ref{fig:diagram-central-counter}). During the first phase each thread decrements the variable, and in the second reads the variable until it reaches the desired value.

\begin{figure}[H]
	\centering
	\input{tikz/diagram-central-counter}
	\caption{The phases of the Central Counter Barrier for five threads}
	\label{fig:diagram-central-counter}
\end{figure}

The actual implementation in GNU~OpenMP is more involved than the pseudocode in Figure~\ref{fig:pseudocode-central-counter} suggests. For example the algorithm shown here is destructive in the sense that it cannot be repeated, because if the variable is zero at the end, repetition would mean that the variable is negative in the next round, which breaks the protocol. To mitigate this in the implementation, the variable is copied, by each thread, upon first reading it and the last thread to enter resets the shared variable to its initial value.

The loop additionally keeps track of the number of unsuccessful reads. If this number reaches a certain amount, an operating system mechanism, called futex\cite{franke2002}, is employed to suspend the thread until a later time when the other threads hopefully have arrived. Through this back-off mechanism, the system avoids unnecessary busy waiting which would negatively impact power consumption. The threshold of this counter is architecture specific and set to an equivalent of about 3 milliseconds of waiting on GNU/Linux. In case the number of threads is larger than the number of physical cores, i.e. the processor is oversubscribed, the counter is set to a much lower value. This way the operating system scheduler can switch between threads more quickly so that threads that need time for computation are not blocked by busy waiting ones.

Throughout this thesis we will assume that the thread and process count is at most as high as the number of physical processor cores in a system, because oversubscription is not useful inside a single program and therefore rarely happens.

% glibc/nptl/sysdeps/pthread/pthread.h, glibc/nptl/pthread_barrier_*, glibc/nptl/sysdeps/unix/sysv/linux/internaltypes.h
The GNU POSIX Threads implementation, called Native POSIX Threads Library (NPTL), is part of the The GNU~C~Library.
The basic barrier algorithm used in it is the same as for GNU~OpenMP. One main differences is that it does not use atomic operations to decrement the variable, but explicitly locks it to guard from interfering access. Second, there is no busy waiting, but upon arriving at the barrier the thread will immediately suspend, via a futex, and wait to be woken up by the last thread arriving. The third difference is that to reset the barrier each thread atomically increments the barrier variable by one upon leaving.

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Distributed Memory Systems}
\label{sssec:background-current-distributed}

In this section we present the barrier algorithms that are used in the two major open source MPI implementations, namely Open~MPI~\cite{openmpi} and MPICH~\cite{mpich}. We restrict our analysis to MPI for the same reasons we only considered MPI in Section~\ref{sssec:background-means-distributed}.

% ompi/mca/coll/tuned/coll_tuned_barrier.c : ompi_coll_tuned_barrier_intra_recursivedoubling, ompi_coll_tuned_barrier_intra_bruck, ompi/mca/coll/tuned/coll_tuned_decision_fixed.c
Open~MPI's barrier implementation distinguishes between process counts that are powers of two and which are not. If it is a power of two, a \emph{recursive doubling} barrier is used. A detailed explanation of this protocol can be found in \cite{hoefler2005}. For non-power-of-two process counts Open~MPI uses the \emph{Dissemination Barrier}, introduced by Hensgen, Finkel and Manber in 1988~\cite{hensgen1988}.

Figure~\ref{fig:pseudocode-dissemination} presents the pseudocode of this algorithm, which is executed on each process individually. A process is identified by its \emph{process index}, which is a number between 0 and $\mathit{processCount} - 1$. For readability reasons we have chosen an RMA-esque representation of the protocol, although it can similarly be implemented using message passing techniques.
In contrast to the Central Counter Barrier, the Dissemination Barrier uses not only one variable but an array of boolean variables per process involved. Each of these arrays is located in the local memory of its owning process and is initially set to false.
The protocol is organised in rounds. Each round the distance (variable \texttt{dist}) to the next two partners, to read from and write to, doubles. Process indices  are always calculated modulo the total number of processes to achieve a wraparound. Figure~\ref{fig:diagram-dissemination} illustrates this communication pattern.

\begin{figure}[htbp]
	\centering
	\input{listings/dissemination}
	\caption{Pseudocode for the Dissemination Barrier}
	\label{fig:pseudocode-dissemination}
\end{figure}

\begin{figure}[htbp]
	\centering
	\input{tikz/diagram-dissemination}
	\caption{Communication pattern of the Dissemination Barrier}
	\label{fig:diagram-dissemination}
\end{figure}

At first a process notifies the next one that is has arrived at the barrier by setting the array element at index \texttt{me} on the next process to true. Then it waits until the previous one notifies its arrival. When that happens the waiting process enters the next round. It now knows that itself, and the previous process have arrived at the barrier and notifies the process at double the previous distance (2) of its knowledge, by setting its array element at the target process to true. Through this communication pattern the number of processes, that each process knows have arrived, doubles with each round. Thus after $\lceil \log_2 \mathit{processCount} \rceil$ rounds every process knows of the arrival of all others and can therefore leave the barrier.

Notifying the next process is done via writing to remote memory, and waiting for notification via busy waiting on local memory. The number of remote access operations per round is the same as the number of processes involved.

The actual implementation in Open~MPI uses MPI's \emph{send-receive} operation instead of our version's remote write and local read. This operation sends a message and waits for another at the same time. The protocol is not destructive because send and receive do not change the state of the protocol. Therefore, no resetting mechanism, as used by GNU~OpenMP for the Central Counter Barrier, is needed.
Open~MPI is able to replace this implementation with hardware specific ones, if it detects supported hardware. For example InfiniBand uses a variation of the Dissemination Barrier based on RMA\cite{hoefler2006a}.

% src/mpi/coll/barrier.c : MPIR_Barrier_intra
MPICH version 3.0, which forms the basis of many other prominent MPI implementations like MVAPICH2 and Cray~MPT, implements the same send-receive-based Dissemination Barrier as is used in Open~MPI, but disregards the number of processes.

Aside from distributed memory architectures it is possible to implement these protocols for shared memory systems, as has been demonstrated for the Intel~XEON~Phi~\cite{hoefler2013}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Barrier Building Blocks}
\label{ssec:background-building-blocks}
During our analysis of general implementation facilities and concrete barrier implementations we observed that one can assemble barriers from orthogonal components.

One independent piece is the employed way of reinitializing the barrier after use. As we have seen for the Central Counter Barrier (Section~\ref{sssec:background-current-shared}) resetting might be achievable through slight modification of the algorithm. This does not work for all protocols, though.

Another possibility is to replace boolean variables, as used in the Dissemination Barrier in Figure~\ref{fig:pseudocode-dissemination}, with a repetition counter. The remote write and local spin-waiting condition are then replaced with atomic increment and a test for the correct number of repetitions. The additional amount of memory needed is linear in the number of processes. The added computational overhead is negligible.

A way of resetting that works in all circumstances is to use three distinct barriers and switch cleverly between them. The proposed procedure, to be executed by each process individually, is illustrated in Figure~\ref{fig:pseudocode-triple-reset}. In the beginning the first two barriers are initialised, whereas the third one is set to the state of a completed barrier. Upon entering the routine one uses the barrier which comes after the currently finished barrier and resets the one which has been finished before. This approach is provably correct, uses triple the amount of memory of a single barrier, and ads minimal computational overhead.

\begin{figure}[htpb]
	\centering
	\input{listings/triple-reset}
	\caption{Pseudocode for the triple barrier reset method}
	\label{fig:pseudocode-triple-reset}
\end{figure}

In order to lower power consumption limiting the amount of busy waiting is important for barrier implementations. Choosing a back-off strategy can be done independently of the used barrier protocol. In almost all cases one wants to take apart the waiting loop and insert a suspension mechanism.

During the first initialisation of the barrier an implementation can decide how to handle the case where the number of threads or processes exceeds the number of physical processor cores. Therefore, implementing a solution to this scenario is orthogonal to the other algorithm components.

Another useful approach, which has been implemented in~\cite{rdma2}, is to assemble multiple different barrier protocols to create a new one. For example assume you have a hardware topology of shared memory nodes which are connected via network. It is conceivable to construct a barrier that executes a Central Counter Barrier on each node, and then one process of each node takes part in a Dissemination Barrier. After the Dissemination Barrier is completed the intra-node processes are notified and the assembled barrier completes.
This way one can benefit from the best performing barrier on each level of the topology.
This might also make sense if barrier algorithms vary in performance for different numbers of threads or processes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleartooddpage
\section{New Barriers}
\label{sec:new}
In this section we first present a new variation on an already presented barrier protocol of ours. The second algorithm reverses the approach of the Central Counter Barrier. Third we demonstrate a pW/CS-influenced modification of the second barrier.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MGB Barrier}
\label{ssec:new-mgb}
The original idea of the following algorithm is part of a private communique between Nicholas Mc~Guire, the pW/CS-inventor, and us. Figure~\ref{fig:pseudocode-mgb} presents the new barrier, which is a variation of the protocol first mentioned in \cite{bre13}. It is intended for shared memory architectures.

\begin{figure}[htbp]
	\centering
	\input{listings/mgb}
	\caption{Pseudocode for the MGB Barrier}
	\label{fig:pseudocode-mgb}
\end{figure}

Before going into detail, we first have to repeat some of the notation used in the pseudocode. It borrows from the C programming language. \&, $|$ and $\sim$ are the bitwise \emph{and}, \emph{or} and \emph{not} operator. The constant \texttt{threadIndex} is a thread identifying number between 0 and $\mathit{threadCount}-1$.

The protocol consists of two symmetric sub-barriers. It uses \texttt{current}, a variable which holds the number of the sub-barrier that is currently used, to switch between them.
Through \texttt{first} and \texttt{second} the barrier keeps track of which threads have arrived.
The variable \texttt{copy} is a local copy of either \texttt{first} or \texttt{second}.
A thread commits itself to \texttt{first} or \texttt{second} by setting its respective bit to 1. In the pseudocode we implement this bit set behaviour through integers and working with powers of two. If all threads committed themselves to one of the sub-barriers, it is a full bit set equal to the constant \texttt{full}.
All threads participating in the barrier synchronisation enter the same of the two parts.
Inside a loop participants try to synchronise. When successful the barrier is reset through the two subsequent assignments of \texttt{current} and either \texttt{first} or \texttt{second}.

Initially \texttt{first} and \texttt{second} are zero, and \texttt{current} is one. When a thread arrives at the barrier it will enter the upper if branch. It then asks which threads have already arrived, adds itself to the acquired bit set, and stores it back to the variable \texttt{first}.
He then repeatedly polls \texttt{first} in order to find out if its commitment attempt has been overwritten, i.e. its bit is not set anymore, by another thread's concurrently issued write operation, and secondly to determine whether the barrier is completed. The upper barrier is completed, if either \texttt{first} equals \texttt{full} or \texttt{current} is 2. The first thread to realise that \texttt{first} has been set to \texttt{full} will exit the loop, set \texttt{current} to 2 and continue its computational work. Subsequently all other threads eventually leave the barrier.
When entered again the same procedure is executed in the lower if branch using the variable \texttt{second} instead of \texttt{first}. A third repetition utilises the upper branch again.

The presented protocol reflects the ideas of pW/CS in that it loosely organises communication between threads and is less enforcing in the sense that threads may overwrite each others' commitment attempts.

In comparison to the Central Counter Barrier this approach does not rely on the existence atomic operations.

The number of threads this protocol supports is limited by the number of bits a processor can atomically commit, which is usually 64 bits. To alleviate this problem one can implement an array-based variant of the protocol. Instead of reading \texttt{first} and \texttt{second} one copies the array element by element. Comparing is \texttt{done} element by element as well. Instead of setting the variable one writes back only the one element where the thread's bit resides on. This way writing still requires only one atomic 64-bit commitment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{B1 Barrier}
\label{ssec:new-b1}

The following barrier protocol reverses the idea of the Central Counter Barrier (Section~\ref{sssec:background-current-shared}) to use a single global counter that all threads atomically access once and read repeatedly. Instead an array of boolean variables (one element for each thread), which is also accessed differently, is employed.
The algorithm's pseudocode, executed individually on each thread taking part, is depicted in Figure~\ref{fig:pseudocode-b1}.

\begin{figure}[htbp]
	\centering
	\input{listings/b1}
	\caption{Pseudocode for the B1 Barrier}
	\label{fig:pseudocode-b1}
\end{figure}

Initially each array element is false. Upon arriving at the barrier a thread registers itself by setting its array element to true. He subsequently queries all other threads for arrival. As soon as it finds one that has not yet arrived, it resets the loop counter and begins querying all others anew. Once all threads arrive, eventually each of them will leave, thus completing the barrier synchronisation.

Similar to the Central Counter Barrier, the protocol can be split into two phases (Figure~\ref{fig:diagram-central-counter}).
The difference between the two protocols is that, because each thread writes only to its own array element, they do not require atomic access to register themselves at the barrier, and during the reading phase each thread has to read each other thread's variable instead of polling just a single global one. The threads may concurrently write, but have to spend more time reading.

The Barrier cannot be repeated as is. A way to add resetting to the protocol is by using the repetition counter approach introduced in Section~\ref{ssec:background-building-blocks}. In order to avoid false~sharing~\cite{falsesharing}, which would majorly degrade performance, an actual implementation of the protocol allocates one cache line for each array element. Cache lines are usually 64 bytes large. Therefore, transforming the array into an array of counters does not require more memory.

A small improvement to the shown algorithm is to keep track of the first few threads that have successfully arrived. The loop counter is then reset to this number instead of 0.

Since we do not use any central information, this protocol can be implemented for both shared and distributed memory architectures.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{B2 Barrier}
\label{ssec:new-b2}
The B2 Barrier is a variation on the previous barrier in which less communication is traded for more computation.

In principle the following protocol can be implemented on both shared and distributed memory architectures. We now switch to a distributed setting, because preliminary analysis showed that the saved memory access cost in comparison to the additional computational overhead favours its use in a distributed scenario.

Figure~\ref{fig:pseudocode-b2} presents the B2 Barrier's RMA-style pseudocode. The notation used in this listing is explained in Section~\ref{ssec:new-mgb}.
\begin{figure}[htbp]
	\centering
	\input{listings/b2}
	\caption{Pseudocode for the B2 Barrier}
	\label{fig:pseudocode-b2}
\end{figure}
Each array element of \texttt{barrier} is located in the local memory of the process with the matching index. These elements are bit sets representing the knowledge of the owning process about the arrival of others.

When the first process arrives at the barrier it registers itself by setting its bit in its local array element and then continues to query the remote processes. It only acquires a remote array element, if it did not yet successfully do so, i.e. the remote process's bit is not set in the local bit set. If this is the case, it fetches the remote element and adds it to its own, thus adding the arrival of all processes the remote end knows about to its own bit set. Querying the other processes is repeated until the bit set indicates that all processes arrived.

Comparing this behaviour to the B1 Barrier, a process now locally keeps track of the processes that have arrived and only queries those where the owning process did not yet get an affirmative answer.
The protocol does not maintain the differentiation between reading and writing phase since it keeps updating its local variable during the loop.

This protocol writes only locally, and reads only remotely. It does not rely on the existence of atomic operations.

In comparison to other distributed memory barrier protocols, e.g. the Dissemination Barrier (Section~\ref{sssec:background-current-distributed}), the communication pattern is largely random. The order of remote access depends on remote memory access latency, local computation and memory access speed, the exact time of arrival of each process and operating system influences like scheduling. The effect of these factors combined makes the order of communication practically random.

Like the B1 Barrier this protocol is destructive. One can add resetting via the triple barrier method presented in Section~\ref{ssec:background-building-blocks}.

Like the MGB Barrier this protocol is restricted to 64 processes. An extension to arrays of bit sets can be implemented in the same way as described in Section~\ref{ssec:new-mgb}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleartooddpage
\section{Analysis and Comparison}
\label{sec:analysis}
% maybe distinguish extreme cases vs more normal cases. And the spectrum in between -- adds structure -- is nice.

In this section we will evaluate one commonly used barrier for each shared and distributed memory architectures. In both categories we compare the chosen protocol to one of our newly presented ones.

Early evaluations have shown the MGB Barrier to perform worse than the Central Counter Barrier, whereas the B1 and B2 Barrier show promise. Therefore, we restrict our attention to these two.

In the shared memory setting the Central Counter Barrier is pitted against the B1 Barrier. In the distributed memory case we compare the Dissemination Barrier to the B2 Barrier.

In our evaluation we assume no back-off strategy is used. This means if a variable is repeatedly requested inside a loop, it is fetched as often as program execution speed allows. The topic of back-off strategies is a subject in its own right and goes beyond the scope of this thesis.

The analysis is split into two parts. First, we present properties, perceivable without measurement and modelling, that are inherent to the protocols. Second, we employ model checking to quantify our observations and, thus, gain a deeper understanding of the chosen barriers.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{General Properties}
\label{sssec:analysis-general}
This section is divided into two parts: shared and distributed memory barriers.
In each we present the memory requirement of the algorithms.
For the Central Counter and B1 Barrier we additionally consider properties of the reading and writing phase.
In the distributed memory scenario we are also interested in the remote access behaviour of both chosen protocols.

%%%%%%%%%%%%%%%%%%%%
\paragraph{Shared Memory Barriers}
\label{sssec:analysis-general-shared}
% - don't talk cache specifics, because we have not given an introduction to MSI. Express yourself in simple terms.
% - make sure that people understand it is about shared read/write, not local variables.

The Central Counter Barrier (Figure~\ref{fig:pseudocode-central-counter} in Section~\ref{sssec:background-current-shared}) and the B1 Barrier (Figure~\ref{fig:pseudocode-b1} in Section~\ref{ssec:new-b1}) are both divided into two phases: the reading and the writing phase.

In the writing phase of the Central Counter Barrier concurrently issued add-fetch operations are serialised, because atomic operations on a single variable cannot be executed in parallel. On the other hand the B1 Barrier allocates one array element per thread. Therefore, all write operations can be performed concurrently. If all threads arrive in the same instant, the commitment of all threads takes only as long as one write operation does, therefore speeding up this process by a factor of $n$, where $n$ is the number of threads participating. In a more balanced scenario, i.e. processes arrive in intervals, this advantage over the Central Counter Barrier diminishes.

During the reading phase of the Central Counter Barrier a thread fetches a single variable repeatedly until it has the desired value. A thread in the B1 Barrier needs to read between 1 and $n$ array elements before it can leave the loop.

The B1 Barrier has the edge in the writing phase, whereas the Central Counter Barrier performs well during reading. We quantify the effect of this behaviour in Section~\ref{ssec:analysis-modelchecking}.

The memory required by the Central Counter Barrier is logarithmic in the number of threads. With a 64-bit counter up to $2^{64}$ threads can be synchronised. The B1 Barrier uses a linear amount of memory -- $\mathit{threadCount} \cdot 64$ bytes, assuming a cache line is 64 bytes large. This might seem like a lot, but since shared memory systems rarely exceed 512 cores, a maximum of only 32 KiB is used.

%%%%%%%%%%%%%%%%%%%%
\paragraph{Distributed Memory Barriers}
\label{sssec:analysis-general-distributed}
In this section we assume $n$ to be the number of processes participating in the barrier synchronisation.

The Dissemination Barrier (Figure~\ref{fig:pseudocode-dissemination} and \ref{fig:diagram-dissemination} in Section~\ref{sssec:background-current-distributed}) and the B2 Barrier (Figure~\ref{fig:pseudocode-b2} in Section~\ref{ssec:new-b2}) communicate very differently.

The minimum number of remote access operations required to perform a barrier synchronisation is $2 \cdot (n-1)$. Figure~\ref{fig:diagram-gather-broadcast} illustrates a simple gather and broadcast  barrier (process zero gathers all arrival notifications and broadcasts to all processes when the barrier is complete) using this number of operations. It does so in $2 \cdot  \lceil \log _2~n \rceil$ rounds. All operations inside a round can be executed in parallel. However rounds themselves are completed serially. This means the minimum execution time of this  barrier is $2 \cdot \lceil \log_2~n \rceil$ times the duration of a remote write operation. In order to lower the number of rounds to $\lceil \log _2~n \rceil$ the Dissemination Barrier issues $n \cdot \lceil \log _2~n \rceil$, more than the minimum needed, remote access operations. The added communication overhead is traded for a decrease in overall execution time.

\begin{figure}[htbp]
	\centering
	\input{tikz/diagram-gather-broadcast}
	\caption{Communication pattern of the gather and broadcast barrier}
	\label{fig:diagram-gather-broadcast}
\end{figure}

Because of its random communication pattern, there is no fixed number of remote accesses issued by the B2 Barrier. The number of successful, non-zero, remote reads, lies between $2 \cdot (n-1)$ and $n \cdot (n-1)$. The amount of unsuccessful reads is not limited, since the loop remotely polls other processes until all arrive.

Where a process in the Dissemination Barrier waits for one designated communication partner each round, the B2 Barrier does not wait for a single process but instead continues to poll the next one.

Requiring $\lceil \log _2~n \rceil$ rounds is obviously worse than $\log _2~n$. Especially from $n$, where $n$ is a power of two, to $n+1$ processes the number of rounds increases by 1. Therefore, the runtime of the Dissemination Barrier is expected to show a stair-esque behaviour. Every time the number of rounds increases the duration of the barrier increases by a large step, whereas adding more processes to the same number of rounds adds little overhead.
In contrast to this the execution time of the B2 Barrier is distributed evenly.

Synchronisation using the Dissemination and B2 Barrier requires the same amount of memory. In principle $\lceil \log _2~n \rceil$ bits per process suffice for both algorithms. The Dissemination variant shown in Figure~\ref{fig:pseudocode-dissemination} uses a total of $n^2$ bits, but, since each process receives from only $\lceil \log _2~n \rceil$ other processes, a modified array index calculation would allow the protocol to work with $n \cdot \lceil \log _2 ~n \rceil$ bits.

Assuming all processes arrive at a similar time, Figure~\ref{fig:diagram-dissemination} in Section~\ref{sssec:background-current-distributed} illustrates the progression through the rounds of the Dissemination Barrier. More often than not, processes arrive in large intervals, compared to the execution time of a barrier itself.
Therefore, some processes advance through the rounds before every process arrived.
But how far exactly can the protocol progress without all processes being present?

Suppose every process but the fourth has entered the barrier. Figure~\ref{fig:diagram-dissemination-progress} then shows the advance of the processes through the rounds.

\begin{figure}[htbp]
	\centering
	\input{tikz/diagram-dissemination-progress}
	\caption{Progress through the rounds of the Dissemination Barrier with process four not having entered}
	\label{fig:diagram-dissemination-progress}
\end{figure}

Every dotted line indicates a remote write operation that is not executed. Processes with dotted circles have not reached the current round.
Since process four does not notify process five of its arrival, in the second round process four and five do not advertise their arrival to process six and zero. The number of stuck processes doubles each round.
Once process four enters, the missing remote operations in all three rounds are still to be completed until the processes may leave the barrier.

Table~\ref{tab:table-dissemination-progress} illustrates which process knows of which other's arrival through the rounds of the Dissemination Barrier and for the B2 Barrier. The formula below each subtable title indicates the number of processes that are known to have arrived.

\begin{table}[htbp]
\centering
\caption{Which process knows of which other's arrival with process four not having entered}
\vspace{0.2cm}
\begin{minipage}{0.42\linewidth}
	\textbf{Diss. after round 1:} \\
	$(n-1) + (n-1) = 14$ \\
	\vspace{-0.1cm} \\
	\resizebox{5cm}{!}{
		\begin{tabular}{c | c c c c c c c c}
			  & 0        & 1        & 2        & 3        & 4 & 5        & 6        & 7 \\
			\hline
			0 & $\times$ &          &          &          &         &          &          & $\times$ \\
			1 & $\times$ & $\times$ &          &          &         &          &          &          \\
			2 &          & $\times$ & $\times$ &          &         &          &          &          \\
			3 &          &          & $\times$ & $\times$ &         &          &          &          \\
			4 &          &          &          & $\times$ &         &          &          &          \\
			5 &          &          &          &          &         & $\times$ &          &          \\
			6 &          &          &          &          &         & $\times$ & $\times$ &          \\
			7 &          &          &          &          &         &          & $\times$ & $\times$ \\
		\end{tabular}
	}
\end{minipage}
\begin{minipage}{0.42\linewidth}
	\textbf{Diss. after round 2:} \\
	$14 + 6 \cdot 2 = 26$ \\
	\vspace{-0.1cm} \\
	\resizebox{5cm}{!}{
		\begin{tabular}{c | c c c c c c c c}
			  & 0        & 1        & 2        & 3        & 4 & 5        & 6        & 7 \\
			\hline
			0 & $\times$ &          &          &          &   & $\times$ & $\times$ & $\times$ \\
			1 & $\times$ & $\times$ &          &          &   &          & $\times$ & $\times$ \\
			2 & $\times$ & $\times$ & $\times$ &          &   &          &          & $\times$ \\
			3 & $\times$ & $\times$ & $\times$ & $\times$ &   &          &          &          \\
			4 &          & $\times$ & $\times$ & $\times$ &   &          &          &          \\
			5 &          &          & $\times$ & $\times$ &   & $\times$ &          &          \\
			6 &          &          &          &          &   & $\times$ & $\times$ &          \\
			7 &          &          &          &          &   &          & $\times$ & $\times$ \\
		\end{tabular}
	}
\end{minipage}
\begin{minipage}{0.42\linewidth}
	\vspace{0.3cm}
	\textbf{Diss. after round 3}: \\
	$26 + 4 \cdot 4 = 44$ \\
	\vspace{-0.1cm} \\
	\resizebox{5cm}{!}{
		\begin{tabular}{c | c c c c c c c c}
			  & 0        & 1        & 2        & 3        & 4 & 5        & 6        & 7 \\
			\hline
			0 & $\times$ &          &          &          &        & $\times$ & $\times$ & $\times$ \\
			1 & $\times$ & $\times$ &          &          &        &          & $\times$ & $\times$ \\
			2 & $\times$ & $\times$ & $\times$ &          &        &          &          & $\times$ \\
			3 & $\times$ & $\times$ & $\times$ & $\times$ &        &          &          &          \\
			4 & $\times$ & $\times$ & $\times$ & $\times$ &        & $\times$ & $\times$ & $\times$ \\
			5 & $\times$ & $\times$ & $\times$ & $\times$ &        & $\times$ & $\times$ & $\times$ \\
			6 & $\times$ & $\times$ & $\times$ &          &        & $\times$ & $\times$ & $\times$ \\
			7 & $\times$ & $\times$ & $\times$ & $\times$ &        &          & $\times$ & $\times$ \\
		\end{tabular}
	}
\end{minipage}
\begin{minipage}{0.42\linewidth}
	\vspace{0.3cm}
	\textbf{B2 Barrier:} \\
	$(n-1) \cdot (n-1) = 49$ \\
	\vspace{-0.1cm} \\
	\resizebox{5cm}{!}{
		\begin{tabular}{c | c c c c c c c c}
			  & 0        & 1        & 2        & 3        & 4 & 5        & 6        & 7 \\
			\hline
			0 & $\times$ & $\times$ & $\times$ & $\times$ &   & $\times$ & $\times$ & $\times$ \\
			1 & $\times$ & $\times$ & $\times$ & $\times$ &   & $\times$ & $\times$ & $\times$ \\
			2 & $\times$ & $\times$ & $\times$ & $\times$ &   & $\times$ & $\times$ & $\times$ \\
			3 & $\times$ & $\times$ & $\times$ & $\times$ &   & $\times$ & $\times$ & $\times$ \\
			4 &          &          &          &          &   &          &          &          \\
			5 & $\times$ & $\times$ & $\times$ & $\times$ &   & $\times$ & $\times$ & $\times$ \\
			6 & $\times$ & $\times$ & $\times$ & $\times$ &   & $\times$ & $\times$ & $\times$ \\
			7 & $\times$ & $\times$ & $\times$ & $\times$ &   & $\times$ & $\times$ & $\times$ \\
		\end{tabular}
	}
\end{minipage}
\label{tab:table-dissemination-progress}
\end{table}

Because the number of blocked processes doubles each round the ratio of stuck to not-stuck processes is worse for non-power-of-two than it is for power of two process counts.

The B2 Barrier does not work in rounds and does not have designated communication partners. It therefore fetches all other process's arrival notifications while one process is missing.

When the last process arrives, the Dissemination Barrier requires at least as many serial remote writes as there are rounds, whereas the B2 Barrier issues its remaining remote read operations concurrently. The number of remaining remote access operations is $2^{\lceil \log_2~n \rceil} - 1$ for the Dissemination Barrier and $n$ for the B2 Barrier.

We attempt to quantify the influence of this behaviour on the execution time and energy consumption in the following section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model Checking}
\label{ssec:analysis-modelchecking}
The formal properties of the chosen protocols can be divided into two broad categories: functional and quantitative properties. Functional properties describe the supposed behaviour of the barriers, whereas quantitative properties express conditions that rely on probabilistic aspects of the protocols, e.g. execution time.

To check correctness of the barriers we model every instance where execution is random through non-deterministic choice. This yields a Markov decision process, which we then use a basis for automated analysis with the SPIN model checker.

To model the inherent randomness of our algorithms we use exponential distributions. This way we obtain continuous-time Markov chains (CTMCs) which serve as a basis for formal quantitative analysis using the PRISM model checker.

Models are obtained in a compositional way, using action labels and global state to synchronise.

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Preliminaries}
\label{sssec:analysis-modelchecking-preliminaries}
In this section we repeat principles of continuous-time Markov chains that are of interest to our analysis. Further details can be found in textbooks on Markov chains, e.g. \cite{kul95, ks76}. Some of this Section borrows from \cite{bai13}

If $S$ is a finite set, then a distribution on $S$ is a function $\nu:S \rightarrow [0,1]$ with $\sum\limits_{s \in S} \nu (s) = 1$.

A CTMC $\mathcal{M}$ is a tuple $(S, \mathit{Act}, R, \mu)$ where $S$ is a finite set of states, $Act$ a finite set of action names and $R$ a function of type $S \times \mathit{Act} \times S \rightarrow \mathbb{R}_{\ge 0}$, called the rate matrix of $\mathcal{M}$. $\mu$ is a distribution on $S$ specifying the probabilities for the initial states.

If $R(s, \alpha, s') = \lambda$ (short $s \xrightarrow{\lambda : \alpha} s'$) and $\lambda > 0$ then $\mathcal{M}$ has a transition from $s$ to $s'$ with action label $\alpha$ and rate $\lambda$.
$\lambda$ specifies the rate of the exponential distribution. That means the probability for the transition $s \xrightarrow{\lambda : \alpha} s'$ to be ready for firing some time in the interval $[0,t]$ is $1-e^{- \lambda t}$. The average delay of this transition is $\frac{1}{\lambda}$.
If $R(s, \alpha, s') = 0$ then there is no transition in $\mathcal{M}$ from $s$ to $s'$ via $\alpha$.
The choice between multiple enabled transitions is made by the following rule.
The probability to trigger a particular transition $s \xrightarrow{\lambda : \alpha} s'$ is $P(s, \alpha, s') = \frac{\lambda}{E(s)}$ where $E(s)$ is the exit rate of state $s$, i.e. the sum of the rates of all outgoing transitions of state $s$.
The probability that $s \xrightarrow{\lambda : \alpha} s'$ will fire within $t$ time units is then $P(s, \alpha, s') \cdot (1 - e^{- E(s) \cdot t})$.

Paths in a CTMC are sequences of consecutive transitions augmented by the time points when they are taken.
We employ the logic CSL~\cite{assb96, bhhk00, knp07} to analyse such transition systems.
To specify sets of infinite paths, we will use an LTL-like notation, such as $\lozenge T$ (``eventually T'') and $\square T$ (``always T'') where $T$ is a set of states.
Instead of naming states we will oftentimes use state predicates, combined using propositional formulas, to describe sets of states. Their meaning will be obvious from the context.

We will also study reward based properties formalised using the logic CSRL~\cite{bhhk00, knp07}.
This requires the extension of $\mathcal{M}$ by two reward functions. The state reward function $\mathit{srew} : S \rightarrow \mathbb{R}$ specifies the reward earned per time unit while being in state $s$. The transition reward function $\mathit{trew} : S \times S \rightarrow \mathbb{R}$ assigns a reward to each transition between two states regardless of action labels.
For finite paths one can then reason about accumulated reward and reachability reward.
Suppose $\pi$ is a finite path where the underlying state sequence is $s_1 s_2 ...s_n$ and let $t_0 = 0$ and $t_i$ the time point where $\pi$ takes the $i$-th transition.
The accumulated reward of $\pi$ is then
\begin{center}
	$\mathit{Rew}(\pi) = \mathlarger{\sum}\limits_{i=0}^{n-1} \big( (t_{i+1} - t_i) \cdot \mathit{srew}(s_i) + \mathit{trew}(s_i, s_{i+1}) \big)$
\end{center}
It is the sum, over all states but the last, of the state rewards multiplied by the time spent in each state plus the reward assigned to each transition between consecutive states in $\pi$.
The reachability reward is defined as
\begin{center}
	$\mathrm{ReaRew}(\pi, \Phi) = \mathit{Rew}(\pi(s_0:s_j))$
\end{center}
where $\Phi$ is a propositional formula describing a set of states, $j = min \{ i~|~s_i \models \Phi \}$ and  $\pi(s_0:s_j)$ stands for the sub-path of $\pi$ starting in $s_0$ and ending in $s_j$.
The extension of these notions from paths to states and CTMCs is done the usual way and can be found in \cite{bhhk03}

Analysing the protocols, we will for example deal with the state reward function that assigns value 1 to each state. In this case $\mathrm{ReaRew}(\Phi)$ can be interpreted as the average number of processor clock cycles to reach a state where the formula $\Phi$ holds.
We also wish to count remote memory access operations. For this we assign value 1 to each transition that represents such an operation. $\mathrm{ReaRew}(\Phi)$ is then the average number of remote memory access operations issued until reaching a $\Phi$-state.

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Modelling}
\label{sssec:analysis-modelchecking-modelling}
To analyse functional and quantitative aspects of the chosen barrier protocols, we model them using two different formalisms and two different software tools. We use ordinary non-deterministic transition systems and the SPIN~\cite{spin, hol97} model checker to verify functional properties. To analyse quantitative properties we employ CTMCs and PRISM~\cite{prism, knp09}.

To verify the functional correctness of a barrier, we need to include a reset mechanism into the model, which is not needed for quantitative analysis. By stripping this unnecessary information off the probabilistic model we shrink its state space. This enables us to verify properties more quickly and/or of larger models.

Depending on circumstances we describe part of a system as non-stochastic control flow diagram and part as CTMC. Control flow diagrams contain local variables, shared variables, control flow information and need to be unwound to form a proper CTMC. The state space of the resulting CTMC is then a combination of control flow location and variable assignments.

The CTMCs of the protocols are composed of multiple modules.
Composed modules are executed in an interleaved fashion except if transitions require synchronising actions. A synchronising action is an action that can be triggered by at least two modules. A transition with such an action can only fire if all modules, that are able to fire this action eventually, are presently in a state where a transition with this action label is enabled. The rate of such a simultaneous transition is determined by any one module. For each simultaneous transition, in our case, only one module specifies a rate.
The presented way of composing modules corresponds to the following SOS rules.

If no other module uses action label $\alpha$:
\begin{center}
	$\mathlarger{\frac{s \xrightarrow{\lambda : \alpha} s'}{\langle s , \overline{x} \rangle \xrightarrow{\lambda : \alpha} \langle s', \overline{x} \rangle}}$
\end{center}
where $\overline{x}$ is the tuple of local state of all other modules. The module's execution is interleaved with that of all other modules.
If $\alpha$ is used by two modules:
\begin{center}
	$\mathlarger{\frac{s \xrightarrow{\lambda : \alpha} s', t \xrightarrow{\alpha} t'}{\langle s, t , \overline{x} \rangle \xrightarrow{\lambda : \alpha} \langle s', t', \overline{x} \rangle}}$
\end{center}
Only if $\alpha$ is enabled in both modules, these two transitions fire simultaneously.

Analogous rules apply for more than two modules sharing an action label.

%%%%%%%%%%
\paragraph{Shared Memory}
\label{sssec:analysis-modelchecking-modelling-shared-memory}
In order to exchange information threads write to and read from the same memory location.
Memory access latency is very large in comparison to the duration of arithmetic operations.
Because synchronisation is mainly about exchanging information between threads, the execution time of barrier protocols is dominated by memory access latency.

Usually memory access is cached. That means repeatedly reading a value is quick, because upon first reading it the processor places a copy of it near its execution units. When a cached value gets changed by another thread that shares it, the cached value needs to be thrown away because otherwise it would soon be incoherent.
In order to model execution time and energy consumption of a barrier protocol, we therefore model the caching behaviour of threads.

We identify a shared variable with the cache line it resides on. A cache line is usually a 64 bytes large unit of contiguous memory to which all cache manipulation operations are applied as a whole. It increases the granularity of address space, which is usually one byte, to decrease management overhead for caching.
Each thread that has access to this shared variable has one copy of such a cache line.
Synchronisation between cache line copies and with other CTMC modules, like the actual algorithm to be modelled, is done via synchronising actions. The operation triggering thread's cache line copy determines the rate of the transition.
For convenience we use combinations of action labels like $\alpha \lor \beta$ meaning that there are two transitions, one with action $\alpha$ and one with $\beta$ between two states. Either action may trigger the transition.

We use the MSI cache coherency protocol, extended by support for atomic operations, to model cache consistency. The MSI protocol is a basic cache model. Further details about it can be found online at for example~\cite{msi}.

In the MSI model a cache line copy can be in one of three states. \emph{Modified} means the thread has the only up-to-date copy of the cache line and all other copies are \emph{invalid}. If a cache line copy is invalid it is outdated and therefore not usable. Being in the \emph{shared} state means the thread has an up-to-date copy, but other threads may have a correct copy, too. We extend these three states by a fourth, called \emph{atomic}. Its meaning is the same as modified, except that as long as a cache line copy is in this state, no other thread may access it.

Figure~\ref{fig:model-shared-memory} illustrates how a cache line copy changes its state. Solid transitions are fired by events occurring on the thread that owns the cache line copy, whereas dashed transitions are due to events triggered by other cache line copies.

\begin{figure}[htbp]
	\centering
	\input{tikz/model-shared-memory}
	\caption{CTMC for one shared memory variable}
	\label{fig:model-shared-memory}
\end{figure}

For example if a thread reads a variable and its own cache line copy is invalid, it first needs to make sure that all other threads take notice and change its cache line copy state to shared in case it was modified before. After this the thread fetches the cache line from main memory, marks its own copy shared, reads the variable and continues program execution.

The durations of the cache line copy state transitions are as follows.
Reading a modified or shared copy is considered instantaneous, because the copy is up-to-date and no other copies need to be notified.
If a thread whose copy is invalid reads a variable, it has to advertise its intent to all other copies. If another copy is modified, it needs to switch to the shared state. Meanwhile the reading thread fetches an up-to-date copy from main memory, sets the state of its local copy to shared, and finally continues program execution. This usually takes around 50 processor clock cycles.
A write operation on a modified variable is considered instantaneous, since no other thread has an up-to-date copy and therefore no one needs to be notified.
If the cache line copy in question is shared or invalid, it broadcasts an invalidation request to all other copies, and waits until every copy fulfills the request. After this the thread can safely mark its copy modified and finally write to it. This operation commonly takes about 100 cycles.
Entering the atomic state works exactly like entering the modified state.
During the atomic state the cache line copy postpones all requests to this copy until it transitions to a different state. After leaving the atomic state, it instantly begins to answer these requests. Therefore, leaving the atomic state is considered instantaneous.

The exact number of processor clock cycles these operations take is strongly hardware-dependent. For the CTMC model we assume a cache read to take 50 cycles, whereas writing and entering an atomic operation requires 100 cycles. The rates of the transitions corresponding to the described events is then the reciprocal of the cycle count.
The rate of an instantaneous operation, which is assumed to be one cycle, is $\lambda_c = 1$. The rate of a cache read is $\lambda_r = \frac{1}{50}$. And the rate of initiating a write or an atomic operation is $\lambda_w = \lambda_a = \frac{1}{100}$

For technical reasons we switch the rate of entering and leaving an atomic operation. The intuitive result is the same.

%%%%%%%%%%
\paragraph{Central Counter Barrier}
\label{sssec:analysis-modelchecking-modelling-central-counter}
The Central Counter Barrier uses one shared variable. Therefore, the final CTMC is composed of one cache line copy module per thread and one algorithm module per thread.
The algorithm's control flow diagram (Figure~\ref{fig:model-central-counter}) directly results from the pseudocode in Figure~\ref{fig:pseudocode-central-counter} (Section~\ref{sssec:background-current-shared}).

\begin{figure}[htbp]
	\centering
	\input{tikz/model-central-counter}
	\caption{Control flow diagram for one Central Counter Barrier thread}
	\label{fig:model-central-counter}
\end{figure}

Initially variable \texttt{barrier} is set to \texttt{threadCount}.
The synchronisation with the cache line copy module is hinted at in grey.
The rates for the shared operations are determined by the thread's cache line copy module, depending on its internal state.

We emulate the interval in which threads arrive at the barrier via an exponential distribution with rate $\lambda_{\mathit{work}}$. One can imagine this as a period where the thread is performing computational work.

The atomic operation has been shortened for readability and really contains multiple transitions. The thread initiates the atomic operation on \texttt{barrier}, reads it, subtracts one from it, writes it back, and finally ends the atomic operation.

Unlike the quantitative model, the functional model repeatedly executes the protocol. Resetting is achieved via the triple barrier method, which is explained in Section~\ref{ssec:background-building-blocks}.

%%%%%%%%%%
\paragraph{B1 Barrier}
\label{ssssec:analysis-modelchecking-modelling-b1}
The B1 Barrier (Figure~\ref{fig:pseudocode-b1} in Section~\ref{ssec:new-b1}) allocates one cache line per array element. Therefore, the final model is composed of $n$ algorithm modules plus $n^2$ cache line copy modules where $n$ is the number of threads.

The algorithm's control flow is illustrated in Figure~\ref{fig:model-b1}. As for the Central Counter Barrier, the B1 Barrier starts with an initial work period, its rates are determined by thread's cache line copy module, and the grey transition labelling indicates the interaction with the cache.

\begin{figure}[htbp]
	\centering
	\input{tikz/model-b1}
	\caption{Control flow diagram for one B1 Barrier thread}
	\label{fig:model-b1}
\end{figure}

Initially each array element is set to false and the loop index to zero.
In comparison to the pseudocode, the assignments are slightly different, but ultimately result in the same behaviour.

Deviating from the quantitative model, the functional model implements repetition and resetting by replacing the boolean array elements with counters. The principle is described in detail in Section~\ref{ssec:background-building-blocks}.
Additionally, the functional model splits some of the transition into multiple transitions to allow for more interleaving and therefore reveal more potential errors in the protocol.

Splitting these transitions in the quantitative model would increase model size majorly. It is important not to merge multiple transitions that depend on shared memory. Merging and splitting other transitions influences timing in a minor way.

%%%%%%%%%%
\paragraph{Dissemination Barrier}
\label{ssssec:analysis-modelchecking-modelling-dissemination}
The Dissemination Barrier (Figure~\ref{fig:pseudocode-dissemination}, Section~\ref{sssec:background-current-distributed}), as a representative for distributed barrier algorithms, does not have shared memory. Therefore, the CTMC model consists of one algorithm module per process and nothing else.

Figure~\ref{fig:model-dissemination} depicts the control flow diagram of the protocol.
The distance is initialised to one and each array element to false.
As in the two previous paragraphs a work period precedes the protocol.
The expressions \texttt{to} and \texttt{from} are shorthands for \texttt{processIndex $\pm$ dist (mod processCount)}.

\begin{figure}[htbp]
	\centering
	\input{tikz/model-dissemination}
	\caption{Control flow diagram for one Dissemination Barrier process}
	\label{fig:model-dissemination}
\end{figure}

Local operations take far less time than remote operations -- between ten to a thousand times usually. Therefore, we assign the $\lambda_c$ (one processor clock cycle) to local operations.
The duration of a remote write (also called put) operation, which is strongly hardware-dependent, is assumed to take 100 cycles. Therefore, we assign $\lambda_p = \frac{1}{100}$ to each corresponding transition.

In the CTMC model, resulting from unwinding this control flow diagram, a remote write operation is represented as a synchronised action. When a process executes a put, the target process then changes its local state accordingly.

The functional model includes repetition and resetting with counters instead of boolean variables as described in Section~\ref{ssec:background-building-blocks}.

%%%%%%%%%%
\paragraph{B2 Barrier}
\label{ssssec:analysis-modelchecking-modelling-b2}
The B2 Barrier, introduced in Section~\ref{ssec:new-b2}, is modelled as a composition of one algorithm module per process.

Figure~\ref{fig:model-b2} illustrates the control flow of the protocol.
Initially each bitset is empty and the loop index is set to \texttt{processIndex}.
The rates of operations are the same as for the Dissemination Barrier except that, instead of remote writing, we assign a rate of $\lambda_g = \frac{1}{100}$ to remote read (also called get) transitions.

\begin{figure}[htbp]
	\centering
	\input{tikz/model-b2}
	\caption{Control flow diagram for one B2 Barrier process}
	\label{fig:model-b2}
\end{figure}

The functional model additionally includes repetition and resetting using the triple barrier method explained in Section~\ref{ssec:background-building-blocks}.
Similar to previously presented models the functional model splits some of the transitions into multiple ones to allow for more interleaving. This way more potential errors can be revealed. The quantitative model refrains from splitting them, to reduce the model's size.

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Functional Properties}
\label{sssec:analysis-modelchecking-functional-properties}
In this section we use the words \emph{thread} and \emph{process} interchangeably, because conditions of functional correctness apply to both shared and distributed memory protocols.

There are two basic conditions that every barrier protocol has to fulfill.
It has to properly synchronise threads, i.e. no thread leaves before all threads arrived at the barrier, and it may not deadlock.
We express these two properties using the linear temporal logic.

\vspace{0.2cm}
\noindent \textbf{(A)} \emph{``A thread may only exit the barrier, if all threads have entered it.''}
\begin{center}
	$\square ( \mathit{one\_left} \implies \mathit{all\_entered} )$
\end{center}

\vspace{0.2cm}
\noindent \textbf{(B)} \emph{``If all threads entered the barrier, each one leaves it in a finite amount of time.''}
\begin{center}
	$\square ( \mathit{all\_entered} \implies \lozenge \mathit{all\_left} )$
\end{center}

We use state predicates to describe sets of states with the intuitive meaning. For example $\mathit{all\_left}$ represents the set of states where each thread has passed the barrier.

In order for B to hold we have to assume that each thread is scheduled infinitely often. This property is also called \emph{fairness}. In fact assuming weak fairness already suffices here. Without this mild side assumption a barrier protocol can deadlock since one thread might never leave the barrier.

We implemented each of the four chosen protocols in the modelling language Promela and verified properties A and B using the SPIN model checker.
All barrier protocols fulfill both conditions.

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Quantitative Properties}
\label{sssec:analysis-modelchecking-quantitative-properties}
As in the previous section we use the words \emph{thread} and \emph{process} interchangeably, since some properties apply to both shared and distributed memory barrier protocols.

The section is divided into two parts. We first identify and formalise interesting quantitative properties of barrier protocols. Second, we present the results of the automated analysis.

%%%%%%%%%%
\paragraph{Identification and Formalisation}
\label{ssssec:analysis-modelchecking-quantitative-properties-identification}
There are a number of interesting questions to ask about barrier protocols. First, let us enumerate what we can quantify.

Measuring time is obviously of great interest to us. We want to determine how many processor clock cycles certain operations take. In order to measure cycles, we assign a state reward of 1 to each state. The accumulated reward is then the desired value.

Another interesting quantity to measure is energy consumption (unit joule) and its rate, measured in watt (joule per second).
To measure these, we use a mix of transition and state rewards.
The following, very limited assumptions are based on a sample measurement conducted on a personal laptop of ours.
We assume a processor clock rate of 2.5 GHz to convert between seconds and cycles.
We assume the processor to consume 11 watts as a baseline.
Local operations are assigned no reward.
For shared memory protocols we assign a cost of 2 nanojoules (nJ) to each instantaneous memory operation and 200 nanojoules to slower memory operations.
For distributed memory we assume a cost 200 nanojoules per remote memory operation and 2 nanojoules for each local memory operation.

For distributed memory protocols we are also interested in the number of remote operations issued. To obtain this measure we assign a reward of 1 to each corresponding transition and take the accumulated reward.

Having listed the three measures we want to quantify, we now consider the points in time when we want to measure.
There are four of these: the moment when the first (A), and the last (B), thread enters the barrier, and the instant when the first (C), and last (D), thread leaves the barrier.
For the shared memory barriers we additionally consider the moment when the writing phase ends (W), i.e. when the last thread finishes writing.
Since the Dissemination Barrier consists of multiple rounds, we are also interested in measuring at the end of each round (R$i$). That is the point in time when the last thread leaves round $i$.

Formalising this enumeration we present the list of properties used for model checking.

\vspace{0.2cm}
\noindent \textbf{(A)} \emph{``Average reward until any one thread entered the barrier''}
\begin{center}
	$\mathrm{ReaRew}( \mathit{one\_entered} )$
\end{center}

\vspace{0.2cm}
\noindent \textbf{(B)} \emph{``Average reward until all threads entered the barrier''}
\begin{center}
	$\mathrm{ReaRew}( \mathit{all\_entered} )$
\end{center}

\vspace{0.2cm}
\noindent \textbf{(C)} \emph{``Average reward until any one thread left the barrier''}
\begin{center}
	$\mathrm{ReaRew}( \mathit{one\_left} )$
\end{center}

\vspace{0.2cm}
\noindent \textbf{(D)} \emph{``Average reward until all threads left the barrier''}
\begin{center}
	$\mathrm{ReaRew}( \mathit{all\_left} )$
\end{center}

\vspace{0.2cm}
\noindent \textbf{(W)} \emph{``Average reward until all threads finished writing''}
\begin{center}
	$\mathrm{ReaRew}( \mathit{all\_done\_writing} )$
\end{center}

\vspace{0.2cm}
\noindent \textbf{(R$i$)} \emph{``Average reward until all threads left round $i$''}
\begin{center}
	$\mathrm{ReaRew}( \mathit{all\_left\_round_{i}} )$
\end{center}

\noindent The state predicates have the intuitive meaning. For example $\mathit{all\_left\_round_{i}}$ describes the set of states where all threads passed round $i$.
We additionally consider combinations of the above properties, for example D minus B, which measures the average reward accumulated between the last thread entering the barrier and the last thread leaving the barrier.

There are two main parameters to variate for the measurements. The first one is the number of threads participating in the barrier synchronisation. Because the model size grows exponentially in the number of threads we are mostly restricted to thread counts of up to four.
The second value to variate is the initial work period's rate. By changing the work rate we can consider scenarios where threads arrive in smaller and larger intervals.

%%%%%%%%%%
\paragraph{Evaluation}
\label{ssssec:analysis-modelchecking-quantitative-properties-evaluation}
We formalised the four chosen barrier protocols using CTMCs and the suggested properties using the logic CSRL.
We then supplied the PRISM model checker with both, in order to retrieve the desired quantitative model checking results.
In the following section we present and comment on these results.
The evaluation is split into shared and distributed memory protocols.

%%%%%
\subparagraph{Shared Memory Barriers}
\label{sssssec:analysis-modelchecking-quantitative-properties-evaluation-shared}
First, we assume the initial work period to take 100 cycles on average, which is extremely low considering that barrier synchronisation takes more time than the actual computation task.

Figure~\ref{fig:c1-time-work-100-B-C-D} presents the basic timing of our two shared memory barriers for different thread counts. We exclude the time before the last thread completes its work period, because the barrier cannot finish earlier.
The execution time until the last thread leaves the barrier (dark red) is similar for both the Central Counter Barrier and the B1 Barrier.
Considering the first thread to leave, the B1 Barrier performs better.

\begin{figure}[htbp]
	\centering
	\includegraphics[width=10cm]{charts/c1-time-work-100-B-C-D}
	\caption{Average execution time for the Central Counter Barrier and B1 Barrier, and a short work period}
	\label{fig:c1-time-work-100-B-C-D}
\end{figure}

Figure~\ref{fig:c1-work-100-partition} shows the total time spent and energy consumed for both barriers and thread counts between two and four.
The diagram presents the four basic points in time we chose, and the moment the last thread finishes writing, in a stacked fashion.
Phase A covers the accumulated time or energy from the starting point until the first thread enters the barrier.
During B there is at least one thread that has not entered the barrier. When the last thread arrives, the writing phase begins.
The writing phase covers the area where at least one thread has not committed itself to the barrier.
Phase C describes the interval in which every thread has written its date, but no thread has left the barrier, yet.
Phase D depicts the accumulated time or energy from the first to the last thread leaving the barrier.
\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.53\linewidth}
		\includegraphics[height=4.3cm]{charts/c1-time-work-100-partition}
	\end{minipage}
	\begin{minipage}{0.46\linewidth}
		\includegraphics[height=4.3cm]{charts/c1-energy-work-100-partition}
	\end{minipage}
	\caption{Average execution time and energy consumption for the Central Counter Barrier and B1 Barrier, and a short work period}
	\label{fig:c1-work-100-partition}
\end{figure}

The length of phase A and B depends only on the thread count. They are equally long for both protocols.
In the Central Counter Barrier the last thread to commit itself, very likely immediately exits the barrier. Therefore, there is few time, on average, between the last thread writing and the first one leaving the barrier (green).

As conjectured in Section~\ref{sssec:analysis-general-shared} writing is quicker for the B1 Barrier and reading, i.e. phase C and D combined, for the Central Counter Barrier.

All phases take longer with increasing thread count, because of the CTMC semantic for simultaneously enabled transitions, described in Section~\ref{sssec:analysis-modelchecking-preliminaries}. The exit rate of a transition is the sum of rates of all currently enabled transitions.
In particular suppose you have $n$ concurrent threads of execution, modelled as interleaved CTMC modules, and in each a transition with rate $\lambda$ is enabled. The average delay of the first transition to fire is then $\frac{1}{n \cdot \lambda}$. The average time of a parallel execution of all $n$ transitions is $\sum_{i=1}^{n} \frac{1}{i \cdot \lambda}$ rather than the intuitive $\frac{1}{\lambda}$.
For example the expected number of cycles until the first thread enters the barrier is $\frac{100}{n}$. The last thread is expected to enter the barrier after roughly 150, 183 and 208 cycles, for two, three and four processes.
Because of this influence, phase D for the Central Counter Barrier takes 50, 75 and 92 cycles for two, three and four threads. It is expected to be about the same across all thread counts, since each thread issues exactly one shared read operation.
For the same reason the duration of the writing phase for the B1 Barrier should be around 100 cycles instead of 125, 138 and 146 for two, three and four threads.

The energy consumption is similar to the timing.
The B1 Barrier requires more energy than the Central Counter Barrier, since it issues more memory operations in a similar amount of time.
The writing phase of the Central Counter Barrier uses few energy since it spends considerable time waiting for atomic operations to finish, during which no operations on the same variable are performed.
Where the Central Counter Barrier's energy consumption scales similarly to the execution time, the B1 Barrier's energy consumption grows faster than the time needed.
Because a thread in the waiting loop of the B1 Barrier iterates between 1 and $n$ times, where $n$ is the number of threads, before leaving the barrier, the two last phases consume considerably more energy than in the Central Counter Barrier.

Aside from the accumulated energy consumption its rate during the different phases, depicted in Figure~\ref{fig:c1-power-work-100}, is also interesting.
Phase A uses the baseline power consumption of 11 watts.
The overall power consumption of the Central Counter Barrier lies between 16 and 18.3 watts. The B1 Barrier's is considerably higher with 19.4 to 30.6 watts.
Because the Central Counter Barrier uses atomic operations and waits busily on only one variable, its power consumption is less in almost every phase.
The only part where power consumption is similar, is phase D. The Central Counter barrier only issues shared read operations in this phase, whereas the B1 Barrier executes a mix of shared and local read operations. The execution rate to energy reward ratio is similar, though.
Both Barriers have very high power consumption between the moment the last thread commits itself and the first one leaves (green). For the Central Counter Barrier this is due to the shortness of this phase. The B1 Barrier issues almost exclusively shared read operations, which are modelled to consume more energy per cycle than other operations.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=10cm]{charts/c1-power-work-100}
	\caption{Average energy consumption rate for the Central Counter Barrier and B1 Barrier, and a short work period}
	\label{fig:c1-power-work-100}
\end{figure}

Figure~\ref{fig:c1-time-work-100-percent} shows the ratio of time spent writing versus reading for both protocols.
Writing begins when the first thread enters the barrier and ends when the last thread announces its arrival.
Reading begins when the last thread committed itself and ends when the barrier completes.
Deviating from our expectation, that the writing percentage would increase, the Central Counter Barrier's ratio is the same across all thread counts. This is due to the CTMC interleaving behaviour, explained earlier in this section.
As expected, from our informal analysis in Section~\ref{sssec:analysis-general-shared}, with increasing thread count the ration of writing to reading time for the B1 Barrier decreases.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=10cm]{charts/c1-time-work-100-percent}
	\caption{Average time spent reading versus writing for the Central Counter Barrier and B1 Barrier, and a short work period}
	\label{fig:c1-time-work-100-percent}
\end{figure}

Having reviewed different metrics for a rather short initial work period, we now have a look at similarly organised figures with an increased work period duration of 1000 cycles. Note that in comparison to real life scenarios this is still small. According to \cite{rab00} each process at a barrier spends 8194 cycles idly\footnote{1813 microseconds accumulated idle time at 450 MHz on 99.6 processes}. But an average of 1000 cycles is already enough to emulate threads arriving in intervals large enough so that each can conduct its desired work before the next one arrives. Increasing the work period further does not reveal new effects.

As one can observe in Figure~\ref{fig:c1-time-work-1000-B-C-D} the basic execution time of the barriers is very different from before.
Being naturally at a disadvantage in the reading phase, and because write operations are serialised due to the large arrival intervals, the B1 Barrier performs worse in every respect.
The duration between the last thread to enter and the first thread to leave for the Central Counter Barrier converges to 100 cycles -- one atomic operation plus a few additional cycles.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=10cm]{charts/c1-time-work-1000-B-C-D}
	\caption{Average execution time for the Central Counter Barrier and B1 Barrier, and a long work period}
	\label{fig:c1-time-work-1000-B-C-D}
\end{figure}

Figure~\ref{fig:c1-work-1000-partition}, again, shows the total time spent and energy consumed during the different phases.
Phases A and B are excluded because they are large and behave similarly for both protocols.
\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.53\linewidth}
		\includegraphics[height=4.3cm]{charts/c1-time-work-1000-partition}
	\end{minipage}
	\begin{minipage}{0.46\linewidth}
		\includegraphics[height=4.3cm]{charts/c1-energy-work-1000-partition}
	\end{minipage}
	\caption{Average execution time and energy consumption for the Central Counter Barrier and B1 Barrier, and a long work period}
	\label{fig:c1-work-1000-partition}
\end{figure}

The timing behaves as expected. The Central Counter Barrier needs one atomic operation and a few more short instructions from the last thread to enter until the first leaves. The B1 Barrier uses a very similar amount of time -- a write operation and a few read operations.
Since changing the work period only influences the phases before each thread has committed its date, the interval between C and D is the same as in Figure~\ref{fig:c1-work-100-partition}.

Energy consumption during the writing phase of the Central Counter Barrier is constant across all thread counts, because atomic operations block all other threads from busily reading the barrier variable.
The B1 Barrier in turn polls constantly, and therefore energy consumption increases with increasing thread count.

The rate of energy consumption behaves similarly to the previous measurement, where we used an average work period duration of 100 cycles.
The overall power consumption is now dominated by phases A and B, which are similar across protocols. The B1 Barrier requires between 15.3 and 23.5 watts and the Central Counter Barrier uses 14.6 to 20.9 watts. The B1 Barrier can catch up considerably.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=10cm]{charts/c1-power-work-1000}
	\caption{Average energy consumption rate for the Central Counter Barrier and B1 Barrier, and a long work period}
	\label{fig:c1-power-work-1000}
\end{figure}

If one increases the length of the work period beyond 1000 cycles, both algorithms issue few serial write operations followed by busy-waiting. The timing is then similar to our 1000 cycle case. The power consumption converges to the same value for both protocols.

%%%%%
\subparagraph{Distributed Memory Barriers}
\label{sssssec:analysis-modelchecking-quantitative-properties-evaluation-distributed}
This section presents a comparison of the Dissemination Barrier and the B2 Barrier, similarly organised as the previous section.
Additionally, we analyse the Dissemination Barrier with up to eight concurrent processes.
If we attempt the same for the B2 Barrier, the model size exceeds the manageable.
Furthermore, we quantify the number of remote operations the B2 Barrier issues.

We begin with a work period of 100 cycles, simulating that processes arrive in extremely small intervals.
The stacked diagrams in Figure~\ref{fig:d2-work-100-partition} show the total time spent and energy consumed for both protocols and varying thread counts through the different phases.
\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.54\linewidth}
		\includegraphics[height=4.3cm]{charts/d2-time-work-100-partition}
	\end{minipage}
	\begin{minipage}{0.45\linewidth}
		\includegraphics[height=4.3cm]{charts/d2-energy-work-100-partition}
	\end{minipage}
	\caption{Average execution time and energy consumption for the Dissemination Barrier and B2 Barrier, and a short work period}
	\label{fig:d2-work-100-partition}
\end{figure}
Both protocols complete similarly fast. The B2 Barrier has an advantage at over two processes.
The Dissemination Barrier finishes 23 cycles earlier for two processes, because remote writes can be issued during another process's work period, whereas the B2 Barrier needs to execute two more remote read operations after the last process enters the barrier.
For the B2 Barrier after the last process arrives, the first one leaves in 50 cycles for two processes. Since a remote read takes 100 cycles on average, this is unrealistically early. The CTMC interleaving behaviour, explained in the previous section, causes this effect.
Furthermore, one can observe that there is a wider spread between the first and the last process to leave for the B2 Barrier.
For the Dissemination Barrier at two processes the first and the last process leave at nearly the same time, because when the first realises its peer has completed its remote write, it already completed its own. Both remote writes finish before either process can leave and therefore phase D is one cycle short.

Energy consumption behaves similarly to execution time through the different phases.

Figure~\ref{fig:d-work-100-partition} presents execution time and energy consumption of the Dissemination Barrier through its rounds for up to eight processes.
\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.43\linewidth}
		\includegraphics[height=4.2cm]{charts/d-time-work-100-partition}
	\end{minipage}
	\begin{minipage}{0.56\linewidth}
		\includegraphics[height=4.2cm]{charts/d-energy-work-100-partition}
	\end{minipage}
	\caption{Average execution time and energy consumption for the Dissemination Barrier, and a short work period}
	\label{fig:d-work-100-partition}
\end{figure}
As suspected in Section~\ref{sssec:analysis-general-distributed} it shows a stair-shaped execution time distribution. With each new round the duration increases by a large step, whereas adding processes and maintaining the number of rounds increases runtime very little.
Normally, because remote writes can be issued concurrently, one expects the runtime to not increase at all. But due to the CTMC interleaving semantic an increase in concurrently issued operations increases the average time it takes to complete all of them.
Here one can interpret this effect as resource contention, i.e. issuing multiple messages concurrently increases the time to transfer all of them. We expect this behaviour to be visible in runtime benchmarks as well.

Energy consumption with increasing process count and constant number of rounds increases faster than the execution time. There are two reasons for this. First, more remote operations are issued if processes are added, as described in Section~\ref{sssec:analysis-general-distributed}. Second, having more processes means more processes are waiting busily and therefore increase energy consumption.

The B2 Barrier's rate of energy consumption is similar to the Dissemination Barrier's as can be observed in Figure~\ref{fig:d2-power-work-100}.
That is, because the execution time and the number of issued remote operations is similar.
The Dissemination Barrier uses between 17.4 and 25.2 watts, whereas the B2 Barrier requires between 16.1 and 22.2 watts.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=10cm]{charts/d2-power-work-100}
	\caption{Average energy consumption rate for the Dissemination Barrier and B2 Barrier, and a short work period}
	\label{fig:d2-power-work-100}
\end{figure}
Only phase D differs, but not significantly.
This is due to the fact that all processes in the Dissemination Barrier poll actively on local memory, where in the B2 Barrier they poll remotely and therefore more slowly.

Switching to a 1000 cycle long work period, thus emulating that processes arrive in larger intervals, Figure~\ref{fig:d2-work-1000-partition} illustrates the distribution of execution time and energy consumption for both barrier protocols.
As in the previous section we omit phase A and B, because they are long and behave equally in both protocols.
The measurement is similar to the 100 cycle case. Here, too, the first process to leave the B2 Barrier does so unrealistically early.
\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.54\linewidth}
		\includegraphics[height=4.3cm]{charts/d2-time-work-1000-partition}
	\end{minipage}
	\begin{minipage}{0.45\linewidth}
		\includegraphics[height=4.3cm]{charts/d2-energy-work-1000-partition}
	\end{minipage}
	\caption{Average execution time and energy consumption for the Dissemination Barrier and B2 Barrier, and a long work period}
	\label{fig:d2-work-1000-partition}
\end{figure}

Figure~\ref{fig:d-work-1000-partition} shows the same measures for the Dissemination Barrier with up to eight processes.
\begin{figure}[htbp]
	\centering
	\begin{minipage}{0.43\linewidth}
		\includegraphics[height=4.2cm]{charts/d-time-work-1000-partition}
	\end{minipage}
	\begin{minipage}{0.56\linewidth}
		\includegraphics[height=4.2cm]{charts/d-energy-work-1000-partition}
	\end{minipage}
	\caption{Average execution time and energy consumption for the Dissemination Barrier, and a long work period}
	\label{fig:d-work-1000-partition}
\end{figure}
Comparing to Figure~\ref{fig:d-work-100-partition} there is almost no additional execution time per process involved, if the number of rounds does not increase.
Remote writes inside one round are approximately serial. Therefore, a write is oftentimes enabled at the same time as other processes repeatedly execute local reads. The expected time such a write takes, according to CTMC semantics explained in Section~\ref{sssec:analysis-modelchecking-preliminaries}, is then 100 processor cycles.
The duration of each round is related to the explanation on progress in Section~\ref{sssec:analysis-general-distributed}. When the last process arrives at the barrier, exactly one remote write needs to be executed to finish round one, two more are needed to finish round two, and four writes are required to complete the third round. Therefore, and because of CTMC interleaving semantics, approximately 100, 150 and 183 cycles are needed to finish rounds one, two and three.

Energy consumption behaves similarly to that illustrated in Figure~\ref{fig:d-work-100-partition} where we assumed a work period of 100 cycles.

As we observed, there is a certain cost for each additional round in the Dissemination Barrier. Due to model checking limitations we are unable to compare the Dissemination Barrier to the B2 Barrier for more than four processes, i.e. two rounds. In Figure~\ref{fig:d2-work-1000-partition} one can see that the B2 Barrier's execution time is spread more evenly than that of the Dissemination Barrier. We expect this advantage to become greater with increasing process count.
Comparing to the 100 cycle work period results, the B2 Barrier does not increase its advantage. The B2 Barrier in phase C and D uses 84.6 and 53.5 fewer cycles for three and four processes than the Dissemination Barrier, whereas for the larger work period it uses 69.5 and 43.4 fewer cycles. This is surprising, and there are a number of possible explanations. First, the B2 Barrier might handle the case where processes arrive in large intervals less efficient than expected. Second, the fact that the Dissemination Barrier exchanges relatively few messages might be more important than assumed. This means there is no clear indication that the progress problem, described in Section~\ref{sssec:analysis-general-distributed} exists. It might surface, if one is able to analyse beyond four processes, though.

Figure~\ref{fig:d2-power-work-1000} displays the rate of energy consumption for both protocols. It is similar to the 100 cycle work period case, except that it uses overall less power, because a longer work period means that more time is spent idly.
\begin{figure}[htbp]
	\centering
	\includegraphics[width=10cm]{charts/d2-power-work-1000}
	\caption{Average energy consumption rate for the Dissemination Barrier and B2 Barrier, and a long work period}
	\label{fig:d2-power-work-1000}
\end{figure}

Aside from execution time and energy consumption, the number of remote transfer operations is another interesting measure to evaluate.
As explained in Section~\ref{sssec:analysis-general-distributed}, this number is fixed for the Dissemination Barrier -- $n \cdot \lceil \log _2~n \rceil$ where $n$ is the process count.
The number of successful, i.e. non-zero, remote read operations for the B2 Barrier lies between $2 \cdot (n-1)$ and $n \cdot (n-1)$. We are unable to give a better estimate. This is where model checking comes into play.

Table~\ref{tab:d2-transfers-100} presents the number of remote operations, assuming a work period duration of 100 cycles.
\begin{table}[htbp]
	\centering
	\caption{Average number of remote transfers for the Dissemination Barrier and B2 Barrier, and a short work period}
	\vspace{0.2cm}
	\begin{tabular}{r | r | r r}
					  & Threads & Successful transfers & Failed transfers \\
		\hline
		Dissemination & 2       & 2                     &                 \\
					  & 3       & 6                     &                 \\
					  & 4       & 8                     &                 \\
		\hline
		B2 Barrier    & 2       & 2                     & 0.99            \\
					  & 3       & 4.74                  & 1.73            \\
					  & 4       & 7.98                  & 2.44            \\
	\end{tabular}
	\label{tab:d2-transfers-100}
\end{table}
The B2 Barrier issues more remote transactions than the Dissemination Barrier, but comparatively few.
For the Dissemination Barrier the number of remote operations depends heavily on the number of rounds, therefore the situation might get better for the B2 Barrier with process counts higher than four.

Table~\ref{tab:d2-transfers-1000} shows the same measurement for an assumed work period length of 1000 cycles.
\begin{table}[htbp]
	\centering
	\caption{Average number of remote transfers for the B2 Barrier, and a long work period}
	\vspace{0.2cm}
	\begin{tabular}{r | r r}
		Threads & Successful transfers & Failed transfers \\
		\hline
		2       & 2                     & 9.9             \\
		3       & 4.95                  & 22.89           \\
		4       & 8.78                  & 38.05           \\
	\end{tabular}
	\label{tab:d2-transfers-1000}
\end{table}
The number of transfers for the Dissemination Barrier is the same as before.
If processes arrive in large intervals, each time a new one arrives it gets concurrently polled by all others. The resulting communication pattern is more inefficient than if processes would share their presence in a chain- or tree-like fashion. Therefore, the B2 Barrier issues slightly more successful remote reads in this case.
The number of failed transfers is, as expected, considerably higher.
Through implementing a back-off strategy this number can be lowered in exchange for a higher execution time.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discussion}
\label{ssec:analysis-discussion}
Each analysed protocol is functionally correct in the sense that they indeed perform a barrier synchronisation and do not deadlock.
Each barrier is reasonable fast and consumes an expected amount of energy.

Despite relying on atomic operations the Central Counter Barrier performs better than our proposed protocol, the B1 Barrier.
The approach to trade less time writing for more time reading does not pay off for two reasons.
First, in a balanced scenario the time lost due to queuing atomic operations is small. Second, atomic operations blocking access to a variable results in less busy waiting, which in turn lowers energy consumption.

In the distributed memory case, the B2 Barrier outperforms the Dissemination Barrier by most measures.
As conjectured in Section~\ref{sssec:analysis-general-distributed} the Dissemination Barrier wastes substantial time due to its fixed, round-based communication pattern. For non-power of two process counts it even gets worse.
The main drawback of the B2 Barrier is, due to busy waiting on remote memory, it issues more remote operations overall and therefore allocates more bandwidth than the Dissemination Barrier.

The CTMC model checking semantic for simultaneously enabled transitions is different from our intuitive notion of parallel execution. Because of this, and since the assumptions we make do not closely reflect reality, one has to exercise caution when interpreting the model checking results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleartooddpage
\section{Conclusion and Future Work}
\label{sec:conclusion}
In this work we first enumerate the available low-level tools for building barriers, survey today's implementation landscape, and show how barriers are constructed from orthogonal components.
After presenting three new barrier protocols, we compare two of them to two known protocols.
The B2 Barrier turns out to be a promising algorithm.
We, therefore, conclude that today's barriers can be improved upon. To achieve the displayed performance, the B2 Barrier does not use a fixed communication pattern but relies on randomness. Weakening determinism and using randomness are core principles of the pW/CS lock. Thus, we consider the B2 Barrier an example for the aptitude of these principles to increase synchronisation performance.
Model checking enables exhaustive and fine-grained analysis beyond what measurement-based methods can deliver.
Although model size and accuracy is admittedly limited, we expect future work in this direction to alleviate these deficiencies.
We belief that model checking is a powerful tool to aid the development and analysis of algorithms, and synchronisation protocols in particular.

The possible future directions of this work can be divided into six categories.
First, this thesis lacks benchmarks.
One should repeat the analysis of Section~\ref{sssec:analysis-modelchecking-functional-properties} using measurement. One then sees how the protocols perform in reality, and can also use the obtained results to refine the existing probabilistic models.

Second, new barriers and variations of our presented ones should be evaluated.
For example one can try different increment strategies for the iteration variable of the B2 Barrier.
Both the B1 and the B2 Barrier exclusively read remotely and write locally. In particular, they repeatedly poll remote memory.
By switching to remote writing in conjunction with local reading, one might be able to get rid of busy waiting on remote memory and leverage the better performance of remote write operations on some architectures, like the Epiphany~\cite{epiphany} multi-core processor.

Third, model checking should be extended. The unfortunate limit of four processes for our analysis ought to be improved. To achieve this, one can employ techniques like symmetry reduction, partial order reduction and manual fine-tuning.
Another interesting pursuit is to model the protocols in greater detail. Today's processors, for example, have a multi-layered cache. In order to get preciser performance predictions one can incorporate such intricacies into the model.
Resource contention is another detail we do not model in this work. Our theoretical computer can concurrently transfer data between processes with arbitrarily high bandwidth.

The fourth area of future research is to expand the focus of analysis. We are interested in analysing The Dissemination Barrier and the B2 Barrier for shared memory architectures, and incorporate back-off strategies into the evaluation.

Fifth, we encourage to search for alternative modelling formalisms or to modify/limit the existing CTMC/CSRL semantic. The way interleaving is treated does make sense, if a formula queries information about one module, for example if we ask for the arrival time of a specific thread. But, when modelling multi-threading through multiple modules, if one checks quantitative properties about multiple modules, the behaviour is unintuitive.

The sixth suggestion is to improve tool support. Model checking complements testing and benchmarking algorithms. But, tools like SPIN and PRISM are both very extensive and difficult to use. They have esoteric input languages, hard-to-use user interfaces, and require the user to have background knowledge about model checking and temporal logics. A specialised tool set to aid the design of parallel algorithms is much desired.

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \appendix
% 
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \cleartooddpage
% \section{Appendix}
% \label{sec:appendix}
% \subsection{Energy reference measurement for estimating the model parameters}
% The CPU we used is an Intel\textregistered~Core\texttrademark~i5-2520M clocked at 2.50GHz.
% Barriers are repeated as quickly as possible, without waiting time in between.
% \label{ssec:energy-measurement}
% \begin{table}[htbp]
% 	\centering
% 	\caption{Reference measurement (without Hyper-Threading)}
% 	\vspace{0.2cm}
% 	\begin{tabular}{r | r | r r r}
% 		                         & Thread & Time     & Energy    & Rate of      \\
% 		                         & count  & (Cycles) & consumpt. & energy cons. \\
% 		                         &        &          & (nJ)      & (W)          \\
% 		\hline
% 		Idle                     &        &          &           &  4.0 \\
% 		Off-core under load      &        &          &           &  6.8 \\
% 		Load                     & 1      &          &           & 19.6 \\
% 		                         & 2      &          &           & 28.4 \\
% 		Busy waiting             & 1      &          &           & 16.4 \\
% 		                         & 2      &          &           & 22.8 \\
% 		\hline
% 		Add\&fetch (contested)   & 2      &    137.6 &     893.7 & 16.2 \\
% 		Add\&fetch (uncontested) & 2      &     19.3 &     158.5 & 20.5 \\
% 		Central Counter Barrier  & 2      &    227.4 &    1667.0 & 18.3 \\
% 		B1 Barrier               & 2      &    114.0 &    1076.1 & 23.6 \\
% 		Dissemination Barrier    & 2      &    144.0 &    1400.3 & 24.3 \\
% 		B2 Barrier               & 2      &     42.1 &     433.4 & 25.7 \\
% 	\end{tabular}
% \end{table}
% \begin{table}[htbp]
% 	\centering
% 	\caption{Reference measurement (with Hyper-Threading)}
% 	\vspace{0.2cm}
% 	\begin{tabular}{r | r | r r r}
% 		                          & Thread & Time     & Energy    & Rate of      \\
% 		                          & count  & (Cycles) & consumpt. & energy cons. \\
% 		                          &        &          & (nJ)      & (W)          \\
% 		\hline
% 		Idle                      &        &          &           &  4.0 \\
% 		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
% 		Off-core under load       &        &          &           & 12.6 \\
% 		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
% 		Load                      & 1      &          &           & 19.6 \\
% 		                          & 2      &          &           & 19.7 \\
% 		                          & 3      &          &           & 28.0 \\
% 		                          & 4      &          &           & 28.3 \\
% 		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
% 		Busy waiting              & 1      &          &           & 17.2 \\
% 		                          & 2      &          &           & 17.5 \\
% 		                          & 3      &          &           & 22.9 \\
% 		                          & 4      &          &           & 22.9 \\
% 		\hline
% 		Add\&fetch (contested)    & 2      &    122.2 &     700.2 & 14.3 \\
% 		                          & 3      &    136.6 &     903.3 & 16.5 \\
% 		                          & 4      &    354.6 &    2349.9 & 16.7 \\
% 		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
% 		Add\&fetch (unconstested) & 2      &     45.2 &     283.8 & 15.7 \\
% 		                          & 3      &     46.9 &     389.0 & 20.7 \\
% 		                          & 4      &     47.5 &     396.6 & 20.9 \\
% 		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
% 		Central Counter Barrier   & 2      &     86.5 &     647.1 & 18.7 \\
% 		                          & 3      &    314.0 &    2613.6 & 20.8 \\
% 		                          & 3      &    442.8 &    3908.3 & 22.0 \\
% 		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
% 		B1 Barrier                & 2      &     45.4 &     359.2 & 19.8 \\
% 		                          & 3      &    157.8 &    1614.5 & 25.6 \\
% 		                          & 4      &    198.3 &    2174.1 & 27.4 \\
% 		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
% 		Dissemination Barrier     & 2      &     94.4 &     785.3 & 20.8 \\
% 		                          & 3      &    297.4 &    3189.0 & 26.8 \\
% 		                          & 4      &    337.1 &    3960.3 & 29.4 \\
% 		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
% 		B2 Barrier                & 2      &     34.3 &     285.9 & 20.8 \\
% 		                          & 3      &    109.1 &    1205.6 & 27.6 \\
% 		                          & 4      &    112.1 &    1297.0 & 28.9 \\
% 	\end{tabular}
% \end{table}
% 
% \clearpage
% \subsection{Raw data of behind figures}
% \begin{itemize}
% 	\item \todo
% \end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleartooddpage
\section*{Bibliography}
\addcontentsline{toc}{section}{\protect Bibliography}
\label{sec:bibliography}
\renewcommand\refname{\vskip -1cm}
\nocite{*} % insert not cited references
\bibliographystyle{abbrv}
\bibliography{bibliography}{}

\end{document}
