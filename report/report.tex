\documentclass[a4paper, 10pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{cite}
\usepackage{color}
	\definecolor{gray}{rgb}{0.5,0.5,0.5}
\usepackage{colortbl}
\usepackage{datetime}
\usepackage{fancyvrb}
\usepackage{float}
\usepackage{graphicx}
	\graphicspath{{images/}}
	\DeclareGraphicsExtensions{.eps, .pdf,.png}
\usepackage{epstopdf}
	\DeclareGraphicsRule{.eps}{pdf}{.pdf}{`epstopdf #1}
	\pdfcompresslevel=9
\usepackage{latexsym}
\usepackage[latin1]{inputenc}
\usepackage{listings}
	\lstset{
		frame=single,
		%numbers=left,
		numberstyle=\small\color{gray},
		tabsize=4,
		morekeywords={and, boolean, do, else, false, finished, for, if, initialized, integer, is, mod, reset, true, until, use, wait, while},
	}
\usepackage{multirow}
\usepackage{relsize} % used for larger sum symbols
\usepackage{setspace}
%\usepackage{silence}% Filter out unwanted warnings and error messages
%\WarningFilter{pdftex}{destination with the same}
\usepackage{tikz}
	\usetikzlibrary{shapes, arrows, positioning}
	\tikzstyle{-}    = [draw]
	\tikzstyle{->}   = [draw, -latex']
	\tikzstyle{<-}   = [draw, latex'-]
	\tikzstyle{<->}  = [draw, latex'-latex']
	\tikzstyle{o}    = [draw, circle]
	\tikzstyle{box}  = [draw, rectangle]
\usepackage{url}

\usepackage{hyperref} % should be last usepackage to avoid errors.

\def \todo{\textbf{\textcolor{yellow}{TODO}}}
\def \citationneeded{\textbf{\textcolor{yellow}{CITATION NEEDED}}}
\newcommand{\listingrule}[1]{\rule{#1}{0.4pt}}

%\setcounter{secnumdepth}{5}

\title{Development and Analysis of Barrier Protocols}
\author{Ronny Brendel\\Tutors: Sascha Kl\"uppelholz \& Marcus V\"olp}

\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
\pagenumbering{alph}

\begin{center}
\includegraphics[width=3cm]{tu-logo}~\\[1cm]
\textsc{\LARGE Dresden University of Technology}\\[0.5cm]
\textsc{\Large Faculty of Computer Science}\\[0.2cm]
\textsc{\large Institute of Theoretical Computer Science}\\[0.2cm]
\textsc{\large Chair for Algebraic and Logical Foundations of Computer Science}\\[3cm]
\Huge Study's Thesis \\[1cm]
\huge Development and Analysis of Barrier Protocols\\[3cm]
\end{center}

\begin{flushleft} \large
	Author: Ronny Brendel \\
	Responsible university professor: Christel Baier \\
	Supervisors: Sascha Kl\"uppelholz \& Marcus V\"olp
\end{flushleft}

\vfill
\begin{flushright}
	\large October 2013
\end{flushright}

\end{titlepage}

\pagebreak
\newpage \thispagestyle{empty} \mbox{}
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}

%\section*{Aufgabenstellung}
%\label{sec:task}
%\begin{enumerate}
%	\item Literaturrecherche und ausf\"uhrlicher \"Uberblick aktuell genutzter Barrierenprotokolle sowie grunds\"atzlicher Implementierungsm\"oglichkeiten f\"ur Barrieren.
%	\item Untersuchung und Weiterentwicklung der von Nicolas Mc~Guire vorgeschlagenen Barriere.
%	\item Identifikation und Formalisierung der zentralen funktionalen Eigenschaften im Bezug auf Korrektheit sowie der quantitativen Eigenschaften zur Ermittlung der Performance von Barrierenprotokollen bez\"uglich Energieverbrauch und Geschwindigkeit.
%		\item Modellierung und Quantitative Analyse:
%			\begin{itemize}
%				\item Modellierung a) eines prominenten Vertreters von Barrierenprotokollen mit verteiltem Speicher, b) eines prominenten Vertreters von Barrierenprotokollen mit geteilten Speicher, c) des auf Mc~Guire beruhenden Barriereprotokolls jeweils im probabilistischem Model Checker PRISM.
%				\item Analyse und Vergleich der drei oben genannten Modelle im Bezug auf die in 3. identifizierten funktionalen sowie quantitativen Eigenschaften in PRISM. Wenn m\"oglich: Erg\"anzung des Performance-Vergleichs der drei Barrieren mittels messbasierter Methoden.
%			\end{itemize}
%		\item Zusammenfassung und Ausblick: Diskussion der in 4. gewonnenen Erkenntnisse
%\end{enumerate}
%
\section*{Task}
\label{sec:task}
\begin{enumerate}
	\item Literature research and detailed survey of currently used barrier protocols as well as implementation possibilities for barriers.
	\item Analysis and improvement of Nicolas Mc~Guire's proposed barrier.
	\item Identification and formalisation of key functional properties concerning correctness as well as quantitative aspects for determining performance of barrier protocols with regard to energy consumption and speed.
		\item Modelling and quantitative analysis:
			\begin{itemize}
				\item Modelling a) a prominent representative of barrier protocols for distributed memory, b) a prominent representative of barrier protocols for shared memory, c) the barrier protocol based on Mc~Guire's idea in each case using the probabilistic model checker PRISM.
				\item Analysis and comparison of the three above-mentioned models, regarding the functional as well as quantitative properties identified in 3., using PRISM. If possible: complement the performance evaluation of the three barriers using measurement-based methods.
			\end{itemize}
		\item Summary and outlook: Discussion of the insights gained in 4.
\end{enumerate}

\pagebreak
\newpage \thispagestyle{empty} \mbox{}
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}

%\section*{Selbstst\"andigkeitserkl\"arung}
%\label{sec:integrity}
%Ich erkl\"are hiermit, dass ich die vorliegende Arbeit selbst\"andig und ohne Benutzung anderer als der angegebenen Hilfsmittel angefertigt habe. Die aus fremden Quellen w\"ortlich oder sinngem\"a\ss ~\"ubernommenen Gedanken sind als solche kenntlich gemacht. Ich erkl\"are ferner, dass ich die vorliegende Arbeit an keiner anderen Stelle als Pr\"ufungsarbeit eingereicht habe oder einreichen werde.

\section*{Statement of academic integrity}
\label{sec:integrity}
I hereby declare that I prepared this thesis independently and without use of tools other than specified. Foreign thoughts, taken literally or in spirit, are marked as such. I also declare that I have not filed the present work at any other location or will submit it.

\pagebreak
\newpage \thispagestyle{empty} \mbox{}
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\thispagestyle{empty}

\renewcommand{\contentsname}{Table of contents}
\tableofcontents

\pagebreak
\newpage \thispagestyle{empty} \mbox{}
\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagenumbering{arabic}
\section{Don't forget}
\begin{itemize}
	\item maybe add more $\sim$ into proper nouns to forbid separation
	\item present tense! Consistency please! Exception where it makes sense.
	\item maybe increase font size in charts
	\item "don't overuse passive voice"
	\item be very aware where you use thread and process, and make sure the reader understands when the distinction does not matter
	\item make clear what is meant in the listings
		\begin{itemize}
			\item C-esque pseudocode
			\item threadCount/processCount,  threadIndex/processIndex
			\item the code in the figures is executed on each thread. Each thread has an identifier between 0 and threadCount-1
			\item \& bit-wise \texttt{AND}, $|$ bit-wise \texttt{OR}, $\sim$ bit-wise \texttt{NOT}
			\item \texttt{x[$*$] := y} is short for assigning y to each element in x
			\item ?more?
		\end{itemize}
	\item mention that we assume atomic read and write at 32/64 bits a piece, when it matters
	\item explain cores vs processes, threads - we always assume threads, processes $>$= cores. Why. Find an appropriate spot to talk about this. I have marked a good spot in the text in Section~\ref{sssec:background-currently-used-shared}
	\item "be forthright about the limitations and assumptions of your design. Also, make sure you justify any shortcuts/limitations convincingly"
	\item \texttt{me} vs \texttt{threadIndex} / \texttt{processIndex}, make its use coherent
	\item color use in diagrams should be consistent (same bars should have same color across multiple diagrams)
	\item put raw data of figures into the appendix
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:introduction}
\begin{itemize}
	\item \todo watch out that the intention/laying down a continuous thread is strong enough ((1)barriers can be improved, (2)through pW/CS, (3)model checking is useful)
	\item what is a barrier
	\item usage example for barriers
	\item barriers are common in parallel programming. OpenMP's\cite{openmp} implicit barriers. MPI barriers (Rolf Rabenseifner\cite{rab00}). Relate to normal send/recv, because absolute numbers without relation are not helpful.
		\begin{itemize}
			\item HLRS, year 2000
			\item many calls,
			\item 5.3\% of all time spent inside MPI calls, 0.7\% of all CPU time, estimated 1.6\% of the barrier execution time is actual synchronization, 100\% minus 1.6\% is waiting for other processes to arrive, because of unbalanced computation. Average 1852 microsecond per call. (Figure~13 in paper)
			\item e.g. receive 12.9\% of all time spent inside MPI calls, 1.8\% of all cpu time, 33\% of the recv executation time is actual latency plus transfer time, 100\% minus 33\% is waiting for other processes to arrive, because of unbalanced comoputation. Average 498 microseconds per call.
		\end{itemize}
	\item motivation for researching barriers: Apply ideas of pW/CS\cite{pwcs} to other synchronization primitives, e.g. barriers in order to gain performance, where performance can be any important measure e.g. speed, energy efficiency, (say something about possible improvements that can still be made).
	\item summarize pW/CS
		\begin{itemize}
			\item usually concurrent programs work in a deterministic fashion. That is the order and partners of remote synchronization (locally, programs are obviously mostly deterministic) are predetermined. The algorithm is executed in a very controlled fashion. Same holds for synchronization primitives themselves. Faulty behavior is avoided at great cost.
			\item don't avoid inconsistency at all cost. Make errors detectable, or ignorable.
			\item use the inherent randomness\cite{mcg09}, induced by caching, scheduling, randomness (ECC, flash access e.g.) at hardware level (jitter), or short: complexity of today's computer systems, in order to view concurrent accesses as really random. Thus we are able to apply probabilistic algorithms and analyse these algorithms using tools of probability theory.
			\item design algorithms that have sufficiently high probability of success and tolerate faulty behaviour.
			\item build them so that on average, or in their intended use case, they perform better than their deterministic counterparts, where better might e.g. be faster, more energy efficient, resource preserving.
			\item It seems to be a promising approach
			\item barriers can be improved. One possible way is through pW/CS.
		\end{itemize}
	\item how do we gather data on these algorithms to build confidence that they work properly, i.e. satisfy certain properties? And get a feeling for how they perform?
	\item Testing/Measurement (explain wording. Be aware of possible confusion) and model checking complement one another:
		\begin{itemize}
			\item Measurement is usually deterministic. If so, certain interesting states in a protocol are impossible to reach because of the randomness inherent to today's computers (add a short example? A special very case of a low level lock being contented?)
			\item Assuming you have randomized measurements, the probability to reach certain states in the protocol is so low that they are not revealed.
			\item Some parts of algorithms are performed in such a short time or need very exact preparation that it is practically impossible to analyse these parts using measurement. In this case model checking can be applied because it offers this granularity.
			\item Modelling has of course its own short comings. For example the state explosion problem.
			\item we can gain better quantitative insights into synchronization protocols by combining both measurement and model checking.
		\end{itemize}
	\item briefly explain the report structure
		\begin{itemize}
			\item existing stuff
				\begin{itemize}
					\item means to implement barriers
					\item currently used barriers
					\item idea to split barriers into orthogonal parts and think about them separately
				\end{itemize}
			\item new barriers
			\item analysis and comparison of one old for shared memory and one old for distributed memory against the new barriers
			\item conclusion \& future work
		\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}
\label{sec:background}
In this section we will first take look at the tools at our disposal to implement barriers on both shared memory and distributed memory architectures, followed by an overview of which barrier algorithms are used in today's parallel programming frameworks.
Finally we present a view of the barrier as a modularized algorithm consisting of various independent pieces.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Means to implement barrier protocols}
\label{ssec:background-means}
A shared memory system consists of a number of processors which are connected to one chunk of memory. Information can be exchanged between processors by simply reading from and writing to the same memory location, since they all share the same address space.
Desktops, laptops and small to medium servers are typical examples of shared memory systems.

Many shared memory systems (called nodes) can be connected via a network to compose a distributed memory system. Such systems do not have one global memory, instead each of the nodes has its own private memory. Computation is done on each node using only local memory. In order to share information between nodes messages via network need to be exchanged.
For example large servers and high-performance computers are oftentimes distributed memory systems.

Programming for shared memory and distributed memory architectures is very different. Therefore we will subsequently differentiate between these two worlds.

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Shared memory systems}
\label{sssec:background-means-shared}
A parallel program for a shared memory system consists of multiple threads of execution -- short \emph{threads}.

The straightforward way to share information between threads is to \emph{load} from and \emph{store} to the same memory address.

Furthermore most modern processors provide means to execute a small series of computations on a date while making the memory unavailable to other threads. They are called \emph{atomic operations}. Two examples are the \emph{test-and-set} and \emph{add-fetch} instructions. The former stores a value to an address if a given condition is fulfilled, the latter loads a date, adds a given value to it and stores it back.

Without these operations parallel programming is much more difficult and error-prone, since concurrently issuing normal load and store operations can easily cause \emph{race conditions}. This convenience, on the other hand, comes at the cost of additional management overhead for the processor. To date shared memory systems are usually limited to 64 threads and less. Some high-performance computing implementations reach numbers of 512 threads. In comparison distributed memory systems can have over a million processing units.

The intuitive ordering of concurrent memory accesses is \emph{sequentially consistent}.
\begin{quote}
	\textit{The result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program.} (Lamport~\cite{sequentialconsistency})
\end{quote}

Enforcing this intuitive semantic comes at a cost. Modern processors buffer memory operations on many levels. To enforce sequential consistency mechanisms to control and circumvent this buffering have to be introduced resulting in more transistors in the processor and higher memory access latency.

If an algorithm is carefully designed, one can weaken the consistency model to gain better performance. One prominent way of such a model is \textit{release consistency}~\cite{gha90}. Memory access appears unordered except for so called \emph{acquire} and \emph{release} operations. No memory access that has been issued after an acquire operation is permitted to finish before this acquire operation. No memory access is allowed to finish later than the next release operation. Acquire and release encapsulate the memory accesses in between.

Aside from implementing barriers in software vendors can wire a barrier implementation into the system's hardware. This is almost never done on shared memory architectures. One example for hardware barrier support is the SGI~UV~2000~\cite{sgiuv2000}, a high-performance shared memory computer.

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Distributed memory systems}
\label{sssec:background-means-distributed}
A parallel program for a distributed memory architecture consists of multiple \emph{processes}. Each process has its own address space and is therefore unable to directly share memory with other processes.

First we will give an overview of general ways to implement synchronization primitives on distributed memory systems followed by an analysis of one specific distributed memory programming standard.

One way to exchange information between two processes is via \emph{synchronous message passing}. Two processes meet at a point in time, exchange information, then part and continue their computational tasks. This method requires the involved peers to wait for each other, buffer and queue information and send it via network.

Another way to approach this is to wait as few as possible, since there is, for example, no need for a sending entity to wait until a receiver arrives. The sender might as well give his information to the communication framework and continue its computational work while the framework delivers the message as soon as the receiver is ready. This method is called \emph{asynchronous message passing}. Aside from less waiting time asynchronous message passing incurs similar overhead to synchronous message passing, because on the communication framework level the same work has to be done. The framework establishes network channels, buffers and queues data, and waits for the receiver to arrive.

A comparatively recent development is the move to \emph{remote memory access} (RMA). Remote memory access makes a portion of main memory available via network so that others can load from it and store to it without actively involving the memory's owning process. It is therefore sometimes called \emph{one-sided communication}. Implementations of RMA oftentimes support atomic operations.
In order for RMA to be available, the network hardware has to support it. For example InfiniBand~\cite{infiniband} has RMA facilities built in.
Due to its one-sided nature RMA incurs less overhead than the two previous techniques, but still buffers data. At very high networking bandwidths buffering data becomes a serious problem. To mitigate this vendors implement so called \emph{zero-copy} techniques.

Hardware support for barriers in distributed memory systems is widespread especially in high-performance computing. Examples include the Earth Simulator~\cite{earthsimulator}, IBM Blue Gene/L~\cite{bluegenel}, IBM Blue Gene/Q~\cite{bluegeneq} and a low-cost FPGA-based system~\cite{hoefler2006b}

Another interesting and seemingly untested approach is to use lossy communication channels for developing synchronization protocols. Hardware is always erroneous. For example network packages conflict, packages drop, packages are corrupted or connections drop all the time. Reliability increasing techniques and protocols such as the Transmission Control Protocol (TCP), that are used to to mitigate these errors, incur a great performance penalty for transmitting short messages. TCP for example creates comparatively huge packages for small amounts of data, and each transmitted package has to be acknowledged by the receiver. Checking for flipped bits in the package adds further latency.
If one would instead craft algorithms so that they don't require 100\% reliable connections, further performance improvements are conceivable.
Two ways one could implement such algorithms are via the User Datagram Protocol (UDP)~\cite{udp} and InfiniBand's unreliable connections~\cite{infiniband}.

We will now investigate the facilities the Message Passing Interface (MPI)~\cite{mpi3}, the most prominent high-performance distributed programming interface, provides to implement barrier protocols. MPI is a standardised, low-level, language-independent and portable interface specification. It is very customizable and supports various modes of operation in order to support many different hardware configurations and allow the user to achieve maximum performance. MPI is widely adopted in high-performance computing and scientific computing\cite{mpiadoptiona, mpiadoptionb, mpiadoptionc} and there exist a variety of implementations in all major programming languages.

In our analysis we will only scratch the surface of what the standard offers, since diving deep involves lots of details, complicated semantics and many of the details don't matter for our purpose.

MPI supports synchronous message passing via \emph{send} and \emph{receive}. Additionally you can issue a send operation that immediately returns if no receiver is already in place waiting for the sender.

Furthermore MPI facilitates asynchronous message passing. One can send and receive messages without blocking and one can test if a message has been successfully transmitted. Synchronous and asynchronous message passing can be mixed. That means one is able to, for example, issue a non-blocking send that is matched by a blocking receive operation.

Since version 3.0 the MPI standard supports \emph{non-blocking collectives}. ``Collectives'' is a collective name for all communication operations where there is a variable number of communicating  peers, like barrier, broadcast, reduce and more.
Non-blocking collectives allow, in the example of the barrier, to not block at the operation but to register at the barrier upon arrival, and subsequently test the barrier for completion while doing computational work meanwhile.
This facility is probably unsuited for building new barriers on top of it, but is nevertheless an interesting recent development.

MPI also supports RMA. Although some MPI implementations, for example Open~MPI~\cite{openmpi}, do not yet support version 3.0 of the specification, we will restrict ourselves to analysing it, because it subsumes the features of version 2.2, it is already a year old and supported by other prominent implementations such as MPICH~3.0~\cite{mpich} and its derivatives MVAPICH2~\cite{mvapich} and Cray MPT~\cite{craympt}.

The definition of RMA according to the standard is rather involved, and we have to go into more detail to highlight important points.

To initiate sharing remote memory each participating process issues a collective call to create a \emph{window} of memory to be shared. A window is associated with the group of processes that created it, and through it each participating process exposes a portion of local memory to the other peers.
Upon creating a window a number of restrictions regarding the use of the window can be established in order to gain better performance. One can for example lower the memory consistency requirements or guarantee to not use certain operations.

Depending on the underlying hardware and MPI implementation there are two different memory models. A window may either have the \emph{separate} or the \emph{unified} model. The separate model states that the memory that is visible to the peers and the memory that is visible to the local process is not the same and might therefore not automatically update between these two copies. That means a change by a peer to one process's memory might not be made visible. This process has to manually update its local copy in order to see the change. In contrast the unified model makes sure that the two copies are eventually updated. Therefore manual refreshing is unnecessary. This differentiation is a direct result of trying to support many different hardware configurations.

Before going into the details of window synchronisation we have to introduce the concept of \emph{epochs}. An epoch is a period between two window synchronization calls. RMA operations like \emph{put}, \emph{get}, \emph{accumulate} may only be used inside an epoch. Epochs are used to accumulate multiple operations to one larger transfer in order to achieve better efficiency.

MPI differentiates between \emph{active} and \emph{passive} target communication. In active target communication all peers are actively involved in window synchronization, whereas in passive target communication only one side's activity is required.

One way to synchronize a window is via the \emph{fence} operation. A fence is a collective operation among all window group members that makes sure all outstanding RMA requests on this window are finished before the operation returns. It implies a full barrier synchronization.

A more fine-grained instance of active target communication is via the operations \emph{start}, \emph{complete}, \emph{post} and \emph{wait}. Before issuing any remote access calls the issuing process first has to advertise his intention through the start operation. Once he is finished he will invoke the complete operation to announce that his transmission is complete. The receiving end similarly issues post and wait to expose and synchronise its memory. It is allowed to access his window's local memory meanwhile. In contrast to a fence operation one can restrict such synchronization to arbitrary processes of the window.

The \emph{lock} and \emph{unlock} operations facilitate one way of passive target communication. Through these two functions one can block all access to a chunk of remote memory for all other window members. Between lock and unlock the process can then issue remote memory access operations without being interfered.

Another method to synchronize memory passively is through \emph{flush}. A flush makes sure all outstanding RMA calls, issued by the caller, are completed before returning. One can choose if the operations have to be completed only at the issuer's side or on the receiver's side as well.

Window synchronization can be optimized through giving the implementation information about which operations are not used during the epoch and what happens before and after this synchronization.

Inside an epoch one may issue remote memory access operations. The two simple ones are \emph{put} and \emph{get}. Memory access through put and get appears unordered inside an epoch.
Furthermore one can use atomic operations like \emph{accumulate}, \emph{fetch-and-op} or \emph{compare-and-swap}, with the intuitive semantic and where \emph{op} is a placeholder for various operations. The default global visibility order of atomic operations is sequentially consistent. When creating a window one can weaken this by, for example, allowing remote write operations to overtake remote read operations. Any combination of overtaking between reads, writes or between equal operations can be allowed.

Remote memory access in MPI is subject to a number of restrictions:
\begin{itemize}
	\item The buffer supplied to a put call should not be written to until the operation is finished.
	\item The buffer supplied to a get call should not be accessed until the operation is finished.
	\item The outcome of concurrent remote memory access to the same location inside an epoch is undefined.
	\item The outcome of concurrent local and remote access to the same location inside an epoch is undefined.
	\item The outcome of a single process issuing multiple put operations to the same location inside an epoch is undefined.
\end{itemize}
Therefore the only way to make sure that data is correctly written or read is through atomic operations or isolating access to the same location with costly window synchronization operations.
Normally a processor commits 32 or 64 bit to main memory atomically, and in the remainder of this thesis we will assume this is the case.
Therefore these restrictions make sense for buffers at sizes above this, because concurrent memory access of interfering operations might overlap and the outcome is then indeed undefined.
For amounts of memory below this threshold we wish to have a semantic that is more specific. For example if two puts of at most 64 bits are concurrently issued to the same location, the outcome is that of the last put committed. Since this is not the case, MPI's restrictive RMA semantic makes it unsuitable to implement barrier algorithms, based on pW/CS's non-determinism ideas, on top of it.

Shared memory barriers usually rely on atomic operations. Most distributed memory barriers use synchronous message passing. These protocols have also been implemented using RMA to improve efficiency, but still behave equivalently.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Currently used barrier protocols}
\label{ssec:background-currently-used}
Having surveyed the tools for implementing synchronization primitives we will now examine the barrier protocols which power today's implementations for both shared and distributed memory systems.

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Shared memory systems}
\label{sssec:background-currently-used-shared}

Many prominent implementations of shared memory concurrent programming facilities are part of closed source commercial products, like for example Intel Composer and Microsoft Visual Studio, and can therefore not be analysed. So we restrict ourselves to examining the GNU~C~Library~\cite{glibc} and GNU~OpenMP~\cite{gomp}.

% gnu-openmp/gcc-4.8.0/libgomp/libgomp.h, gnu-openmp/gcc-4.8.0/libgomp/barrier.c, gnu-openmp/gcc-4.8.0/libgomp/config/linux/, gnu-openmp/gcc-4.8.0/libgomp/config/linux/bar.h, gnu-openmp/gcc-4.8.0/libgomp/config/linux/bar.c, gnu-openmp/gcc-4.8.0/libgomp/config/linux/futex.h (cpu_relax), gnu-openmp/gcc-4.8.0/libgomp/config/linux/wait.h
The GNU~OpenMP implementation, included in the GNU Compiler Collection (GCC) version 4.8.0, implements a \emph{Central Counter Barrier}. Figure~\ref{fig:pseudocode-central-counter} describes the algorithm in pseudocode which is executed on each thread taking part in the barrier synchronisation.
The Central Counter Barrier uses the variable \texttt{barrier} to hold the number of threads that do not yet have arrived at the barrier. Initially it is assigned the number of threads. Upon entering the barrier each thread atomically decrements the variable by one and then waits until it reaches zero. After it reaches zero, all threads eventually leave the barrier.

\begin{figure}[H]
	\centering
	\input{listings/central-counter}
	\caption{Pseudocode of the Central Counter Barrier}
	\label{fig:pseudocode-central-counter}
\end{figure}

The protocol can be separated into two phases (Figure~\ref{fig:diagram-central-counter}). During the first phase each thread decrements the variable, and in the second reads the variable until it reaches the desired value.

\begin{figure}[H]
	\centering
	\input{tikz/diagram-central-counter}
	\caption{Five threads performing the two phases of the Central Counter Barrier}
	\label{fig:diagram-central-counter}
\end{figure}

The actual implementation in GNU~OpenMP is more involved than the pseudocode in Figure~\ref{fig:pseudocode-central-counter} suggests. For example the algorithm shown here is destructive in the sense that it cannot be repeated, because if the variable is zero at the end, repetition would mean that the variable is negative in the next round, which breaks the protocol. To mitigate this the variable is copied, by each thread, upon first reading it and the last thread to enter resets the shared variable to its initial value.

The loop additionally keeps track number of unsuccessful reads. If this number reaches a certain amount, an operating system mechanism, called futex\cite{franke2002}, is employed to suspend the thread until a later time when the other threads hopefully have arrived. Through this back-off mechanism the system avoids unnecessary busy waiting which would negatively impact power consumption. The threshold of this counter is architecture specific and set to an equivalent of about 3 milliseconds of waiting on GNU/Linux. In case the number of threads is larger than the number of physical cores, i.e. the processor is oversubscribed, the counter is set to a much lower value. This way the operating system scheduler can switch between threads more quickly so that threads that need time for computation are not blocked by busy waiting ones.

Throughout this thesis we will assume that the thread and process count is at most as high as the number of physical processor cores in a system, because oversubscription is not useful inside a single program and therefore rarely happens.

% glibc/nptl/sysdeps/pthread/pthread.h, glibc/nptl/pthread_barrier_*, glibc/nptl/sysdeps/unix/sysv/linux/internaltypes.h
The GNU POSIX Threads implementation, called Native POSIX Threads Library (NPTL), is part of the The GNU~C~Library.
The basic barrier algorithm used in it is the same as for GNU~OpenMP. One main differences is that it does not use atomic operations to decrement the variable, but explicitly locks it to guard from interfering access. Second there is no busy waiting, but upon arriving at the barrier the thread will immediately suspend, via a futex, and wait to be woken up by the last thread arriving. The third difference is that to reset the barrier each thread atomically increments the barrier variable by one upon leaving.

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Distributed memory systems}
\label{sssec:background-currently-used-distributed}

In this section we present the barrier algorithms that are used in the two major open source MPI implementations, namely Open~MPI~\cite{openmpi} and MPICH~\cite{mpich}. We restrict our analysis to MPI for the same reasons we only considered MPI in Section~\ref{sssec:background-means-distributed}.

% ompi/mca/coll/tuned/coll_tuned_barrier.c : ompi_coll_tuned_barrier_intra_recursivedoubling, ompi_coll_tuned_barrier_intra_bruck, ompi/mca/coll/tuned/coll_tuned_decision_fixed.c
Open~MPI distinguishes between process counts that are powers of two and which are not. If it is a power of two, a \emph{recursive doubling} barrier is used. A detailed explanation of this protocol can be found in \cite{hoefler2005}. For non-power of two process counts Open~MPI uses the \emph{Dissemination Barrier}, introduced by Hensgen, Finkel and Manber in 1988~\cite{hensgen1988}.

Figure~\ref{fig:pseudocode-dissemination} presents the pseudocode of this algorithm, which is executed on each process individually. A process is identified by its \emph{process index}, which is a number between 0 and $\mathit{processCount} - 1$. For readability reasons we have chosen an RMA-esque representation of the protocol, although it can similarly be implemented using message passing techniques.
In contrast to the Central Counter Barrier, the Dissemination Barrier uses not only one variable but an array of boolean variables per process involved. Each of these arrays is located in the local memory of its owning process and is initially set to false.
The protocol is organised in rounds. Each round the distance (variable \texttt{dist}) to the next two partners, to read from and write to, doubles. Process indices  are always calculated modulo the total number of processes to achieve a wraparound. Figure~\ref{fig:diagram-dissemination} illustrates this communication pattern.

\begin{figure}[htbp]
	\centering
	\input{listings/dissemination}
	\caption{Pseudocode of the Dissemination Barrier}
	\label{fig:pseudocode-dissemination}
\end{figure}

\begin{figure}[htbp]
	\centering
	\input{tikz/diagram-dissemination}
	\caption{The rounds of the Dissemination Barrier}
	\label{fig:diagram-dissemination}
\end{figure}

At first a process notifies the next one that is has arrived at the barrier by setting the array element at index \texttt{me} on the next process to true. Then it waits until the previous one notifies its arrival. When that happens the waiting process enters the next round. It now knows that itself, and the previous process have arrived at the barrier and notifies the process at double the previous distance (2) of its knowledge by setting its array element at the target process to true. Through this communication pattern the number of processes, that each process knows have arrived, doubles with each round. Thus after $\lceil \log_2 \mathit{processCount} \rceil$ rounds every process knows of the arrival of all others and can therefore leave the barrier.

Notifying the next process is done via writing to remote memory, and waiting for notification via busy waiting on local memory. The number of remote access operations per round is the same as the number of processes involved.

The actual implementation in Open~MPI uses MPI's \emph{send-receive} operation instead of our version's remote write and local read. This operation sends a message and waits for another at the same time. The protocol is not destructive because send and receive do not change the state of the protocol. Therefore no resetting mechanism, as used by GNU~OpenMP for the Central Counter Barrier, is needed.
Open~MPI is able to replace this implementation with hardware specific ones, if it detects supported hardware. For example InfiniBand uses a variation of the Dissemination Barrier based on RMA\cite{hoefler2006a}.

% src/mpi/coll/barrier.c : MPIR_Barrier_intra
MPICH version 3.0, which forms the basis of many other prominent MPI implementations like MVAPICH2 and Cray~MPT, implements the same send-receive-based Dissemination Barrier as is used in Open~MPI, but disregards the number of processes.

Aside from distributed memory architectures it is possible to implement these protocols for shared memory systems, as has been demonstrated for the Intel~XEON~Phi~\cite{hoefler2013}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Barrier building blocks}
\label{ssec:background-building-blocks}
During our analysis of general implementation facilities and concrete barrier implementations we observed that one can assemble barriers from orthogonal components.

One independent piece is the employed way of reinitializing the barrier after use. As we have seen for the Central Counter Barrier (Section~\ref{sssec:background-currently-used-shared}) resetting might be achievable through slight modification of the algorithm. This does not work for all protocols, though.

Another possibility is to replace boolean variables, as used in the Dissemination Barrier in Figure~\ref{fig:pseudocode-dissemination}, with a repetition counter. The remote write and local spin-waiting condition are then replaced with atomic increment and a test for the correct number of repetitions. The additional amount of memory needed is linear in the number of processes. The added computational overhead is negligible.

A way of resetting that works in all circumstances is to use three distinct barriers and switch cleverly between them. The proposed procedure, to be executed by each process individually, is illustrated in Figure~\ref{fig:pseudocode-triple-reset}. In the beginning the first two barriers are initialized, whereas the third one is set to the state of a finished barrier. Upon entering the routine one uses the barrier which comes after the currently finished barrier and resets the one which has been finished before. This approach is provably correct, uses triple the amount of memory of a single barrier, and ads minimal computational overhead.

\begin{figure}[htpb]
	\centering
	\input{listings/triple-reset}
	\caption{Pseudocode of the triple barrier reset approach}
	\label{fig:pseudocode-triple-reset}
\end{figure}

In order to lower power consumption limiting the amount of busy waiting is important for barrier implementations. Choosing a back-off strategy can be done independently of the used barrier protocol. In almost all cases one wants to take apart the waiting loop and insert a suspension mechanism.

During the first initialisation of the barrier an implementation can decide how to handle the case where the number of threads or processes exceeds the number of physical processor cores. Therefore implementing a solution to this scenario is orthogonal to the other algorithm components.

Another useful approach is to assemble multiple different barrier protocols to create a new one. For example assume you have a hardware topology of shared memory nodes which are connected via network. It is conceivable to construct a barrier that executes a Central Counter Barrier on each node, and then one process of each node takes part in a Dissemination Barrier. After the Dissemination Barrier is completed the intra-node processes are notified and the assembled barrier completes.
This way one can reap the benefits of the best performing barrier on each level of the topology.
This might also make sense if barrier algorithms vary in performance for different numbers of threads or processes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Three new barriers}
\label{sec:new}
\begin{itemize}
	\item \todo pay attention where to draw the line between this section and the analysis in the next section (perhaps, leave some/most of the analysis for the next section)
\end{itemize}
In this section we first present a new variation on a barrier protocol of ours. The second algorithm reverses the approach of the Central Counter Barrier. Third we demonstrate a pW/CS-influenced modification of the second barrier.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MGB Barrier}
\label{ssec:new-mgb}
The original idea of the following algorithm is part of a private communique between Nicholas Mc~Guire, the pW/CS-inventor, and us. Figure~\ref{fig:pseudocode-mgb} presents the new barrier, which is a variation of the protocol first mentioned in \cite{bre13}. It is intended for shared memory architectures.

\begin{figure}[htbp]
	\centering
	\input{listings/mgb}
	\caption{Pseudocode of the MGB Barrier}
	\label{fig:pseudocode-mgb}
\end{figure}

Before going into detail, we first have to repeat some of the notation used in the pseudocode. It borrows from the C programming language. \&, $|$ and $\sim$ are the bitwise \emph{and}, \emph{or} and \emph{not} operator. The constant \texttt{threadIndex} is a thread identifying number between 0 and $\mathit{threadCount}-1$.

The protocol consists of two symmetric sub-barriers. It uses \texttt{current}, a variable which holds the number of the sub-barrier that is currently used, to switch between them.
Through \texttt{first} and \texttt{second} the barrier keeps track of which threads have arrived.
The variable \texttt{copy} is a local copy of either \texttt{first} or \texttt{second}.
A thread commits itself to \texttt{first} or \texttt{second} by setting its respective bit to 1. In the pseudocode we implement this bit set behaviour through integers and working with powers of two. If all threads committed themselves to one of the sub-barriers, it is a full bit set equal to the constant \texttt{full}.
All threads participating in the barrier synchronization enter the same of the two parts.
Inside a loop participants try to synchronize. When successful the barrier is reset through the two subsequent assignments of \texttt{current} and either \texttt{first} or \texttt{second}.

Initially \texttt{first} and \texttt{second} are zero, and \texttt{current} is one. When a thread arrives at the barrier it will enter the upper if-branch. It then asks which threads have already arrived, adds himself to the acquired bit set, and stores it back to the variable \texttt{first}.
He then repeatedly polls \texttt{first} in order to find out if his commitment attempt has been overwritten, i.e. its bit is not set anymore, by another thread's concurrently issued write operation, and secondly to determine whether the barrier is completed. The upper barrier is completed, if either \texttt{first} equals \texttt{full} or \texttt{current} is 2. The \texttt{first} thread to realise that \texttt{first} has been set to \texttt{full} will exit the loop, set \texttt{current} to 2 and continue his computational work. Subsequently all other threads eventually leave the barrier.
When entered again the same procedure is executed in the lower if-branch using the variable \texttt{second} instead of \texttt{first}. A third repetition utilises the upper branch again.

The presented protocol reflects the ideas of pW/CS in that it loosely organizes communication between threads and is less enforcing in the sense that threads may overwrite each others' commitment attempts.

In comparison to the Central Counter Barrier this approach does not rely on the existence atomic operations.

The number of threads this protocol supports is limited by the number of bits a processor can atomically commit, which is usually 64 bits. To alleviate this problem one can implement an array-based variant of the protocol. Instead of reading \texttt{first} and \texttt{second} one copies the array element by element. Comparing is \texttt{done} element by element as well. Instead of setting the variable one writes back only the one element where the thread's bit resides on. This way writing still requires only one atomic 64-bit commitment.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{B1 Barrier}
\label{ssec:new-b1}

The following barrier protocol reverses the idea of the Central Counter Barrier (Section~\ref{sssec:background-currently-used-shared}) to use a single global counter that all threads atomically access once and read repeatedly. Instead it uses an array of boolean variables, one element for each thread, which is also accessed differently.
The algorithm's pseudocode, executed individually on each thread taking part, is depicted in Figure~\ref{fig:pseudocode-b1}.

\begin{figure}[htbp]
	\centering
	\input{listings/b1}
	\caption{Pseudocode of the B1 Barrier}
	\label{fig:pseudocode-b1}
\end{figure}

Upon arriving at the barrier a thread registers itself by setting its array element to true. He subsequently queries all other threads for arrival. As soon as it finds one that has not yet arrived, it resets the loop counter and begins querying all others anew. Once all threads arrive, eventually each of them will leave, thus completing the barrier synchronization.

Similar to the Central Counter Barrier the protocol can be split into two phases (Figure~\ref{fig:diagram-central-counter}).
The two differences are that, because each thread writes only to its own array element, they don't require atomic access to register themselves at the barrier, and during the reading phase each thread has to read each other thread's variable instead of polling just a single global one. The threads may concurrently write, but have to spend more time reading.

The Barrier cannot be repeated as is. A way to add resetting to the protocol is by using the repetition counter approach introduced in Section~\ref{ssec:background-building-blocks}. In order to avoid false~sharing~\cite{falsesharing}, which would majorly degrade performance, an actual implementation of the protocol allocates one cache line for each array element. Cache lines are usually 64 bytes large. Therefore transforming the array into an array of counters does not require more memory.

A small improvement to the shown algorithm is to keep track of the first few threads that have successfully arrived. The loop counter is then reset to this number instead of 0.

Since we don't use any central information, this protocol can be implemented for both shared and distributed memory architectures.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{B2 Barrier}
\label{ssec:new-b2}
\begin{itemize}
	\item variation of B1 Barrier where not only will the own presence be stored, but the presence of other processes, we know have arrived, as well.
	\item evaluation has shown that cache latency is so low, that the additional computation this mechanism introduces, makes this algorithm run slower on shared memory systems than the unmodified one. Thus we will now switch to a distributed memory scenario where communication latency is much higher. If we assume RMA for communication we can use the protocol without modification.
	\item Figure~\ref{fig:pseudocode-b2}
		\begin{figure}[htbp]
			\centering
			\input{listings/b2}
			\caption{Pseudocode of the B2 Barrier}
			\label{fig:pseudocode-b2}
		\end{figure}
	\item each array element is located in the memory of the respective process.
	\item Only the own element is written. Only remote reading.
	\item When a process reads remotely he is informed of other processes, the remote peer knows, have arrived.
	\item probabilistic runtime. Depending on circumstances the protocol will finish in more or less steps (or perhaps successful non-zero reads). It is not predetermined what is communicated and with whom.
	\item advantage: no atomic ops. The writing does not have to be atomic, since processes only write to local memory, and no inconsistent data can be read since 64 bit reads and writes are atomic.
	\item more computational overhead (bit shuffling) for less communication.
	\item discuss resetting through triple barrier. Perhaps, briefly hint that this kind of resetting can be added to any barrier (perhaps add a listing how this would look like (the spin model should have it, or one of the pseudocode files)), more on this in the section on the orthogonal design of barriers.
	\item restricted to 64 threads. An extension to arrays of bit masks, similar to the MGB Barrier is possible.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis and comparison}
\label{sec:analysis}
\begin{itemize}
	\item \todo maybe distinguish extreme cases vs more normal cases. And the spectrum in between -- adds structure -- is nice.
	\item In this section we will evaluate one commonly used barrier for each shared memory and distributed memory architectures and pit them against one novel protocol in each category.
	\item Early evaluations have shown the MGB Barrier to perform worse than the Central Counter Barrier, whereas the B1 and B2 Barrier show promise. Therefore we restrict our attention to the latter.
	\item For the shared memory case the Central Counter Barrier (Figure~\ref{fig:pseudocode-central-counter} in Section~\ref{sssec:background-currently-used-shared}) is pitted against B1 Barrier (Figure~\ref{fig:pseudocode-b1} in Section~\ref{ssec:new-b1}).
	\item For distributed memory architectures the Dissemination Barrier (Figure~\ref{fig:pseudocode-dissemination} in Section~\ref{sssec:background-currently-used-distributed}) and the B2 Barrier (Figure~\ref{fig:pseudocode-b2} in Section~\ref{ssec:new-b2}) are compared. (explain choice with a few words? -- ref existing-currently-used?)
	\item In our evaluations we always assume no back-off strategy is used. Each algorithm polls busily on the requested variable until the desired value is read. Threads do not wait sleeping or use callback mechanisms. The topic of back-off strategies is a large and intricate subject in its own right and goes beyond the scope of this thesis.
	\item The analysis is split into two parts: general properties (bla, perceived without measuring, bla), model checking.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{General properties}
\label{ssec:analysis-general}
\begin{itemize}
	\item group properties into measurable and non-measurable (model checking, sharp looking at)
	\item present considerations of interest about the chosen algorithms.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%
\paragraph{Shared memory barriers}
\label{sssec:analysis-general-shared}
\begin{itemize}
	\item don't talk cache specifics, because we have not given an introduction to MSI. Express yourself in simple terms.
	\item ref algorithms
	\item make sure that people understand it is about shared read/write, not local variables.
	\item PHASES: both algorithms are performed in two phases. A writing phase and a reading phase.
	\item In the writing phase of the Central Counter Barrier concurrently issued add-fetch operations will be queued (because atomic ops on a single variable). On the other hand the B1 Barrier allocates one cache line per array element. Thus all writes to shared memory can be executed in parallel. If all threads arrive at the same instant, writing can be sped up to a factor of $\mathit{threadCount}$ in comparison to executing serialized write operations. In more balanced scenarios this effect diminishes.
	\item The reading phase of the Central Counter Barrier reads a single variable repeatedly. The cache line this variable resides on will be invalidated for each core exactly $\mathit{threadCount} - 1$ times (disregarding outside influences) ((, thus each core fetches the variable at most $\mathit{threadCount} - 1$ times)).
	\item The reading phase of the B1 Barrier needs to read each array element once until it can make a decision whether to leave or repeat the loop. Each array element is invalidated exactly once. Thus each array element, from the viewpoint of one thread, has to be fetched from memory at most twice. From then on it is cached and quickly accessible.
	\item the Central Counter Barrier has the obvious advantage in this phase. How big this advantage is depends heavily on the underlying hardware. For example if the cache line prefetcher predicts the access pattern of the reading phase of the B1 Barrier, those reads can be executed in parallel. (Vorsicht. Evtl weglassen)
	\item The Central Counter Barrier is quicker during busy waiting, whereas the B1 Barrier barrier performs well during writing. We attempt to quantify the effects of this behaviour in the following subsections.
	\item MEMORY: the Central Counter barrier needs a logarithmic amount of memory in the number of threads (i.e. 64 bits, for up to $2^{64}$ threads), whereas B1 Barrier needs a linear amount -- $\mathit{threadCount} \cdot 64$ bytes, assuming a cache line is 64 bytes. This looks like a lot, but since shared memory systems rarely exceed 512 cores, a maximum of only about 32KiB of memory would be used. 64 bytes per thread. Furthermore one thread team needs only one barrier and repeat it. There would be no benefit in allocating the memory for more than one.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%
\paragraph{Distributed memory barriers}
\label{sssec:analysis-general-distributed}
\begin{itemize}
	\item ref algorithms
	\item For the Dissemination Barrier the number of remote writes is exactly $n \cdot \lceil \log _2~n \rceil$ -- $n$ processes times $\lceil \log _2~n \rceil$ rounds. Dissemination remotely writes more than the minimum needed in order to gain speed over the straightforward gather + broadcast approach, which is, illustrated in Figure~\ref{fig:diagram-gather-broadcast}, $2 \cdot (n-1)$ writes in $2 \cdot  \lceil \log _2~n \rceil$ rounds.
		\begin{figure}[htbp]
			\centering
			\input{tikz/diagram-gather-broadcast}
			\caption{Communication pattern of a basic gather and broadcast barrier}
			\label{fig:diagram-gather-broadcast}
		\end{figure}

	\item Reading is only done locally.
	\item B2 Barrier needs between $2 \cdot (n-1)$ and $n \cdot (n-1)$ successful (non-zero) remote reads. Writing is only done locally. (perhaps add picture of n*(n-1), n*(n-1) might also be completely wrong)
	\item failed remote gets. Maybe give a rough estimate of the lost bandwidth.
	\item Dissemination needs exactly $\lceil \log _2~n \rceil$ rounds. The B2 Barrier does not have rounds and tries reading from the next process regardless of whether a remote read has been successful (non-zero) where the Dissemination Barrier waits for its designated communication partner.
	\item needing $\lceil \log _2~n \rceil$ rounds is obviously worse than $\log _2~n$. This especially means that from $n$, where $n$ is a power of 2 threads, to $n+1$ number of rounds increases by 1. Therefore, for core counts other that are not a power of two, the performance degrades. Super wasteful shows a more balanced picture with increasing core counts where Dissemination has jumps at power of two core counts. Benchmarks on shared memory have shown this to be true.
	\item MEMORY: The memory requirements of the Dissemination Barrier and the B2 Barrier are the same. Each process uses some local memory to keep track of the other processes. The exact number depends on the implementation. Theoretically $\lceil \log _2~n \rceil$ bits per process for both algorithms suffice. The Dissemination variant shown in Figure~\ref{fig:pseudocode-dissemination} uses a total of $n \cdot n$ bits, but, since each thread communicates only with $\lceil \log _2~n \rceil$ other threads, a modified index calculation would allow it to work with $n \cdot \lceil \log _2 ~n \rceil$ bits. The memory usage is minuscule either way.
	\item the progress problem
		\begin{itemize}
			\item normal Figure~\ref{fig:diagram-dissemination} in Section~\ref{sssec:background-currently-used-distributed}. All processes arrive timely.
			\item more normal that one or more processes are missing.
			\item how much progress can a barrier make while not all processes have arrived?
			\item suppose process 4 has not entered. Modified diagram of the progress through the rounds: Figure~\ref{fig:diagram-dissemination-progress}
				\begin{figure}[htbp]
					\centering
					\input{tikz/diagram-dissemination-progress}
					\caption{Progression through the rounds of the Dissemination Barrier with process four not having entered}
					\label{fig:diagram-dissemination-progress}
				\end{figure}
			\item process 4 does not notify process 5, therefore in the next round process 5 cannot notify process 7 and so on.
			\item number of blocked processes doubles each round.
			\item once process 4 arrives each one of these blocked processes then needs to finish the still to do rounds until the protocol succeeds
			\item who knows of whose arrival Figure~\ref{tab:table-dissemination-progress}
				\begin{table}[htbp]
					\centering
					\caption{Which process knows of which other's arrival}
					\vspace{0.2cm}
					\begin{minipage}{0.42\linewidth}
						\textbf{After round 1:} \\
						$(n-1) + (n-1) = 14$ \\
						\vspace{-0.1cm} \\
						\resizebox{5cm}{!}{
							\begin{tabular}{c | c c c c c c c c}
								  & 0        & 1        & 2        & 3        & 4 & 5        & 6        & 7 \\
								\hline
								0 & $\times$ &          &          &          &         &          &          & $\times$ \\
								1 & $\times$ & $\times$ &          &          &         &          &          &          \\
								2 &          & $\times$ & $\times$ &          &         &          &          &          \\
								3 &          &          & $\times$ & $\times$ &         &          &          &          \\
								4 &          &          &          & $\times$ &         &          &          &          \\
								5 &          &          &          &          &         & $\times$ &          &          \\
								6 &          &          &          &          &         & $\times$ & $\times$ &          \\
								7 &          &          &          &          &         &          & $\times$ & $\times$ \\
							\end{tabular}
						}
					\end{minipage}
					\begin{minipage}{0.42\linewidth}
						\textbf{After round 2:} \\
						$14 + 6 \cdot 2 = 26$ \\
						\vspace{-0.1cm} \\
						\resizebox{5cm}{!}{
							\begin{tabular}{c | c c c c c c c c}
								  & 0        & 1        & 2        & 3        & 4 & 5        & 6        & 7 \\
								\hline
								0 & $\times$ &          &          &          &   & $\times$ & $\times$ & $\times$ \\
								1 & $\times$ & $\times$ &          &          &   &          & $\times$ & $\times$ \\
								2 & $\times$ & $\times$ & $\times$ &          &   &          &          & $\times$ \\
								3 & $\times$ & $\times$ & $\times$ & $\times$ &   &          &          &          \\
								4 &          & $\times$ & $\times$ & $\times$ &   &          &          &          \\
								5 &          &          & $\times$ & $\times$ &   & $\times$ &          &          \\
								6 &          &          &          &          &   & $\times$ & $\times$ &          \\
								7 &          &          &          &          &   &          & $\times$ & $\times$ \\
							\end{tabular}
						}
					\end{minipage}
					\begin{minipage}{0.42\linewidth}
						\vspace{0.3cm}
						\textbf{After round 3}: \\
						$26 + 4 \cdot 4 = 44$ \\
						\vspace{-0.1cm} \\
						\resizebox{5cm}{!}{
							\begin{tabular}{c | c c c c c c c c}
								  & 0        & 1        & 2        & 3        & 4 & 5        & 6        & 7 \\
								\hline
								0 & $\times$ &          &          &          &        & $\times$ & $\times$ & $\times$ \\
								1 & $\times$ & $\times$ &          &          &        &          & $\times$ & $\times$ \\
								2 & $\times$ & $\times$ & $\times$ &          &        &          &          & $\times$ \\
								3 & $\times$ & $\times$ & $\times$ & $\times$ &        &          &          &          \\
								4 & $\times$ & $\times$ & $\times$ & $\times$ &        & $\times$ & $\times$ & $\times$ \\
								5 & $\times$ & $\times$ & $\times$ & $\times$ &        & $\times$ & $\times$ & $\times$ \\
								6 & $\times$ & $\times$ & $\times$ &          &        & $\times$ & $\times$ & $\times$ \\
								7 & $\times$ & $\times$ & $\times$ & $\times$ &        &          & $\times$ & $\times$ \\
							\end{tabular}
						}
					\end{minipage}
					\begin{minipage}{0.42\linewidth}
						\vspace{0.3cm}
						\textbf{B2 Barrier:} \\
						$(n-1) \cdot (n-1) = 49$ \\
						\vspace{-0.1cm} \\
						\resizebox{5cm}{!}{
							\begin{tabular}{c | c c c c c c c c}
								  & 0        & 1        & 2        & 3        & 4 & 5        & 6        & 7 \\
								\hline
								0 & $\times$ & $\times$ & $\times$ & $\times$ &   & $\times$ & $\times$ & $\times$ \\
								1 & $\times$ & $\times$ & $\times$ & $\times$ &   & $\times$ & $\times$ & $\times$ \\
								2 & $\times$ & $\times$ & $\times$ & $\times$ &   & $\times$ & $\times$ & $\times$ \\
								3 & $\times$ & $\times$ & $\times$ & $\times$ &   & $\times$ & $\times$ & $\times$ \\
								4 &          &          &          &          &   &          &          &          \\
								5 & $\times$ & $\times$ & $\times$ & $\times$ &   & $\times$ & $\times$ & $\times$ \\
								6 & $\times$ & $\times$ & $\times$ & $\times$ &   & $\times$ & $\times$ & $\times$ \\
								7 & $\times$ & $\times$ & $\times$ & $\times$ &   & $\times$ & $\times$ & $\times$ \\
							\end{tabular}
						}
					\end{minipage}
					\label{tab:table-dissemination-progress}
				\end{table}
			\item for non-power of two process counts the effect worsens. Where for power of two process counts $n-1$ remote writes need to be executed before the barrier is finished, non-power of two process counts need to issue $2^{\lceil \log_2 n \rceil} - 1$ writes.
			\item the B2 Barrier does need multiple rounds to finish. The last arriving thread can fetch all others from one other process. And all others need to do one more successful remote read. Therefore $n$ remote reads, which can be issued in parallel, are needed to complete the barrier.
			\item quantifying this effect will be done in the following section.
		\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model checking}
\label{ssec:analysis-modelchecking}
\begin{itemize}
	\item SPIN\cite{spin, hol97}, PRISM\cite{prism, knp09}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Preliminaries}
\label{sssec:analysis-modelchecking-preliminaries}
\begin{itemize}
	\item continuous-time Markov chains.
	\item further details in textbooks like \cite{kul95, ks76}
	\item if $S$ is a finite set, then a distribution on $S$ is a function $\nu:S \rightarrow [0,1]$ with $\sum\limits_{s \in S} \nu (s) = 1$. For $U \subseteq S$, $\nu (U)$ is a shortform notation for $\sum\limits_{s \in U} \nu (s)$.
	\item CTMC tuple $\mathcal{M} = (S, \mathit{Act}, R, \mu)$, where $S$ is a finite set of states, $Act$ a finite set of action names and $R$ a function of type $S \times \mathit{Act} \times S \rightarrow \mathbb{R}_{\ge 0}$, called the rate matrix of $\mathcal{M}$. $\mu$ is a distribution on $S$ specifying the probabilities for the initial states.
	\item $s \xrightarrow{\lambda : \alpha} s'$ if $R(s, \alpha, s') = \lambda > 0$ means $\mathcal{M}$ has a transition from $s$ to $s'$ with action label $\alpha$ and rate $\lambda$
	\item $\lambda$ specifies the rate of the exponential distribution. That is the probability for the transition $s \xrightarrow{\lambda : \alpha} s'$ to be ready for firing some time in the interval $[0,t]$ is $1-e^{- \lambda t}$. The average delay  of this transition is $\frac{1}{\lambda}$.
	\item If $R(s, \alpha, s') = 0$ then there is no transition in $\mathcal{M}$ from $s$ to $s'$ via $\alpha$.
	\item choice between enabled transition is made through the race condition. Thus, the time-abstract probability to fire a particular transition $s \xrightarrow{\lambda : \alpha} s'$ is $P(s, \alpha, s') = \frac{\lambda}{E(s)}$ where $E(s)$ is the exit rate of state $s$, i.e., the sum of the rates of all outgoing transitions of state $s$. The probability that $s \xrightarrow{\lambda : \alpha} s'$ will fire within $t$ time units is then $P(s, \alpha, s') \cdot (1 - e^{- E(s) \cdot t})$.
	\item paths in a CTMC are sequences of consecutive transitions augmented by the time points when they are taken.
	\item For the analysis of such transition systems we employ the logic CSL \cite{assb96, bhhk00, knp07}. (my formulation)
	\item To specify measurable sets of infinite paths, we will use an LTL-like notation, such as $\lozenge T$ (``eventually T'') and $\square T$ (``always T'') where $T \subseteq S$.
	\item Instead of naming states we will oftentimes use state predicates and propositional formulas to describe sets of states. Their meaning will be obvious from the context. (my formulation)
	\item we will also study reward based properties formalized using the logic CSRL \cite{bhhk00, knp07}
	\item This requires the extension of $\mathcal{M}$ by two reward functions. $\mathit{srew} : S \rightarrow \mathbb{R}$ specifies the reward to be earned per time  unit when staying in state $s$. $\mathit{trew} : S \times S \rightarrow \mathbb{R}$ assigns a reward to each transition between two states regardless of action labels.
	\item For finite paths one can then reason about cumulative and reachability and rewards.
	\item Suppose $\pi$ is a finite path where the underlying state sequence is $s_1 s_2 ...s_n$ and let $t_0 = 0$ and $t_i$ the time point where $\pi$ takes the $i$-th transition.
	\item The accumulated reward of $\pi$ is
		\begin{center}
			$\mathit{Rew}(\pi) = \mathlarger{\sum}\limits_{i=0}^{n-1} \big( (t_{i+1} - t_i) \cdot \mathit{srew}(s_i) + \mathit{trew}(s_i, s_{i+1}) \big)$
		\end{center}
		It is the sum, over all states but the last, of the state reward multiplied by the time spent in this state plus the reward assigned to each transition between consecutive states in $\pi$.
	\item cumulative reward
		\begin{center}
			$\mathrm{CumRew}(\pi, t) = \mathit{Rew}(\pi(s_0:s_j))$
		\end{center}
		where $j = min \{ i~|~t_i \ge t \}$, $\pi(s_0:s_j)$ for the sub path of $\pi$ starting in $s_0$ and ending in $s_j$
	\item reachability reward
		\begin{center}
			$\mathrm{ReaRew}(\pi, \Phi) = \mathit{Rew}(\pi(s_0:s_j))$
		\end{center}
		where $j = min \{ i~|~s_i \models \Phi \}$
	\item the extension of these functions from paths to states and models is done the usual way and can, for example, be found in \cite{bhhk03}
	\item in the analysis of the protocols we will for example deal with the state reward function that assigns value 1 to each state. In this case $\mathrm{ReaRew}(T)$ can be interpreted as the average cycle count to reach $T$.
	\item We also wish to count remote memory access operations. For this we assign value 1 to each transition that represents such an operation. $\mathrm{ReaRew}(T)$ is then the average number of remote memory access operations issued until reaching $T$.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Modelling} %AE: modeling, BE: modelling
\label{sssec:analysis-modelchecking-modelling}
\begin{itemize}
	\item distinct models for functional and quantitative properties
	\item because resetting and repetition only needed for functional properties + we can save state space by stripping the model of unnecessary information for the quantitative analysis
	\item ordinary (non-stochastic) transition systems for functional models and CTMCs for quantitative models
	\item Spin\cite{spin, hol97} for model checking functional properties
	\item PRISM\cite{prism, knp09} for quantitative analysis
	\item in the following graphs we will use control flow diagrams and CTMCs
	\item The CTMCs of the protocols will be composed of multiple modules.
	\item synchronization between modules on the CTMC level is done via synchronizing actions.
	\item composed modules are executed interleaved except if transitions require synchronizing actions. A synchronizing action is an action that can be issued by at least two modules. A transition with such an action can only fire if all modules, that are able to fire this action eventually, are presently in a state where they can fire it. The rate of such a transition is determined by any one module. For each synchronization action, in our case, only one module will specify a rate for this action.
	\item This corresponds to the following SOS rules
	\item If no other module uses the action $\alpha$
		\begin{center}
			$\mathlarger{\frac{s \xrightarrow{\lambda : \alpha} s'}{\langle s , \overline{x} \rangle \xrightarrow{\lambda : \alpha} \langle s', \overline{x} \rangle}}$
		\end{center}
	\item $\overline{x}$ stands for the tuple of local state of all other modules
	\item if $\alpha$ is used by two modules
		\begin{center}
			$\mathlarger{\frac{s \xrightarrow{\lambda : \alpha} s', t \xrightarrow{\alpha} t'}{\langle s, t , \overline{x} \rangle \xrightarrow{\lambda : \alpha} \langle s', t', \overline{x} \rangle}}$
		\end{center}
	\item analogous rules can be stated for more than two modules sharing an action.
\end{itemize}

%%%%%%%%%%
\paragraph{Shared memory}
\label{sssec:analysis-modelchecking-modelling-shared-memory}
\begin{itemize}
	\item todo reason why modelling cache? (very short. Has been mentioned above)
		\begin{itemize}
			\item memory access determines performance of synchronization protocols
			\item memory access is cached so that effectively we only deal with cache latency
		\end{itemize}
	\item shared memory variable encapsulated in own CTMC modules.
	\item We identify a variable with the cache line it resides on.
	\item each thread has one copy of such a cache line.
	\item synchronization between cache line copies and with other modules of a model, like the actual program to be modelled, is done via synchronizing actions. The triggering thread of a cache operation determines the rate of the operation. For convenience we use combinations of action labels like $\alpha \lor \beta$ meaning that we have two transitions, one with action $\alpha$ and one with $\beta$ between the two states. Either action can trigger a transition between these two states.
	\item we use an MSI cache coherency protocol extended by support for atomic operations. MSI model is a basic cache model and explanations on it can readily be found online\cite{msi}.
	\item A cache line copy can be in one of four states. \emph{modified}, means the thread has the only up-to-date copy of the cache line and all other copies are marked invalid. \emph{shared}, which means the core has an up-to-date copy, but other threads may a correct copy, too. \emph{invalid}, meaning the local copy is not up-to-date. We extend these three states by a fourth named \emph{atomic}. It is the same as modified, except that between $\mathit{atomic\_begin}$ and $\mathit{atomic\_end}$ the access to this variable is blocked from access through other modules. Figure~\ref{fig:model-shared-memory} shows how a cache line copy changes its state depending on events occurring. The solid transitions are triggered by events occurring on the thread owning the cache line copy, whereas dashed transitions are due to events issued indirectly by actions of other threads.
	\item CTMC Figure~\ref{fig:model-shared-memory}
		\begin{figure}[htbp]
			\centering
			\input{tikz/model-shared-memory}
			\caption{CTMC module of a shared memory variable}
			\label{fig:model-shared-memory}
		\end{figure}
	\item For example if a core reads a variable, it first needs to make sure that all other cores take notice and change its cache line copy state to shared in case it was modified before. After this the core fetches the cache line, marks its copy shared and continues reading the variable.
	\item taken from complex-lab, rephrase: The timings of shared read and shared write are as follows. A read on a modified or shared variable does not imply any extra work. It can immediately use the cached data. Therefore we consider this operation instantaneous. If a read is done on an invalid copy, it has to first fetch an up-to-date copy of the cache line and make sure all other cores are notified before it can proceed. This usually takes around 50 CPU cycles. A write on a modified variable is instantaneous, since all other cores do not have an up-to-date copy and therefore do not have to be notified. If the cache line in question is in a shared or invalid state, we first have to wait until all other cores follow the request to invalidate their copies of the cache line. After this we can safely mark our copy modified and write to it. This operation usually takes around 100 cycles.
	\item taken from complex-lab, rephrase: The number of cycles these operations take is strongly dependent on the CPU itself. For the CTMC model we assume a cache read to use 50 cycles and a cache write to use 100 cycles. The rates of the transitions corresponding to the described events is then the reciprocal of the cycle time: 1, $\frac{1}{50}$ or $\frac{1}{100}$.
	\item $\lambda_c$, $\lambda_r$, $\lambda_w$, $\lambda_a$ one cycle, read, write, atomic\_begin rate. Reciprocal of the average number of cycles needed for each of these actions
	\item explain CTMC behaviour and work around through setting atomic\_begin to something higher than write, although normally it would be just that
	\item explain to move atomic rate to atomic end to make parallel and serial code look different (explain how parallel and serial doesn't make a difference for interleaved CTMCs)
\end{itemize}

%%%%%%%%%%
\paragraph{Central Counter Barrier}
\label{sssec:analysis-modelchecking-modelling-central-counter}
\begin{itemize}
	\item repeat variables and initial configuration from the algorithm box
	\item Algorithm explained in Section~\ref{sssec:background-currently-used-shared}
	\item Control flow diagram Figure~\ref{fig:model-central-counter}
		\begin{figure}[htbp]
			\centering
			\input{tikz/model-central-counter}
			\caption{Control flow diagram of the Central Counter Barrier}
			\label{fig:model-central-counter}
		\end{figure}
	\item rates for reading writing and atomic operations are determined by the module for the shared variable \texttt{barrier}
	\item the synchronization to the shared variable module is hinted at in grey
	\item The control flow has been augmented by one rate that the resulting CTMC will use. $\lambda_{work}$ is the work rate, meaning the time spent doing actual work. It is used to add an initial distribution to the model so that we can control how the arrival times of the threads at the barrier are distributed. In particular not all threads arrive at the exact same instant in time at the barrier, which is an extreme case.
	\item atomic op consists of multiple smaller transitions: begin atomic (locks the variable), read, decrement, write, end atomic (releases the lock).
	\item functional model does resetting and repetition. Three barriers, as described in~\ref{ssec:background-building-blocks}. (if you did not describe it there, you have to do it here)
\end{itemize}

%%%%%%%%%%
\paragraph{B1 Barrier}
\label{ssssec:analysis-modelchecking-modelling-b1}
\begin{itemize}
	\item repeat variables and initial configuration from the algorithm box
	\item Algorithm explained in Section~\ref{ssec:new-b1}
	\item Control flow diagram Figure~\ref{fig:model-b1}
		\begin{figure}[htbp]
			\centering
			\input{tikz/model-b1}
			\caption{Control flow diagram of the B1 Barrier}
			\label{fig:model-b1}
		\end{figure}
	\item each element of the array \texttt{barrier} is located in an own cache line. Therefore each thread has \texttt{threadCount} cache line copy modules, one for each element in the array. In total a CTMC model of this protocol has $\mathtt{threadCount}^2$ cache line copy modules plus $\mathtt{threadCount}$ thread modules.
	\item As described in the previous paragraph, $\lambda_{work}$ is the work rate and all other rates are determined by the cache line copy modules.
	\item functional model does resetting and repetition using a counter instead of bools as described in Section~\ref{ssec:background-building-blocks} (if you did not describe it there, you have to do it here)
	\item functional model splits some of the transitions to allow for more interleaving, thus more errors can be revealed.
\end{itemize}

%%%%%%%%%%
\paragraph{Dissemination Barrier}
\label{ssssec:analysis-modelchecking-modelling-dissemination}
\begin{itemize}
	\item repeat variables and initial configuration from the algorithm box
	\item no special module for distributed memory, since all variables are independently stored in the local memory of each process. There is no influence that copies data between processes other then the user-specified program itself.
	\item Algorithm explained in Section~\ref{sssec:background-currently-used-distributed}
	\item Control flow diagram augmented with rates: Figure~\ref{fig:model-dissemination}
		\begin{figure}[htbp]
			\centering
			\input{tikz/model-dissemination}
			\caption{Augmented Control flow diagram of the Dissemination Barrier}
			\label{fig:model-dissemination}
		\end{figure}
	\item As in the two previous paragraphs $\lambda_{work}$ is the work rate.
	\item we assume all local operations to be much quicker, ten to a thousand times, than remote operations. Therefore we assign the rate $\lambda_c$ (one CPU cycle) to local operations. $\lambda_p$, which is assigned to remote write (put) operations, is assigned a rate of $\frac{1}{100}$. This value is, like the shared memory rates, hardware-dependent. For simplicity's sake we therefore assume a remote transfer to take 100 cycles.
	\item one could make this more fine grained by introducing separate rates for local reading and writing.
	\item \texttt{to} and \texttt{from} are shorthands for \texttt{processIndex $\pm$ dist (mod processCount)}
	\item In the CTMC model, resulting from unwinding this control flow diagram, a put onto a remote process is represented as a synchronized action, so the target process stores the new value in its CTMC module.
	\item functional model does resetting and repetition with a counter instead of bools as described in Section~\ref{ssec:background-building-blocks}.
\end{itemize}

%%%%%%%%%%
\paragraph{B2 Barrier}
\label{ssssec:analysis-modelchecking-modelling-b2}
\begin{itemize}
	\item repeat variables and initial configuration from the algorithm box
	\item Algorithm explained in Section~\ref{ssec:new-b2}
	\item Control flow diagram augmented with rates: Figure~\ref{fig:model-b2}
		\begin{figure}[htbp]
			\centering
			\input{tikz/model-b2}
			\caption{Augmented Control flow diagram of the B2 Barrier}
			\label{fig:model-b2}
		\end{figure}
	\item Rates are like in previous section, except we now use remote read (get) with a rate of $\lambda_g = \frac{1}{100}$.
	\item functional model does resetting and repetition using three barriers as described in~\ref{ssec:background-building-blocks}.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Functional properties}
\label{sssec:analysis-modelchecking-functional-properties}
\begin{itemize}
	\item intro text here
	\item in this section we will use \emph{thread} and \emph{process} interchangeable, because for the modelling side the wording makes no difference.
	\item LTL as logic, Spin as model checker
	\item $T$ is the set of all threads
	\item $\mathit{left_t}$ is a predicate describing all states where thread $t$ has passed the barrier
	\item $entered_t$ is a predicate describing all states where thread $t$ has passed the initial work period and therefore entered the barrier.
	\item \textbf{A:} \emph{a thread may only exit the barrier if all threads have entered it}
		\begin{center}
			$\square \big( ~ ( \exists t \in T . \mathit{left_t} ) \implies \forall s \in T. \mathit{entered_s} ~ \big)$
		\end{center}
	\item \textbf{B:} \emph{if all threads entered the barrier, they will all leave it in a finite amount of time}
		\begin{center}
			$\square \big( ~(\forall t \in T . \mathit{entered_t} ) \implies \forall s \in T. \lozenge \mathit{left_s} ~ \big)$
		\end{center}
	\item A and B are the two basic properties a barrier protocol must satisfy
	\item A could be seen as safety and B as liveness of a barrier protocol
	\item yes it works.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%
\subsubsection{Quantitative properties}
\label{sssec:analysis-modelchecking-quantitative-properties}
\begin{itemize}
	\item intro text here
	\item in this section we will use \emph{thread} and \emph{process} interchangeably, because for the modelling side the wording makes no difference. (same is in section before)
	\item CSRL as logic, PRISM as model checker
\end{itemize}

%%%%%%%%%%
\paragraph{Identification and formalisation}
\label{ssssec:analysis-modelchecking-quantitative-properties-identification}
\begin{itemize}
	\item interesting values to query for
		\begin{itemize}
			\item time: unit: CPU cycles. srew 1.
			\item energy consumption: unit: Joule, and rate of energy consumption: unit: Joule per second = Watt. (assume 2.5ghz. W (nJ/ns) = 2.5 * nJ/cycle)
				\begin{itemize}
					\item trew shared memory operations = 200nJ
					\item trew cached shared memory operation = 2nJ
					\item trew any other operation = 0nJ
					\item for dist: remote 200nJ, local memory op = 2nJ, others none
					\item baseline of 11W
					\item These numbers are a very limited estimate derived from a small reference measurement. The numbers can be found in Section~\ref{ssec:energy-measurement}.
				\end{itemize}
			\item (diss, B2) number of remote transfers. trew for remote operations = 1. srew =0.
		\end{itemize}
	\item interesting points in time
		\begin{itemize}
			\item (A) first thread entered the barrier
			\item (B) last thread entered the barrier
			\item (C) first thread left the barrier
			\item (D) last thread left the barrier
			\item (W) (only for Central Counter and B1) the last thread finished writing
			\item (R$i$) (only for Dissemination) last process left round $i$, i.e. entered round $i+1$. The current round of process is determined through the variable $\mathit{dist}$. $\mathit{round} = 1 + \log_2 \mathit{dist}$.
		\end{itemize}
	\item resulting properties
		\begin{itemize}
			\item $T$ is the set of all threads
			\item \textbf{A:} \emph{Average reward until the first thread enters the barrier}
				\begin{center}
					$\mathrm{ReaRew}(\exists t \in T . \mathit{entered_t} )$
				\end{center}
			\item \textbf{B:} \emph{Average reward until the last thread enters the barrier}
				\begin{center}
					$\mathrm{ReaRew}(\forall t \in T . \mathit{entered_t} )$
				\end{center}
			\item \textbf{C:} \emph{Average reward until the first thread leaves the barrier}
				\begin{center}
					$\mathrm{ReaRew}(\exists t \in T . \mathit{left_t} )$
				\end{center}
			\item \textbf{D:} \emph{Average reward until the last thread leaves the barrier}
				\begin{center}
					$\mathrm{ReaRew}(\forall t \in T . \mathit{left_t} )$
				\end{center}
			\item \textbf{W:} \emph{Average reward until the last thread finished writing}
				\begin{center}
					$\mathrm{ReaRew}(\forall t \in T . \mathit{done\_writing_t} )$
				\end{center}
			\item \textbf{R$i$:} \emph{Average reward until the last thread left round $i$}
				\begin{center}
					$\mathrm{ReaRew}(\forall t \in T . \mathit{left\_round_{i,t}} )$
				\end{center}
			\item where the predicates have the obvious meaning.
			\item and combinations of these to get measures like (D-B) last in until last out.
		\end{itemize}
	\item which parameters to variate
		\begin{itemize}
			\item thread count. Maximum 4 is almost always considered. Model checking limitations bla.
			\item work rate. Variate the distribution of the times the threads enter the barrier.
		\end{itemize}
\end{itemize}

%%%%%%%%%%
\paragraph{Evaluation}
\label{ssssec:analysis-modelchecking-quantitative-properties-results}
\begin{itemize}
	\item 
\end{itemize}

%%%%%
\subparagraph{Shared memory barriers}
\label{sssssec:analysis-modelchecking-quantitative-properties-results-shared}
\begin{itemize}
	\item work = 100. Expected time it takes a thread to arrive at the barrier is 100 cycles.
	\item last to last, last to first Figure~\ref{fig:c1-time-work-100-B-C-D}
		\begin{figure}[htbp]
			\centering
			\includegraphics[width=10cm]{charts/c1-time-work-100-B-C-D}
			\caption{Expected execution time}
			\label{fig:c1-time-work-100-B-C-D}
		\end{figure}
		\begin{itemize}
			\item both barriers similarly fast when considering the last one to finish
			\item B1 is faster for the first one to leave
		\end{itemize}
	\item distribution of total time and energy spent: Figure~\ref{fig:c1-work-100-partition}
		\begin{figure}[htbp]
			\centering
			\begin{minipage}{0.53\linewidth}
				\includegraphics[height=4.3cm]{charts/c1-time-work-100-partition}
			\end{minipage}
			\begin{minipage}{0.46\linewidth}
				\includegraphics[height=4.3cm]{charts/c1-energy-work-100-partition}
			\end{minipage}
			\caption{Distribution of total time spent and energy consumed}
			\label{fig:c1-work-100-partition}
		\end{figure}
	\item time
		\begin{itemize}
			\item stacked fashion. Including the time of the previous bar.
			\item the initial work distribution is the same for all processes and protocols
			\item cc: first thread leaves a few cycles after writing is done. Because the last one to write its value will likely immediately read the completed barrier and leave.
			\item from the moment the first thread leaves, all threads only read. No one writes anymore.
			\item writing, as expected, takes longer for Central Counter and reading takes longer for the B1 Barrier
			\item all parts take longer with increasing thread count, since CTMC semantic (described in Section~\ref{sssec:analysis-modelchecking-preliminaries}) makes a transition get an exit rate of the sum of the rates of all enabled transitions.
			\item in particular, assume you have $n$ interleaved modules (or concurrent threads of execution) where one transition in each module is enabled. The exit rate of the first transition to be triggered is then  $n \cdot \lambda$. The expected time of a parallel execution of all transitions is $\sum_{i=1}^{n} \frac{1}{i \cdot \lambda}$ time rather than $\frac{1}{\lambda}$.
			\item for example the expected number of cycles for the first process to enter is $\approx \frac{100}{n}$. The last one is finished working after $\approx$ 150, 183 and 208 cycles, for two, three and four processes.
			\item Especially reading for Central Counter (2: 50, 3:75, 4:92 cycles). Normally this period is expected to be nearly the same for all thread counts.
			\item writing for the B1 Barrier is expected to take less time than shown here (100 across the board instead of 2:125, 3:138, 4:146), for the same reason as reading takes longer as expected for the Central Counter Barrier.
		\end{itemize}
	\item energy
		\begin{itemize}
			\item similar to time
			\item the B1 Barrier needs more energy. Because it performs more shared memory operations and more local operations in the same amount of time, where the Central Counter Barrier spends considerable time waiting for atomic operations to finish.
			\item Central Counter's energy scales similarly to time.
			\item amount of energy for the B1 Barrier grows faster than time needed.
			\item the B1 Barrier spends considerable energy from the last commitment to first leaving the barrier.
		\end{itemize}
	\item rate of power consumption per protocol phase: Figure~\ref{fig:c1-power-work-100}
		\begin{figure}[htbp]
			\centering
			\includegraphics[width=10cm]{charts/c1-power-work-100}
			\caption{Rate of energy consumption for each phase}
			\label{fig:c1-power-work-100}
		\end{figure}
		\begin{itemize}
			\item same partitioning as the previous figure.
			\item (A) all working has 11W - the baseline.
			\item overall low for Central Counter (16-18.3W), B1 (19.4 - 30.6)
			\item writing considerably higher for rs, because quicker and more shared memory operations.
			\item no one left: cc: high because very very short, rs: high because many shared reads across many threads.
			\item one left similar because cc: shared reads, rs: shared reads, and local reads.
		\end{itemize}
	\item reading vs writing: Figure~\ref{fig:c1-time-work-100-percent}
		\begin{figure}[htbp]
			\centering
			\includegraphics[width=10cm]{charts/c1-time-work-100-percent}
			\caption{Time spent reading versus writing}
			\label{fig:c1-time-work-100-percent}
		\end{figure}
		\begin{itemize}
			\item writing is from (A) to (W). Reading is from (W) to (D).
			\item with increasing thread count there is a shift from writing to reading in execution time.
		\end{itemize}
	\item work=1000. Still very small. (According to \cite{rab00} at a barrier each process spends on average 8194 CPU cycles idly\footnote{1813 microseconds accumulated idle time at 450MHz on 99.6 processes})
	\item last to last, last to first Figure~\ref{fig:c1-time-work-1000-B-C-D}
		\begin{figure}[htbp]
			\centering
			\includegraphics[width=10cm]{charts/c1-time-work-1000-B-C-D}
			\caption{Expected execution time}
			\label{fig:c1-time-work-1000-B-C-D}
		\end{figure}
		\begin{itemize}
			\item very different picture.
			\item Central Counter faster in all respects.
			\item probability of a thread trying to write while no one is writing at the moment increases due to the larger working period distribution. Writes for the B1 Barrier tend to be serialized for the same reason, but B1 still reads longer.
			\item time for one thread from entry to exit converges to one atomic op + a few cycles. 105 cycles for Central Counter.
		\end{itemize}
	\item distribution of total time and energy spent: Figure~\ref{fig:c1-work-1000-partition}
		\begin{figure}[htbp]
			\centering
			\begin{minipage}{0.53\linewidth}
				\includegraphics[height=4.3cm]{charts/c1-time-work-1000-partition}
			\end{minipage}
			\begin{minipage}{0.46\linewidth}
				\includegraphics[height=4.3cm]{charts/c1-energy-work-1000-partition}
			\end{minipage}
			\caption{Distribution of total time spent and energy consumed}
			\label{fig:c1-work-1000-partition}
		\end{figure}
		\begin{itemize}
			\item Excluding (A) and (B) since they are large in comparison to the barrier
		\end{itemize}
	\item time
		\begin{itemize}
			\item as expected
		\end{itemize}
	\item energy
		\begin{itemize}
			\item writing is constant across multiple threads because the atomic op forbids reading the variable thus there is few busy waiting, whereas B1 is constantly polling. And the amount increases with increasing thread count.
		\end{itemize}
	\item rate of power consumption per protocol phase: Figure~\ref{fig:c1-power-work-1000}
		\begin{figure}[htbp]
			\centering
			\includegraphics[width=10cm]{charts/c1-power-work-1000}
			\caption{Rate of energy consumption for each phase}
			\label{fig:c1-power-work-1000}
		\end{figure}
		\begin{itemize}
			\item more even. (1) longer work period. (2) cc and rs use similarly many shared and local operations (due to busy waiting) in the reading phase ((D) one left). Energy usage is dominated by busy waiting of threads.
			\item the B1 Barrier can catch up considerably. (cc:14.6-20.9, rs:15.3-23.5)
		\end{itemize}
	\item with larger and more realistic, since 1000 cycles is still very few, work periods this yields a series of writing operations for both algorithms, and a little more time spent reading for the B1 Barrier.
\end{itemize}

%%%%%
\subparagraph{Distributed memory barriers}
\label{sssssec:analysis-modelchecking-quantitative-properties-results-distributed}
\begin{itemize}
	\item work = 100
	\item distribution of total time spent and total energy consumed: Figure~\ref{fig:d2-work-100-partition}
		\begin{figure}[htbp]
			\centering
			\begin{minipage}{0.54\linewidth}
				\includegraphics[height=4.3cm]{charts/d2-time-work-100-partition}
			\end{minipage}
			\begin{minipage}{0.45\linewidth}
				\includegraphics[height=4.3cm]{charts/d2-energy-work-100-partition}
			\end{minipage}
			\caption{Distribution of total time spent and energy consumed}
			\label{fig:d2-work-100-partition}
		\end{figure}
		\begin{itemize}
			\item the B2 Barrier is quicker for process counts over two
			\item Dissemination finishes 23 cycles earlier than the B2 Barrier for 2 processes because remote write can be executed during the work period of the other process, whereas the B1 Barrier still needs to execute 2 remote reads after the last process entered the barrier.
			\item rf: first one to leave unrealistic. Should not be quicker than 100 cycles. CTMC exit rate calculation.
			\item more time between first and last to leave.
			\item last and first exit at almost the same time for Dissemination, because when the second arrives and writes his part into the remote peers memory both will quickly exit. This cannot happen for processes since when the first leaves there might still be pending remote writes.
			\item energy consumption similar to time spent in proportion
		\end{itemize}
	\item Dissemination: distribution of total time spent and energy consumed: Figure~\ref{fig:d-work-100-partition}
		\begin{figure}[htbp]
			\centering
			\begin{minipage}{0.43\linewidth}
				\includegraphics[height=4.2cm]{charts/d-time-work-100-partition}
			\end{minipage}
			\begin{minipage}{0.56\linewidth}
				\includegraphics[height=4.2cm]{charts/d-energy-work-100-partition}
			\end{minipage}
			\caption{Distribution of total time spent and energy consumed for the Dissemination Barrier}
			\label{fig:d-work-100-partition}
		\end{figure}
		\begin{itemize}
			\item stair-esque running time due to $\lceil \log_2 \mathit{processCount} \rceil$ number of rounds. During those rounds remote writes are issued concurrently.
			\item unfavourable for non-power-of-two processes counts. Ref to Section~\ref{sssec:analysis-general-distributed}
			\item slight increase in per-round execution time with number of processes is result of CTMC interleaving semantics. This behaviour is realistic, because resource contention for remote writes and + non-uniform distribution durations for those writes, but we didn't specifically model this.
			\item energy consumption increase per round a bit larger than runtime increase. This is partly because more remote operations are issued as described in Section~\ref{ssssec:analysis-modelchecking-modelling-dissemination}. And partly because one more process is locally spinning before entering a new round.
		\end{itemize}
	\item rate of power consumption per phase: Figure~\ref{fig:d2-power-work-100}
		\begin{figure}[htbp]
			\centering
			\includegraphics[width=10cm]{charts/d2-power-work-100}
			\caption{Rate of energy consumption for each phase}
			\label{fig:d2-power-work-100}
		\end{figure}
		\begin{itemize}
			\item All phases except the last are very similar.
			\item largely because comparing number of local and remote operations issued to the rewards given for these operations the amount of energy consumed is expected to be similar.
			\item Dissemination issues less remote operations, but due to local wait spinning a lot of local operations, the B2 Barrier on the other hand polls remotely.
			\item in the last phase the B2 Barrier draws less power because it takes a lot more time for a little more energy to be consumed.
			\item overall the B2 Barrier performs slightly better.
		\end{itemize}
	\item longer work period. work=1000
	\item distribution of total time spent and energy consumed: Figure~\ref{fig:d2-work-1000-partition}
		\begin{figure}[htbp]
			\centering
			\begin{minipage}{0.54\linewidth}
				\includegraphics[height=4.3cm]{charts/d2-time-work-1000-partition}
			\end{minipage}
			\begin{minipage}{0.45\linewidth}
				\includegraphics[height=4.3cm]{charts/d2-energy-work-1000-partition}
			\end{minipage}
			\caption{Distribution of total time spent and energy consumed}
			\label{fig:d2-work-1000-partition}
		\end{figure}
		\begin{itemize}
			\item excluding (A) and (B) because long.
			\item first one to leave for rf unrealistic as in Figure~\ref{fig:d2-work-100-partition}
		\end{itemize}
	\item Dissemination: distribution of total time spent and energy consumed: Figure~\ref{fig:d-work-1000-partition}
		\begin{figure}[htbp]
			\centering
			\begin{minipage}{0.43\linewidth}
				\includegraphics[height=4.2cm]{charts/d-time-work-1000-partition}
			\end{minipage}
			\begin{minipage}{0.56\linewidth}
				\includegraphics[height=4.2cm]{charts/d-energy-work-1000-partition}
			\end{minipage}
			\caption{Distribution of total time spent and energy consumed for the Dissemination Barrier}
			\label{fig:d-work-1000-partition}
		\end{figure}
		\begin{itemize}
			\item as expected
			\item In comparison to Figure~\ref{fig:d-work-100-partition} there is almost no additional time cost per process associated. CTMC semantic effect diminishes because the remote writes in the rounds are approximately serial. Therefore a write is oftentimes activatable at the same time as other processes are repeatedly executing local reads. The expected time such a write takes is then $\approx 100$ CPU cycles.
			\item the durations for the rounds are related to the progress explanation in Section~\ref{sssec:analysis-general-distributed}. When the last process arrives at the barrier, exactly one remote write is needed to finish round one, two more writes are needed to finish round two and four writes are needed to finish the third round. Therefore, and because of CTMC interleaving semantics, approximately 100, 150 and 183 cycles are needed to finish rounds one, two and three.
			\item Energy usage increases with each additional progress for the same reasons as in Figure~\ref{fig:d-work-100-partition}
			\item progress problem.
			\item there is a fixed cost for each additional round in the Dissemination Barrier.
			\item due to model checking limitations (a very large state space), we cannot quantify beyond four processes, i.e. two rounds.
			\item as shown in Figure~\ref{fig:d-work-1000-partition} the B2 Barrier is indeed quicker.
			\item in relation to the results from work period 100, the B2 Barrier could not increase its headway at work period 1000.
			\item this does not necessarily mean that the progress problem does not exist. It might for example mean that the B2 Barrier does not handle this situation as efficiently as we expected or that it did better than we expected for the 100 cycle work period case. Either way it is definitely possible to be generally faster than the Dissemination Barrier is for this scenario.
		\end{itemize}
	\item rate of power consumption per phase: Figure~\ref{fig:d2-power-work-1000}
		\begin{figure}[htbp]
			\centering
			\includegraphics[width=10cm]{charts/d2-power-work-1000}
			\caption{Rate of energy consumption for each phase}
			\label{fig:d2-power-work-1000}
		\end{figure}
		\begin{itemize}
			\item overall power consumption slightly lower because of the 1000 cycle work period.
			\item everything else similar to Figure~\ref{fig:d2-power-work-100}.
		\end{itemize}
	\item number of remote transfers
		\begin{itemize}
			\item remote transfer is a generic term including remote reads and remote writes.
			\item fixed for Dissemination as explained in Section~\ref{sssec:analysis-general-distributed}. $n \cdot \lceil \log _2~n \rceil$
			\item number of successful transfers (transfers that read from an arrived processes) between $2 \cdot (n-1)$ and $n \cdot (n-1)$. Both practically never happen because randomness and access pattern.
			\item failed transfers (transfers where you read from a not yet arrived process).
			\item hard to estimate, because the randomness of the system defines this number.
			\item work=100. Figure~\ref{tab:d2-transfers-100}
				\begin{table}[htbp]
					\centering
					\caption{Expected number of remote transfers}
					\vspace{0.2cm}
					\begin{tabular}{r | r | r r}
						              & Threads & Successful transfers & Failed transfers \\
						\hline
						Dissemination & 2       & 2                     &                 \\
						              & 3       & 6                     &                 \\
						              & 4       & 8                     &                 \\
						\hline
						B2 Barrier    & 2       & 2                     & 0.99            \\
						              & 3       & 4.74                  & 1.73            \\
						              & 4       & 7.98                  & 2.44            \\
					\end{tabular}
					\label{tab:d2-transfers-100}
				\end{table}
				\begin{itemize}
					\item as expected the B2 Barrier issues more remote transactions than the Dissemination Barrier.
					\item it is still relatively low.
					\item especially the number of failed remote accesses is low in comparison to the number of successful ones.
					\item For process count 5, and other non-power of two process counts the situation might turn out better for the B2 Barrier, because of the additional rounds and thus additional transfers needed.
				\end{itemize}
			\item work=1000. Table~\ref{tab:d2-transfers-1000}
				\begin{table}[htbp]
					\centering
					\caption{Expected number of remote transfers}
					\vspace{0.2cm}
					\begin{tabular}{r | r r}
						Threads & Successful transfers & Failed transfers \\
						\hline
						2       & 2                     & 9.9             \\
						3       & 4.95                  & 22.89           \\
						4       & 8.78                  & 38.05           \\
					\end{tabular}
					\label{tab:d2-transfers-1000}
				\end{table}
				\begin{itemize}
					\item number of successful transfers very similar to work=100.
					\item failed transfers is of course more than before since the B2 Barrier is busy waiting on remote memory.
					\item back-off strategies could be used, if one would want to save the lost bandwidth, latency and energy in exchange for a longer duration of the protocol.
				\end{itemize}
		\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Discussion}
\label{ssec:analysis-discussion}
\begin{itemize}
	\item meta
		\begin{itemize}
			\item maybe leave out and put it in the evaluation section. If you have something to sum up or perhaps compare measurement and model checking then this section might still be useful.
			\item reread \url{http://www.usca.edu/biogeo/researchguide/writing.html#Results}
			\item "Discuss and explain your results"
			\item "Show how they support your thesis (or, if they don't, come up with a damned good reason real quick)"
			\item "It is important to separate objective facts clearly from their discussion"
		\end{itemize}

	\item all analysed barriers are functionally correct in the sense that they indeed perform a barrier synchronization.
	\item all shown barriers finish reasonably quick.
	\item all considered barriers use a reasonable amount of energy.

	\item shared memory
	\item despite atomic operations the Central Counter barrier is quick.
	\item the B1 Barrier approach to trade less time writing for more time reading does not pay off. Because: (1) in a balanced scenario the time lost due to queuing atomic operations is small. (2) no atomic operations means more busy waiting, thus more energy used.
	\item the B1 Barrier performs worse in all considered scenarios.

	\item distributed memory
	\item the B2 Barrier issues not much more remote operations than the Dissemination barrier despite busy waiting on remote memory.
	\item as conjectured in Section~\ref{sssec:analysis-general-distributed} the Dissemination barrier wastes substantial time when processes arrive even in small intervals.
	\item for non-power of two process counts the Dissemination barrier wastes substantial time and energy due to its $\lceil \log_2 n \rceil$ number of rounds and $n \cdot \lceil \log_2 n \rceil$ number of remote operations.
	\item B2 outperforms the Dissemination barrier in all scenarios.

	\item CTMC model checking semantic for interleaving modules is different from the intuitive notion of parallel execution. Therefore one has to exercise caution when interpreting model checking results.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and future work}
\label{sec:conclusion}
Conclusion:
\begin{itemize}
	\item meta
		\begin{itemize}
			\item "Don't leave it at the discussion: discuss what you/we can learn from the results. Draw some real conclusions. Separate discussion/interpretation of the results clearly from the conclusions you draw from them"
			\item condense results into a small passage
			\item ?repeat claim - overarching thesis - present an answer?
		\end{itemize}
	\item (overarching (1) barriers can be improved, (2) pW/CS (3) model checking is useful)
	\item we had an in-depth look at how current barriers work, and by which means barriers can be designed.
	\item We presented three new barriers, two of which show promise and we therefore conclude that there exists potential to improve barrier protocols.
	\item To achieve the shown performance, both new barriers do not use a fixed communication pattern, as today's barrier algorithms do, but rely on randomness to yield better results. This is one of the fundamental principles of pW/CS and shows that it is useful.
	\item Model checking enabled us to look into barrier algorithms at a more fine-grained level than measurement-based methods could hope to.
	\item We demonstrated the aptitude of model checking as a method for analysing synchronization protocols and algorithms in general.
\end{itemize}

Future work:
\begin{itemize}
	\item meta
		\begin{itemize}
			\item "Identify all shortcomings/limitations of your work, and discuss how they could be fixed"
		\end{itemize}
	\item measurement
		\begin{itemize}
			\item conduct measurements of the shared memory barriers (easy). Preliminary tests show that both the B1 and the B2 Barrier perform better than model checking analysis suggests.
			\item implement the B2 Barrier on RDMA hardware and benchmark against established algorithms
		\end{itemize}
	\item widen focus
		\begin{itemize}
			\item evaluate back-off strategies. (technical possibilities. Strategies like exponential back-off, linear back-off, etc.)
			\item mwait (wait for a change in a variable. Not polling actively).
			\item model and measure dissemination and B2 for shared memory.
		\end{itemize}
	\item CTMC exit rate semantic
		\begin{itemize}
			\item why is it like that?
			\item work around it?
			\item attempt to invent an variations on the CTMC semantic to suit modelling parallel execution better?
			\item try alternative model checking formalisms.
		\end{itemize}
	\item modelcheck larger: more threads. Symmetry reduction. Partial order reduction. Manual reduction.
	\item model check more fine-grained
		\begin{itemize}
			\item model multi layered cache. More realistic.
			\item model resource conflict. (We currently assume infinite bandwidth and amount of communication that can take at the same time)
			\item model topology between processes. Like cores/dies/packages/nodes/blades. Because it influence latency.
		\end{itemize}
	\item explore variations of the new barriers
		\begin{itemize}
			\item B2: Try different i increment strategies. Or perhaps randomize the choice of i.
			\item use remote write \& local read instead of local write \& remote read, because remote writing is preferred on some platforms, e.g. on the Epiphany\cite{epiphany}. AND this way one might be able to avoid failed remote operations entirely and keep busy waiting local.
		\end{itemize}
	\item misc
		\begin{itemize}
			\item~
		\end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
\section{Appendix}
\label{sec:appendix}
\subsection{Energy reference measurement for estimating the model parameters}
The CPU we used is an Intel\textregistered~Core\texttrademark~i5-2520M clocked at 2.50GHz.
\label{ssec:energy-measurement}
\begin{table}[htbp]
	\centering
	\caption{Reference measurement (without Hyper-Threading)}
	\vspace{0.2cm}
	\begin{tabular}{r | r | r r r}
		                         & Thread & Time     & Energy    & Rate of      \\
		                         & count  & (Cycles) & consumpt. & energy cons. \\
		                         &        &          & (nJ)      & (W)          \\
		\hline
		Idle                     &        &          &           &  4.0 \\
		Off-core under load      &        &          &           &  6.8 \\
		Load                     & 1      &          &           & 19.6 \\
		                         & 2      &          &           & 28.4 \\
		Busy waiting             & 1      &          &           & 16.4 \\
		                         & 2      &          &           & 22.8 \\
		\hline
		Add\&fetch (contested)   & 2      &    137.6 &     893.7 & 16.2 \\
		Add\&fetch (uncontested) & 2      &     19.3 &     158.5 & 20.5 \\
		Central Counter Barrier  & 2      &    227.4 &    1667.0 & 18.3 \\
		B1 Barrier               & 2      &    114.0 &    1076.1 & 23.6 \\
		Dissemination Barrier    & 2      &    144.0 &    1400.3 & 24.3 \\
		B2 Barrier               & 2      &     42.1 &     433.4 & 25.7 \\
	\end{tabular}
\end{table}
\begin{table}[htbp]
	\centering
	\caption{Reference measurement (with Hyper-Threading)}
	\vspace{0.2cm}
	\begin{tabular}{r | r | r r r}
		                          & Thread & Time     & Energy    & Rate of      \\
		                          & count  & (Cycles) & consumpt. & energy cons. \\
		                          &        &          & (nJ)      & (W)          \\
		\hline
		Idle                      &        &          &           &  4.0 \\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		Off-core under load       &        &          &           & 12.6 \\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		Load                      & 1      &          &           & 19.6 \\
		                          & 2      &          &           & 19.7 \\
		                          & 3      &          &           & 28.0 \\
		                          & 4      &          &           & 28.3 \\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		Busy waiting              & 1      &          &           & 17.2 \\
		                          & 2      &          &           & 17.5 \\
		                          & 3      &          &           & 22.9 \\
		                          & 4      &          &           & 22.9 \\
		\hline
		Add\&fetch (contested)    & 2      &    122.2 &     700.2 & 14.3 \\
		                          & 3      &    136.6 &     903.3 & 16.5 \\
		                          & 4      &    354.6 &    2349.9 & 16.7 \\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		Add\&fetch (unconstested) & 2      &     45.2 &     283.8 & 15.7 \\
		                          & 3      &     46.9 &     389.0 & 20.7 \\
		                          & 4      &     47.5 &     396.6 & 20.9 \\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		Central Counter Barrier   & 2      &     86.5 &     647.1 & 18.7 \\
		                          & 3      &    314.0 &    2613.6 & 20.8 \\
		                          & 3      &    442.8 &    3908.3 & 22.0 \\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		B1 Barrier                & 2      &     45.4 &     359.2 & 19.8 \\
		                          & 3      &    157.8 &    1614.5 & 25.6 \\
		                          & 4      &    198.3 &    2174.1 & 27.4 \\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		Dissemination Barrier     & 2      &     94.4 &     785.3 & 20.8 \\
		                          & 3      &    297.4 &    3189.0 & 26.8 \\
		                          & 4      &    337.1 &    3960.3 & 29.4 \\
		\arrayrulecolor{lightgray}\hline\arrayrulecolor{black}
		B2 Barrier                & 2      &     34.3 &     285.9 & 20.8 \\
		                          & 3      &    109.1 &    1205.6 & 27.6 \\
		                          & 4      &    112.1 &    1297.0 & 28.9 \\
	\end{tabular}
\end{table}

\clearpage
\subsection{Raw data of behind figures}
\begin{itemize}
	\item \todo
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
\section{Bibliography}
\label{sec:bibliography}
\renewcommand\refname{\vskip -1cm} %TODO fine tune perhaps
\nocite{*} % insert not cited references
\bibliographystyle{abbrv}
\bibliography{bibliography}{}

\end{document}
